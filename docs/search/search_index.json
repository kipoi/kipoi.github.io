{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kipoi: Model zoo for genomics This repository implements a python package and a command-line interface (CLI) to access and use models from Kipoi-compatible model zoo's. Links kipoi.org - Main website kipoi.org/docs - Documentation github.com/kipoi/models - Model zoo for genomics maintained by the Kipoi team biorxiv preprint - Kipoi: accelerating the community exchange and reuse of predictive models for genomics Installation 1. Install miniconda/anaconda Kipoi requires conda to manage model dependencies. Make sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see Installing python on OSX . Supported python versions: 2.7 and >=3.5. 2. Install Git LFS For downloading models, Kipoi uses git and Git Large File Storage (LFS). See how to install git here . To install git-lfs on Ubuntu, run: curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash sudo apt-get install -y git git-lfs git-lfs install Alternatively, you can install git-lfs through conda: conda install -c conda-forge git-lfs && git lfs install 3. Install Kipoi Next, install Kipoi using pip : pip install kipoi Quick start The following diagram gives a short overview over Kipoi's workflow: If you want to check which models are available in Kipoi you can use the website , where you can also see example commands for how to use the models on the CLI, python and R. Alternatively you can run kipoi ls in the command line or in python: import kipoi kipoi.list_models() Once a model has been selected (here: rbp_eclip/UPF1 ), the model environments have to be installed. To do so use one of the kipoi env commands. For example to install an environment for model rbp_eclip/UPF1 : kipoi env create rbp_eclip/UPF1 Aside: models versus model groups : A Kipoi model is a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide a path to a model group (e.g \"rbp_eclip\" or \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"rbp_eclip/UPF1\" or \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, a ValueError will be thrown. If you are working on a machine that has GPUs, you will want to add the --gpu flag to the command. And if you want to make use of the kipoi-veff plug-in then add --vep . For more options please run kipoi env create --help . The command will tell you how the execution environment for the model is called, e.g.: INFO [kipoi.cli.env] Environment name: kipoi-rbp_eclip__UPF1 Before using a model in any way, make sure that you have activated its environment, e.g.: prior to executing kipoi or python or R in the attempt to use Kipoi with the model. To activate the model environment run for example: source activate kipoi-rbp_eclip__UPF1 Command-line Once the model environment is activated Kipoi's API can be accessed from the commandline using: $ kipoi usage: kipoi <command> [-h] ... # Kipoi model-zoo command line tool. Available sub-commands: # - using models: ls List all the available models list_plugins List all the available plugins info Print dataloader keyword argument info predict Run the model prediction pull Download the directory associated with the model preproc Run the dataloader and save the results to an hdf5 array env Tools for managing Kipoi conda environments # - contributing models: init Initialize a new Kipoi model test Runs a set of unit-tests for the model test-source Runs a set of unit-tests for many/all models in a source # - plugin commands: veff Variant effect prediction interpret Model interpretation using feature importance scores like ISM, grad*input or DeepLIFT. Explore the CLI usage by running kipoi <command> -h . Also, see docs/using/getting started cli for more information. Python Once the model environment is activated ( source activate kipoi-rbp_eclip__UPF1 ) Kipoi's python API can be used to: The following commands give a short overview. For details please take a look at the python API documentation. Load the model from model the source: import kipoi model = kipoi.get_model(\"rbp_eclip/UPF1\") # load the model To get model predictions using the dataloader we can run: model.pipeline.predict(dict(fasta_file=\"hg19.fa\", intervals_file=\"intervals.bed\", gtf_file=\"gencode.v24.annotation_chr22.gtf\")) # runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions To predict the values of a model input x , which for example was generated by dataloader, we can use: model.predict_on_batch(x) # implemented by all the models regardless of the framework Here x has to be a numpy.ndarray or a list or a dict of a numpy.ndarray , depending on the model requirements, for details please see the documentation of the API or of datalaoder.yaml and model.yaml . For more information see: notebooks/python-api.ipynb and docs/using getting started Configure Kipoi in .kipoi/config.yaml You can add your own (private) model sources. See docs/using/03_Model_sources/ . Contributing models See docs/contributing getting started and docs/tutorials/contributing/models for more information. Plugins Kipoi supports plug-ins which are published as additional python packages. Two plug-ins that are available are: kipoi_veff Variant effect prediction plugin compatible with (DNA) sequence based models. It allows to annotate a vcf file using model predictions for the reference and alternative alleles. The output is written to a new VCF file. For more information see https://kipoi.org/veff-docs/ . pip install kipoi_veff kipoi_interpret Model interpretation plugin for Kipoi. Allows to use feature importance scores like in-silico mutagenesis (ISM), saliency maps or DeepLift with a wide range of Kipoi models. example notebook pip install kipoi_interpret Documentation Documentation can be found here: kipoi.org/docs Citing Kipoi If you use Kipoi for your research, please cite the publication of the model you are using (see model's cite_as entry) and our Biorxiv preprint: https://doi.org/10.1101/375345. @article {kipoi, author = {Avsec, Ziga and Kreuzhuber, Roman and Israeli, Johnny and Xu, Nancy and Cheng, Jun and Shrikumar, Avanti and Banerjee, Abhimanyu and Kim, Daniel S and Urban, Lara and Kundaje, Anshul and Stegle, Oliver and Gagneur, Julien}, title = {Kipoi: accelerating the community exchange and reuse of predictive models for genomics}, year = {2018}, doi = {10.1101/375345}, publisher = {Cold Spring Harbor Laboratory}, URL = {https://www.biorxiv.org/content/early/2018/07/24/375345}, eprint = {https://www.biorxiv.org/content/early/2018/07/24/375345.full.pdf}, journal = {bioRxiv} } Development If you want to help with the development of Kipoi, you are more than welcome to join in! For the local setup for development, you should install kipoi using: conda install pytorch-cpu pip install -e '.[develop]' This will install some additional packages like pytest . You can test the package by running py.test . If you wish to run tests in parallel, run py.test -n 6 .","title":"Home"},{"location":"#kipoi-model-zoo-for-genomics","text":"This repository implements a python package and a command-line interface (CLI) to access and use models from Kipoi-compatible model zoo's.","title":"Kipoi: Model zoo for genomics"},{"location":"#links","text":"kipoi.org - Main website kipoi.org/docs - Documentation github.com/kipoi/models - Model zoo for genomics maintained by the Kipoi team biorxiv preprint - Kipoi: accelerating the community exchange and reuse of predictive models for genomics","title":"Links"},{"location":"#installation","text":"","title":"Installation"},{"location":"#1-install-minicondaanaconda","text":"Kipoi requires conda to manage model dependencies. Make sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see Installing python on OSX . Supported python versions: 2.7 and >=3.5.","title":"1. Install miniconda/anaconda"},{"location":"#2-install-git-lfs","text":"For downloading models, Kipoi uses git and Git Large File Storage (LFS). See how to install git here . To install git-lfs on Ubuntu, run: curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash sudo apt-get install -y git git-lfs git-lfs install Alternatively, you can install git-lfs through conda: conda install -c conda-forge git-lfs && git lfs install","title":"2. Install Git LFS"},{"location":"#3-install-kipoi","text":"Next, install Kipoi using pip : pip install kipoi","title":"3. Install Kipoi"},{"location":"#quick-start","text":"The following diagram gives a short overview over Kipoi's workflow: If you want to check which models are available in Kipoi you can use the website , where you can also see example commands for how to use the models on the CLI, python and R. Alternatively you can run kipoi ls in the command line or in python: import kipoi kipoi.list_models() Once a model has been selected (here: rbp_eclip/UPF1 ), the model environments have to be installed. To do so use one of the kipoi env commands. For example to install an environment for model rbp_eclip/UPF1 : kipoi env create rbp_eclip/UPF1 Aside: models versus model groups : A Kipoi model is a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide a path to a model group (e.g \"rbp_eclip\" or \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"rbp_eclip/UPF1\" or \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, a ValueError will be thrown. If you are working on a machine that has GPUs, you will want to add the --gpu flag to the command. And if you want to make use of the kipoi-veff plug-in then add --vep . For more options please run kipoi env create --help . The command will tell you how the execution environment for the model is called, e.g.: INFO [kipoi.cli.env] Environment name: kipoi-rbp_eclip__UPF1 Before using a model in any way, make sure that you have activated its environment, e.g.: prior to executing kipoi or python or R in the attempt to use Kipoi with the model. To activate the model environment run for example: source activate kipoi-rbp_eclip__UPF1","title":"Quick start"},{"location":"#command-line","text":"Once the model environment is activated Kipoi's API can be accessed from the commandline using: $ kipoi usage: kipoi <command> [-h] ... # Kipoi model-zoo command line tool. Available sub-commands: # - using models: ls List all the available models list_plugins List all the available plugins info Print dataloader keyword argument info predict Run the model prediction pull Download the directory associated with the model preproc Run the dataloader and save the results to an hdf5 array env Tools for managing Kipoi conda environments # - contributing models: init Initialize a new Kipoi model test Runs a set of unit-tests for the model test-source Runs a set of unit-tests for many/all models in a source # - plugin commands: veff Variant effect prediction interpret Model interpretation using feature importance scores like ISM, grad*input or DeepLIFT. Explore the CLI usage by running kipoi <command> -h . Also, see docs/using/getting started cli for more information.","title":"Command-line"},{"location":"#python","text":"Once the model environment is activated ( source activate kipoi-rbp_eclip__UPF1 ) Kipoi's python API can be used to: The following commands give a short overview. For details please take a look at the python API documentation. Load the model from model the source: import kipoi model = kipoi.get_model(\"rbp_eclip/UPF1\") # load the model To get model predictions using the dataloader we can run: model.pipeline.predict(dict(fasta_file=\"hg19.fa\", intervals_file=\"intervals.bed\", gtf_file=\"gencode.v24.annotation_chr22.gtf\")) # runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions To predict the values of a model input x , which for example was generated by dataloader, we can use: model.predict_on_batch(x) # implemented by all the models regardless of the framework Here x has to be a numpy.ndarray or a list or a dict of a numpy.ndarray , depending on the model requirements, for details please see the documentation of the API or of datalaoder.yaml and model.yaml . For more information see: notebooks/python-api.ipynb and docs/using getting started","title":"Python"},{"location":"#configure-kipoi-in-kipoiconfigyaml","text":"You can add your own (private) model sources. See docs/using/03_Model_sources/ .","title":"Configure Kipoi in .kipoi/config.yaml"},{"location":"#contributing-models","text":"See docs/contributing getting started and docs/tutorials/contributing/models for more information.","title":"Contributing models"},{"location":"#plugins","text":"Kipoi supports plug-ins which are published as additional python packages. Two plug-ins that are available are:","title":"Plugins"},{"location":"#kipoi_veff","text":"Variant effect prediction plugin compatible with (DNA) sequence based models. It allows to annotate a vcf file using model predictions for the reference and alternative alleles. The output is written to a new VCF file. For more information see https://kipoi.org/veff-docs/ . pip install kipoi_veff","title":"kipoi_veff"},{"location":"#kipoi_interpret","text":"Model interpretation plugin for Kipoi. Allows to use feature importance scores like in-silico mutagenesis (ISM), saliency maps or DeepLift with a wide range of Kipoi models. example notebook pip install kipoi_interpret","title":"kipoi_interpret"},{"location":"#documentation","text":"Documentation can be found here: kipoi.org/docs","title":"Documentation"},{"location":"#citing-kipoi","text":"If you use Kipoi for your research, please cite the publication of the model you are using (see model's cite_as entry) and our Biorxiv preprint: https://doi.org/10.1101/375345. @article {kipoi, author = {Avsec, Ziga and Kreuzhuber, Roman and Israeli, Johnny and Xu, Nancy and Cheng, Jun and Shrikumar, Avanti and Banerjee, Abhimanyu and Kim, Daniel S and Urban, Lara and Kundaje, Anshul and Stegle, Oliver and Gagneur, Julien}, title = {Kipoi: accelerating the community exchange and reuse of predictive models for genomics}, year = {2018}, doi = {10.1101/375345}, publisher = {Cold Spring Harbor Laboratory}, URL = {https://www.biorxiv.org/content/early/2018/07/24/375345}, eprint = {https://www.biorxiv.org/content/early/2018/07/24/375345.full.pdf}, journal = {bioRxiv} }","title":"Citing Kipoi"},{"location":"#development","text":"If you want to help with the development of Kipoi, you are more than welcome to join in! For the local setup for development, you should install kipoi using: conda install pytorch-cpu pip install -e '.[develop]' This will install some additional packages like pytest . You can test the package by running py.test . If you wish to run tests in parallel, run py.test -n 6 .","title":"Development"},{"location":"api/dataloader/","text":"get_dataloader_descr get_dataloader_descr(dataloader, source='kipoi') Get dataloder description Arguments datalaoder : dataloader's relative path/name in the source. 2nd column in the kipoi.list_dataloader() pd.DataFrame`. source : Model source. 1st column in the kipoi.list_models() pd.DataFrame . get_dataloader get_dataloader(dataloader, source='kipoi') Loads the dataloader Arguments dataloader (str) : dataloader name source (str) : source name Returns Instance of class inheriting from kipoi.data.BaseDataLoader (like kipoi.data.Dataset ) decorated with additional attributes. Methods batch_iter(batch_size, num_workers, **kwargs) Arguments batch_size : batch size num_workers : Number of workers to use in parallel. **kwargs : Other kwargs specific to each dataloader Yields dict with \"inputs\" , \"targets\" and \"metadata\" batch_train_iter(cycle=True, **kwargs) Arguments cycle : if True, cycle indefinitely **kwargs : Kwargs passed to batch_iter() like batch_size Yields tuple of (\"inputs\", \"targets\") from the usual dict returned by batch_iter() batch_predict_iter(**kwargs) Arguments **kwargs : Kwargs passed to batch_iter() like batch_size Yields \"inputs\" field from the usual dict returned by batch_iter() load_all(**kwargs) - load the whole dataset into memory Arguments **kwargs : Kwargs passed to batch_iter() like batch_size Returns dict with \"inputs\" , \"targets\" and \"metadata\" init_example() - instantiate the dataloader with example kwargs print_args() - print information about the required arguments Appended attributes type (str): dataloader type (class name) defined_as (str): path and dataloader name args (list of kipoi.specs.DataLoaderArgument): datalaoder argument description info (kipoi.specs.Info): general information about the dataloader schema (kipoi.specs.DataloaderSchema): information about the input/output data modalities dependencies (kipoi.specs.Dependencies): class specifying the dependencies. (implements install method for running the installation) name (str): model name source (str): model source source_dir (str): local path to model source storage postprocessing (dict): dictionary of loaded plugin specifications example_kwargs (dict): kwargs for running the provided example","title":"Dataloader"},{"location":"api/install_/","text":"install_model_requirements install_model_requirements(model, source='kipoi', and_dataloaders=True) Install model dependencies Arguments model (str) : model name source (str) : model source and_dataloaders (bool) : if True, install also the dependencies for the default dataloader install_dataloader_requirements install_dataloader_requirements(dataloader, source='kipoi') Install dataloader dependencies Arguments datalaoder (str) : dataloader name source (str) : model source","title":"install"},{"location":"api/list_/","text":"list_models list_models(sources=None) List models Arguments sources : list of model sources to use. If None, use all Returns pandas.DataFrame list_dataloaders list_dataloaders(sources=None) List dataloaders Arguments sources : list of model sources to use. If None, use all Returns pandas.DataFrame","title":"list"},{"location":"api/metadata/","text":"kipoi.metadata Module defining different metadata classes compatible with dataloaders All classes inherit from collections.Mapping which allows to use kipoi.data_utils.numpy_collate on them (e.g. they behave as a dictionary). GenomicRanges GenomicRanges(self, chr, start, end, id, strand='*') Container for genomic interval(s) All fields can be either a single values (str or int) or a numpy array of values. Arguments chr (str or np.array) : Chromosome(s) start (int or np.array) : Interval start (0-based coordinates) end (int or np.array) : Interval end (0-based coordinates) id (str or np.array) : Interval id strand (str or np.array) : Interval strand (\"+\", \"-\", or \"*\") from_interval GenomicRanges.from_interval(interval) Create the ranges object from pybedtools.Interval Arguments interval : pybedtools.Interval instance to_interval GenomicRanges.to_interval(self) Convert GenomicRanges object to a Interval object Returns (pybedtools.Interval or list[pybedtools.Interval])","title":"metadata"},{"location":"api/model/","text":"get_model_descr get_model_descr(model, source='kipoi') Get model description Arguments model : model's relative path/name in the source. 2nd column in the kipoi.list_models() pd.DataFrame`. source : Model source. 1st column in the kipoi.list_models() pd.DataFrame . get_model get_model(model, source='kipoi', with_dataloader=True) Load the model from source , as well as the default dataloder to model.default_dataloder. Arguments model (str) : model name source (str) : source name with_dataloader (bool) : if True, the default dataloader is loaded to model.default_dataloader and the pipeline at model.pipeline enabled. Returns Instance of class inheriting from kipoi.models.BaseModel (like kipoi.models.KerasModel ) decorated with additional attributes. Methods predict_on_batch(x) : Make model predictions given a batch of data x Appended attributes type ( str ): model type (class name) args ( dict ): model args used to instantiate the model class info ( kipoi.specs.Info ): information about the author (etc) schema ( kipoi.specs.ModelSchema ): information about the input/outputdata modalities dependencies ( kipoi.specs.Dependencies ): class specifying the dependencies. (implements install method for running the installation) default_dataloader (class inheriting from kipoi.data.BaseDataLoader ): default dataloader. None if with_dataloader=False was used. name ( str ): model name source ( str ): model source source_dir ( str ): local path to model source storage postprocessing ( dict ): dictionary of loaded plugin specifications pipeline ( kipoi.pipeline.Pipeline ): handle to a Pipeline object","title":"Model"},{"location":"api/pipeline/","text":"Pipeline Pipeline(self, model, dataloader_cls) Runs model predictions from raw files: raw files --(dataloader)--> data batches --(model)--> prediction Arguments model : model returned by kipoi.get_model dataloader_cls : dataloader class returned by kipoi.get_dataloader_factory of kipoi.get_model().default_dataloader predict_example Pipeline.predict_example(self, batch_size=32, output_file=None) Run model prediction for the example file Arguments batch_size : batch_size output_file : if not None, inputs and predictions are stored to output_file path **kwargs : Further arguments passed to batch_iter predict Pipeline.predict(self, dataloader_kwargs, batch_size=32, **kwargs) Arguments dataloader_kwargs : Keyword arguments passed to the pre-processor **kwargs : Further arguments passed to batch_iter Returns np.array, dict, list : Predict the whole array predict_generator Pipeline.predict_generator(self, dataloader_kwargs, batch_size=32, layer=None, **kwargs) Prediction generator Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Size of batches produced by the dataloader layer : If not None activation of specified layer will be returned. Only possible for models that are a subclass of LayerActivationMixin . **kwargs : Further arguments passed to batch_iter Yields dict : model batch prediction predict_to_file Pipeline.predict_to_file(self, output_file, dataloader_kwargs, batch_size=32, keep_inputs=False, **kwargs) Make predictions and write them iteratively to a file Arguments output_file : output file path. File format is inferred from the file path ending. Available file formats are: 'bed', 'h5', 'hdf5', 'tsv' dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader keep_inputs : if True, inputs and targets will also be written to the output file. **kwargs : Further arguments passed to batch_iter input_grad Pipeline.input_grad(self, dataloader_kwargs, batch_size=32, filter_idx=None, avg_func=None, layer=None, final_layer=True, selected_fwd_node=None, pre_nonlinearity=False, **kwargs) Get input gradients Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader filter_idx : filter index of layer for which the gradient should be returned avg_func : String name of averaging function to be applied across filters in layer layer layer : layer from which backwards the gradient should be calculated final_layer : Use the final (classification) layer as layer selected_fwd_node : None - not supported by KerasModel at the moment pre_nonlinearity : Try to use the layer output prior to activation (will not always be possible in an automatic way) **kwargs : Further arguments passed to input_grad Returns dict : A dictionary of all model inputs and the gradients. Gradients are stored in key 'grads' input_grad_generator Pipeline.input_grad_generator(self, dataloader_kwargs, batch_size=32, filter_idx=None, avg_func=None, layer=None, final_layer=True, selected_fwd_node=None, pre_nonlinearity=False, **kwargs) Get input gradients Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader filter_idx : filter index of layer for which the gradient should be returned avg_func : String name of averaging function to be applied across filters in layer layer layer : layer from which backwards the gradient should be calculated final_layer : Use the final (classification) layer as layer selected_fwd_node : None - not supported by KerasModel at the moment pre_nonlinearity : Try to use the layer output prior to activation (will not always be possible in an automatic way) **kwargs : Further arguments passed to input_grad Yields dict : A dictionary of all model inputs and the gradients. Gradients are stored in key 'grads'","title":"Pipeline"},{"location":"api/readers/","text":"kipoi.readers Readers useful for creating new dataloaders HDF5Reader HDF5Reader HDF5Reader(self, file_path) Read the HDF5 file. Convenience wrapper around h5py.File Arguments file_path : File path to an HDF5 file ls HDF5Reader.ls(self) Recursively list the arrays load_all HDF5Reader.load_all(self, unflatten=True) Load the whole file Arguments unflatten : if True, nest/unflatten the keys. e.g. an entry f['/foo/bar'] would need to be accessed using two nested get call : f['foo']['bar'] batch_iter HDF5Reader.batch_iter(self, batch_size=16, **kwargs) Create a batch iterator over the whole file Arguments batch_size : batch size **kwargs : ignored argument. Used for consistency with other dataloaders open HDF5Reader.open(self) Open the file close HDF5Reader.close(self) Close the file load HDF5Reader.load(file_path, unflatten=True) Load the data all at once (classmethod). Arguments file_path : HDF5 file path unflatten : see load_all","title":"readers"},{"location":"api/sources/","text":"get_source get_source(source) Get the source object Arguments source (str) : source string Returns Source child class instance : like kipoi.sources.GitLFSSource list_sources list_sources() Returns a pandas.DataFrame of possible sources","title":"sources"},{"location":"api/writers/","text":"kipoi.writers Writers used in kipoi predict TsvBatchWriter BedBatchWriter HDF5BatchWriter RegionWriter BedGraphWriter BigWigWriter TsvBatchWriter TsvBatchWriter(self, file_path, nested_sep='/') Tab-separated file writer Arguments file_path (str) : File path of the output tsv file nested_sep : What separator to use for flattening the nested dictionary structure into a single key batch_write TsvBatchWriter.batch_write(self, batch) Write a batch of data Arguments batch : batch of data. Either a single np.array or a list/dict thereof. BedBatchWriter BedBatchWriter(self, file_path, metadata_schema, header=True) Bed-file writer Arguments file_path (str) : File path of the output tsv file dataloader_schema : Schema of the dataloader. Used to find the ranges object nested_sep : What separator to use for flattening the nested dictionary structure into a single key batch_write BedBatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof. HDF5BatchWriter HDF5BatchWriter(self, file_path, chunk_size=10000, compression='gzip') HDF5 file writer Arguments file_path (str) : File path of the output tsv file chunk_size (str) : Chunk size for storing the files nested_sep : What separator to use for flattening the nested dictionary structure into a single key compression (str) : default compression to use for the hdf5 datasets. see also : http://docs.h5py.org/en/latest/high/dataset.html#dataset-compression batch_write HDF5BatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof. close HDF5BatchWriter.close(self) Close the file handle dump HDF5BatchWriter.dump(file_path, batch) In a single shot write the batch/data to a file and close the file. Arguments file_path : file path batch : batch of data. Either a single np.array or a list/dict thereof. BedGraphWriter BedGraphWriter(self, file_path) Arguments file_path (str) : File path of the output bedgraph file region_write BedGraphWriter.region_write(self, region, data) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first. write_entry BedGraphWriter.write_entry(self, chr, start, end, value) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first. close BedGraphWriter.close(self) Close the file BigWigWriter BigWigWriter(self, file_path) BigWig entries have to be sorted so the generated values are cached in a bedgraph file. Arguments file_path (str) : File path of the output tsv file write_entry BigWigWriter.write_entry(self, chr, start, end, value) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first. close BigWigWriter.close(self) Close the file","title":"writers"},{"location":"contributing/01_Getting_started/","text":"Contributing models - Getting started Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the kipoi/models github repository. The minimum requirement for a model is that a model.yaml file is available in the model folder, which defines the type of the model, file paths / URLs, the dataloader, description, software dependencies, etc. All files necessary for a model to be executed have to be published on zenodo or figshare to insure functionality and versioning of models. One key element for a model to be contributed to Kipoi is its dataloader. The main aim of a dataloader is to generate batches of data with which a model can be run. Its inputs should be files in the most common formats of the respective field, such as .bed and .fasta files for genomic sequences and regions. Pre-defined datalaoders To simplify the process of contributing models to Kipoi we have created kipoiseq , a repository that offers pre-defined dataloaders for common applications. If you can use one of the dataloaders in kipoiseq for you model then the Kipoi model will consist solely in a folder and one model.yaml file inside it: MyModel \u2514\u2500\u2500 model.yaml # describes the model Non-default dataloaders If the model you want to contribute requires different input from what is available out of the box in kipoiseq , you are encouraged to use the tested tools available in kipoiseq to write your own dataloader and its companion yaml-file . If you do so you should keep the standard Kipoi way of defining models with all the files and their assigned places: MyModel \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u2514\u2500\u2500 model.yaml # describes the model Required steps for contribution Using pre-defined datalaoders If the dataloaders offered in kipoiseq are what your model needs then submitting a new model can even be done online using github or locally as explained here . Contribute model online You can contribute a model online on github by clicking Create new file in the models repository . The filename would then be MyModel/model.yaml . The name of folder (here: MyModel ) which contains the model.yaml file. You can then select Create a new branch for this commit and start a pull request to attempt adding your model to Kipoi. If you want to test your model locally you have to make sure that kipoi and git are installed locally. You can test your models as described here . Defining your own dataloader If the pre-defined dataloaders don't cover your use-case you will have to define your own. You therefore have to set up kipoi as described here . Setting up Kipoi for model contribution Here is a list of steps required to contribute a model to kipoi/models : 1. Install Kipoi Install git There are many ways to do so and on many systems git is already installed. If not you can follow this guide. Install kipoi pip install kipoi Run kipoi ls (this will checkout the kipoi/models repo to ~/.kipoi/models ) 2. Add the model cd ~/.kipoi/models Write the model : Create a new folder <my new model> containing all the required files. The required files can be created by doing one of the following three options: Option 1: Copy the existing model: cp -R <existing model> <my new model> , edit/replace/add the copied files until they fit your new model. Option 2: Run kipoi init , answer the questions, edit/replace the created files until they fit your new model. Option 3: mkdir <my new model> & write all the files from scratch Test the model Step 1: kipoi test ~/.kipoi/models/my_new_model Step 2: kipoi test-source kipoi --all -k my_new_model 3. Submit the pull-request Option 1: Fork the repository Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Commit your changes git add my_new_model/ git commit -m \"Added <my new model>\" Fork the https://github.com/kipoi/models repo on github (click on the Fork button) Add your fork as a git remote to ~/.kipoi/models git remote add fork https://github.com/<username>/models.git Push to your fork git push fork master Submit a pull-request click the New pull request button on your github fork - https://github.com/<username>/models> Option 2: Create a new branch on kipoi/models If you wish to contribute models more frequently, please join the team . You will be added to the Kipoi organization. This will allow you to push to branches of the kipoi/models github repo directly. Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Create a new branch in ~/.kipoi/models git stash - this will store/stash all local changes in git stash git checkout -b my_new_model - create a new branch git stash pop - get the stashed files back Commit changes git add my_new_model/ git commit -m \"Added <my new model>\" Push changes to my_new_model branch git push -u origin my_new_model Submit a pull-request click the New pull request button on my_new_model branch of repo https://github.com/kipoi/models . Rest of this document will go more into the details about steps writing the model and testing the model. How to write the model Best place to start figuring out which files you need to contribute is to look at some of the existing models. Explore the https://github.com/kipoi/models repository and see if there are any models similar to yours (in terms of the dependencies, framework, input-output data modalities). See tutorials/contributing_models for a step-by-step procedure for contributing models. In terms of what to include in your model: The information in these pages here are the minimum requirement. The more information you can share with other users the better! If you have converted the model from using a script, please add that. If you have additional test and validation scripts that you wrote while verifying the Kipoi model, etc. , please add them. You will make future users happy. Hint: If you want to take a look at a specific model that is already in the zoo, but instead of the content of the model files there is just a hash entry, then use kipoi pull <model_name> to download the model data. Option #1: Copy existing model Once you have found the closest match, simply copy the directory and start editing/replacing the files. Edit the files in this order: model.yaml dataloader.yaml (If you wrote your own dataloader) dataloader.py (If you wrote your own dataloader) LICENSE Option #2: Use kipoi init Alternatively, you can use kipoi init instead of copying the existing model: cd ~/.kipoi/models && kipoi init This will ask you a few questions and create a new model folder. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model Please answer the questions below. Defaults are shown in square brackets. You might find the following links useful: - (model_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_models.md - (dataloader_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_dataloaders.md -------------------------------------------- model_name [my_model]: my_new_model author_name [Your name]: Ziga Avsec author_github [Your github username]: avsecz author_email [Your email(optional)]: model_doc [Model description]: Model predicting iris species Select model_license: 1 - MIT 2 - BSD 3 - ISCL 4 - Apache Software License 2.0 5 - Not open source Choose from 1, 2, 3, 4, 5 [1]: Select model_type: 1 - keras 2 - custom 3 - sklearn Choose from 1, 2, 3 [1]: 1 Select model_input_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 2 Select model_output_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 3 Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: 1 -------------------------------------------- INFO [kipoi.cli.main] Done! Created the following folder into the current working directory: my_new_model The created folder contains a model and a dataloader for predicting the Iris species. You will now have to edit the model.yaml and to edit the dataloader.yaml files according to your model. You can check whether you have succeeded and your model is setup correctly with the commands below. How to test the model Be aware that the test functions will only check whether the definition side of things (model.yaml, dataloader.yaml, syntax errors, etc.) is setup correctly, you will have to validate yourself whether the outputs created by using the predict function produce the desired model output! Step 1: Run kipoi test ~/.kipoi/models/my_new_model This checks the yaml files and runs kipoi predict for the example files (specified in dataloader.yaml > args > my_arg > example ). Once this command returns no errors or warnings proceed to the next step. Step 2: Run kipoi test-source kipoi --all -k my_new_model This will run kipoi test in a new conda environment with dependencies specified in model.yaml and dataloader.yaml . Removing or updating models To remove, rename or update an existing model, send a pull-request (as when contributing models, see 3. Submit the pull-request ).","title":"Getting started"},{"location":"contributing/01_Getting_started/#contributing-models-getting-started","text":"Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the kipoi/models github repository. The minimum requirement for a model is that a model.yaml file is available in the model folder, which defines the type of the model, file paths / URLs, the dataloader, description, software dependencies, etc. All files necessary for a model to be executed have to be published on zenodo or figshare to insure functionality and versioning of models. One key element for a model to be contributed to Kipoi is its dataloader. The main aim of a dataloader is to generate batches of data with which a model can be run. Its inputs should be files in the most common formats of the respective field, such as .bed and .fasta files for genomic sequences and regions.","title":"Contributing models - Getting started"},{"location":"contributing/01_Getting_started/#pre-defined-datalaoders","text":"To simplify the process of contributing models to Kipoi we have created kipoiseq , a repository that offers pre-defined dataloaders for common applications. If you can use one of the dataloaders in kipoiseq for you model then the Kipoi model will consist solely in a folder and one model.yaml file inside it: MyModel \u2514\u2500\u2500 model.yaml # describes the model","title":"Pre-defined datalaoders"},{"location":"contributing/01_Getting_started/#non-default-dataloaders","text":"If the model you want to contribute requires different input from what is available out of the box in kipoiseq , you are encouraged to use the tested tools available in kipoiseq to write your own dataloader and its companion yaml-file . If you do so you should keep the standard Kipoi way of defining models with all the files and their assigned places: MyModel \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u2514\u2500\u2500 model.yaml # describes the model","title":"Non-default dataloaders"},{"location":"contributing/01_Getting_started/#required-steps-for-contribution","text":"","title":"Required steps for contribution"},{"location":"contributing/01_Getting_started/#using-pre-defined-datalaoders","text":"If the dataloaders offered in kipoiseq are what your model needs then submitting a new model can even be done online using github or locally as explained here .","title":"Using pre-defined datalaoders"},{"location":"contributing/01_Getting_started/#contribute-model-online","text":"You can contribute a model online on github by clicking Create new file in the models repository . The filename would then be MyModel/model.yaml . The name of folder (here: MyModel ) which contains the model.yaml file. You can then select Create a new branch for this commit and start a pull request to attempt adding your model to Kipoi. If you want to test your model locally you have to make sure that kipoi and git are installed locally. You can test your models as described here .","title":"Contribute model online"},{"location":"contributing/01_Getting_started/#defining-your-own-dataloader","text":"If the pre-defined dataloaders don't cover your use-case you will have to define your own. You therefore have to set up kipoi as described here .","title":"Defining your own dataloader"},{"location":"contributing/01_Getting_started/#setting-up-kipoi-for-model-contribution","text":"Here is a list of steps required to contribute a model to kipoi/models :","title":"Setting up Kipoi for model contribution"},{"location":"contributing/01_Getting_started/#1-install-kipoi","text":"Install git There are many ways to do so and on many systems git is already installed. If not you can follow this guide. Install kipoi pip install kipoi Run kipoi ls (this will checkout the kipoi/models repo to ~/.kipoi/models )","title":"1. Install Kipoi"},{"location":"contributing/01_Getting_started/#2-add-the-model","text":"cd ~/.kipoi/models Write the model : Create a new folder <my new model> containing all the required files. The required files can be created by doing one of the following three options: Option 1: Copy the existing model: cp -R <existing model> <my new model> , edit/replace/add the copied files until they fit your new model. Option 2: Run kipoi init , answer the questions, edit/replace the created files until they fit your new model. Option 3: mkdir <my new model> & write all the files from scratch Test the model Step 1: kipoi test ~/.kipoi/models/my_new_model Step 2: kipoi test-source kipoi --all -k my_new_model","title":"2. Add the model"},{"location":"contributing/01_Getting_started/#3-submit-the-pull-request","text":"","title":"3. Submit the pull-request"},{"location":"contributing/01_Getting_started/#option-1-fork-the-repository","text":"Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Commit your changes git add my_new_model/ git commit -m \"Added <my new model>\" Fork the https://github.com/kipoi/models repo on github (click on the Fork button) Add your fork as a git remote to ~/.kipoi/models git remote add fork https://github.com/<username>/models.git Push to your fork git push fork master Submit a pull-request click the New pull request button on your github fork - https://github.com/<username>/models>","title":"Option 1: Fork the repository"},{"location":"contributing/01_Getting_started/#option-2-create-a-new-branch-on-kipoimodels","text":"If you wish to contribute models more frequently, please join the team . You will be added to the Kipoi organization. This will allow you to push to branches of the kipoi/models github repo directly. Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Create a new branch in ~/.kipoi/models git stash - this will store/stash all local changes in git stash git checkout -b my_new_model - create a new branch git stash pop - get the stashed files back Commit changes git add my_new_model/ git commit -m \"Added <my new model>\" Push changes to my_new_model branch git push -u origin my_new_model Submit a pull-request click the New pull request button on my_new_model branch of repo https://github.com/kipoi/models . Rest of this document will go more into the details about steps writing the model and testing the model.","title":"Option 2: Create a new branch on kipoi/models"},{"location":"contributing/01_Getting_started/#how-to-write-the-model","text":"Best place to start figuring out which files you need to contribute is to look at some of the existing models. Explore the https://github.com/kipoi/models repository and see if there are any models similar to yours (in terms of the dependencies, framework, input-output data modalities). See tutorials/contributing_models for a step-by-step procedure for contributing models. In terms of what to include in your model: The information in these pages here are the minimum requirement. The more information you can share with other users the better! If you have converted the model from using a script, please add that. If you have additional test and validation scripts that you wrote while verifying the Kipoi model, etc. , please add them. You will make future users happy. Hint: If you want to take a look at a specific model that is already in the zoo, but instead of the content of the model files there is just a hash entry, then use kipoi pull <model_name> to download the model data.","title":"How to write the model"},{"location":"contributing/01_Getting_started/#option-1-copy-existing-model","text":"Once you have found the closest match, simply copy the directory and start editing/replacing the files. Edit the files in this order: model.yaml dataloader.yaml (If you wrote your own dataloader) dataloader.py (If you wrote your own dataloader) LICENSE","title":"Option #1: Copy existing model"},{"location":"contributing/01_Getting_started/#option-2-use-kipoi-init","text":"Alternatively, you can use kipoi init instead of copying the existing model: cd ~/.kipoi/models && kipoi init This will ask you a few questions and create a new model folder. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model Please answer the questions below. Defaults are shown in square brackets. You might find the following links useful: - (model_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_models.md - (dataloader_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_dataloaders.md -------------------------------------------- model_name [my_model]: my_new_model author_name [Your name]: Ziga Avsec author_github [Your github username]: avsecz author_email [Your email(optional)]: model_doc [Model description]: Model predicting iris species Select model_license: 1 - MIT 2 - BSD 3 - ISCL 4 - Apache Software License 2.0 5 - Not open source Choose from 1, 2, 3, 4, 5 [1]: Select model_type: 1 - keras 2 - custom 3 - sklearn Choose from 1, 2, 3 [1]: 1 Select model_input_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 2 Select model_output_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 3 Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: 1 -------------------------------------------- INFO [kipoi.cli.main] Done! Created the following folder into the current working directory: my_new_model The created folder contains a model and a dataloader for predicting the Iris species. You will now have to edit the model.yaml and to edit the dataloader.yaml files according to your model. You can check whether you have succeeded and your model is setup correctly with the commands below.","title":"Option #2: Use kipoi init"},{"location":"contributing/01_Getting_started/#how-to-test-the-model","text":"Be aware that the test functions will only check whether the definition side of things (model.yaml, dataloader.yaml, syntax errors, etc.) is setup correctly, you will have to validate yourself whether the outputs created by using the predict function produce the desired model output!","title":"How to test the model"},{"location":"contributing/01_Getting_started/#step-1-run-kipoi-test-kipoimodelsmy_new_model","text":"This checks the yaml files and runs kipoi predict for the example files (specified in dataloader.yaml > args > my_arg > example ). Once this command returns no errors or warnings proceed to the next step.","title":"Step 1: Run kipoi test ~/.kipoi/models/my_new_model"},{"location":"contributing/01_Getting_started/#step-2-run-kipoi-test-source-kipoi-all-k-my_new_model","text":"This will run kipoi test in a new conda environment with dependencies specified in model.yaml and dataloader.yaml .","title":"Step 2: Run kipoi test-source kipoi --all -k my_new_model"},{"location":"contributing/01_Getting_started/#removing-or-updating-models","text":"To remove, rename or update an existing model, send a pull-request (as when contributing models, see 3. Submit the pull-request ).","title":"Removing or updating models"},{"location":"contributing/02_Writing_model.yaml/","text":"model.yaml The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct definitions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future. To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example model.yaml : defined_as: kipoi.model.KerasModel args: # arguments of `kipoi.model.KerasModel` arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc default_dataloader: . # path to the dataloader directory. Or to the dataloader class, e.g.: `kipoiseq.dataloaders.SeqIntervalDl info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" The model.yaml file has the following mandatory fields: defined_as The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the respective class name in Kipoi has to be used. Therefore defined_as can be one of the followinf values: kipoi.model.KerasModel , kipoi.model.PyTorchModel , kipoi.model.SklearnModel , and kipoi.model.TensorFlowModel . If you wrote your own Kipoi model class, you called it MyModel , and you defined it in the file my_model.py , then the type field would be: my_model.MyModel . The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks. In the model.yaml file the definition of a Keras model would like this: defined_as: kipoi.model.KerasModel args Model arguments define where the files are files and functions are located to instantiate the model. Most entries of args will contain links to zenodo or figshare downloads. The correct definition of args depends on the model defined_as that was selected: kipoi.model.KerasModel models For Keras models the following args are available: weights : URL and md5 of the hdf5 weights or the hdf5 Keras model. arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : URL and md5 of python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', 'cntk') image_dim_ordering : 'tf' or 'th' : Whether the model was trained with using 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. The Keras framework offers different ways to store model architecture and weights: Architecture and weights can be stored separately: defined_as: kipoi.model.KerasModel args: arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc The architecture can be stored together with the weights: defined_as: kipoi.model.KerasModel args: weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that is uploaded with the model architecture and weights. This file defines a dictionary containing custom Keras components called OBJECTS . These objects will be added to custom_objects when loading the model with keras.models.load_model . Example of a custom_keras_objects.py : from concise.layers import SplineT OBJECTS = {\"SplineT\": SplineT} Example of the corresponding model.yaml entry: defined_as: kipoi.model.KerasModel args: ... custom_objects: custom_keras_objects.py Here all the objects present in custom_keras_objects.py will be made available to Keras when loading the model. kipoi.model.PyTorchModel models Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following args : weights , module_class , module_kwargs , module_obj . PyTorch models require python code in which the model is defined. The code that defines the model should not attempt load the weights, as this is done inside the PyTorchModel class in Kipoi using the model.load_state_dict(torch.load(weights)) command. For example the pytorch model definition could be in a file my_pytorch_model.py : from torch import nn class DummyModel(nn.Module): def __init__(self, x, y, z): super(DummyModel, self).__init__() # Some code here def forward(self, x): # some code here return x Assuming that the my_pytorch_model.py file lies in the same folder as the model.yaml , the default way for loading this model in Kiopi is then as follows: defined_as: kipoi.model.PyTorchModel args: module_class: my_pytorch_model.DummyModel module_kwargs: x: 1 y: 2 z: 3 weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If the module class does not have any arguments then module_kwargs can be omitted. If you use Sequential models ( torch.nn.Sequential ) or you generate a module instance in your my_sequential.py file, then you can use the module_obj in model.yaml to load that module: defined_as: kipoi.model.PyTorchModel args: module_obj: my_sequential.sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc where my_sequential.py for example contains: import torch sequential_model = torch.nn.Sequential(...) If you have trouble with the imports or if you would like to import a module from a parent directory you can explicitly specify the python file path: defined_as: kipoi.model.PyTorchModel args: module_file: ./my_sequential.py module_obj: sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If cuda is available on the system then the model will automatically be switched to cuda mode, so the user does not have to take care of that. kipoi.model.SklearnModel models SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: joblib.load(pkl_file) defined_as: kipoi.model.SklearnModel args: pkl_file: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc predict_method: predict_proba # Optional. predict by default. Available: predict, predict_proba, predict_log_proba kipoi.model.TensorFlowModel models Tensorflow models are expected to be stored by calling saver = tf.train.Saver(); saver.save(checkpoint_path) . The input_nodes argument is then a string, list of strings or dictionary of strings that define the input node names. The target_nodes argument is a string, list of strings or dictionary of strings that define the model target node names. defined_as: kipoi.model.TensorFlowModel args: input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: https://zenodo.org/path/to/my/model.tf md5: 1234567890abc If a model requires a constant feed of data which is not provided by the dataloader the const_feed_dict_pkl argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in const_feed_dict_pkl . defined_as: kipoi.model.TensorFlowModel args: ... const_feed_dict_pkl: url: https://zenodo.org/path/to/my/const_feed_dict.pkl md5: 1234567890abc custom models It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined Model class must be a subclass of BaseModel defined in kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! If for example a custom model class definition ( MyModel ) is in a file my_model.py , then the model.yaml will contain: defined_as: my_model.MyModel Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of BaseModel , which in other words means that def predict_on_batch(self, x) has to be implemented. So if batch is for example what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to work. It is likely that MyModel will require additional files to work. The Kipoi way of using such files is by defining Model in the following way: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, external_file): self.data = read_my_file(external_file) #... The file will be downloaded from zenodo or figshare automatically and assigned to the external_file argument if the model.yaml contains: defined_as: my_model.MyModel args: external_file: default: url: https://zenodo.org/path/to/my/data md5: 1234567890abc info The info field of a model.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the model. A short description of what it does and what it is designed for. license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the model and its use cases cite_as : Link to the journal, arXiv, ... trained_on : Description of the training dataset training_procedure : Description of the training procedure A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com - name: Second Author doc: My model description license: GNU tags: - TFBS - tag2 cite_as: http://www.the_journal.com/mypublication trained_on: The XXXX dataset from YYYY training_procedure: 10-fold cross validation default_dataloader The default_dataloader defines the dataloader that should be used for the given model. It can either be defined by a package like kipoiseq or it can be defined by the contributor. Using a pre-defined dataloader If one of the ready-made dataloaders on kipoiseq fits the needs of your model, then please follow the instructions on kipoiseq . The default_dataloader in the model.yaml would then for example be: default_dataloader: defined_as: kipoiseq.dataloaders.SeqIntervalDl default_args: auto_resize_len: 1000 alphabet_axis: 0 dummy_axis: 1 dtype: float32 Using a custom dataloader If you need a specialised dataloader you are encouraged to used as many methods and classes from within kipoiseq as possible as their functionality is tested. See more information on writing a dataloader and its companion yaml. Both of those files should lie in the same folder as the model.yaml . Then the default_dataloader entry in model.yaml is: default_dataloader: . It points to the location of the dataloader.yaml file. If dataloader.yaml lies in different subfolder then default_dataloader: path/to/folder would be used where dataloader.yaml would lie in folder . schema Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are. schema contains two categories inputs and targets which each specify the shapes of the model input and model output. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A dictionary of numpy arrays as inputs or targets: schema: inputs: seq: shape: (1000,4) inp2: shape: (10) inputs The inputs fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input , not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction. targets The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model input name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line). How model types handle schemas The different model types handle those three different encapsulations of numpy arrays differently: KerasModel models Input In case a Keras model is used the batch produced by the dataloader is passed on as it is to the model.predict_on_batch() function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to model.predict_on_batch() . Output The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema. PyTorchModel models Pytorch needs torch.autograd.Variable instances to work. Hence all inputs are automatically converted into Variable objects and results are converted back into numpy arrays transparently. If cuda is available the model will automatically be used in cuda mode and also the input variables will be switched to cuda . Input For prediction the following will happen to the tree different encapsulations of input arrays: A single array: Will be passed directly as the only argument to model call: model(Variable(from_numpy(x))) A list of arrays: The model will be called with the list of converted array as args (e.g.: model(*list_of_variables) ) A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: model(**dict_of_variables) ) Output The model return values will be converted back into encapsulations of numpy arrays, where: a single Variable object will be converted into a numpy arrays lists of Variable objects will be converted into a list of numpy arrays in the same order and SklearnModel models The batch generated by the dataloader will be passed on directly to the SciKit-learn model using model.predict(x) , model.predict_proba(x) or model.predict_log_proba (depending on the predict_method argument). TensorFlowModel models Input The feed_dict for running a tensorflow session is generated by converting the batch samples into the feed_dict using input_nodes defined in the args section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays: If input_nodes is a single string the model will be fed with a dictionary {input_ops: x} If input_nodes is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that. If input_nodes is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that. Output The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the schema field of model.yaml has to match that. dependencies One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. If your model is a python-based model and you have not tested whether your model works in python 2 and python 3, then make sure that you also add the correct python version as a dependency e.g.: python=2.7 . conda Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used. pip Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). postprocessing The postprocessing section of a model.yaml is necessary to indicate that a model is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your model up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"model.yaml"},{"location":"contributing/02_Writing_model.yaml/#modelyaml","text":"The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct definitions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future. To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example model.yaml : defined_as: kipoi.model.KerasModel args: # arguments of `kipoi.model.KerasModel` arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc default_dataloader: . # path to the dataloader directory. Or to the dataloader class, e.g.: `kipoiseq.dataloaders.SeqIntervalDl info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" The model.yaml file has the following mandatory fields:","title":"model.yaml"},{"location":"contributing/02_Writing_model.yaml/#defined_as","text":"The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the respective class name in Kipoi has to be used. Therefore defined_as can be one of the followinf values: kipoi.model.KerasModel , kipoi.model.PyTorchModel , kipoi.model.SklearnModel , and kipoi.model.TensorFlowModel . If you wrote your own Kipoi model class, you called it MyModel , and you defined it in the file my_model.py , then the type field would be: my_model.MyModel . The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks. In the model.yaml file the definition of a Keras model would like this: defined_as: kipoi.model.KerasModel","title":"defined_as"},{"location":"contributing/02_Writing_model.yaml/#args","text":"Model arguments define where the files are files and functions are located to instantiate the model. Most entries of args will contain links to zenodo or figshare downloads. The correct definition of args depends on the model defined_as that was selected:","title":"args"},{"location":"contributing/02_Writing_model.yaml/#kipoimodelkerasmodel-models","text":"For Keras models the following args are available: weights : URL and md5 of the hdf5 weights or the hdf5 Keras model. arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : URL and md5 of python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', 'cntk') image_dim_ordering : 'tf' or 'th' : Whether the model was trained with using 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. The Keras framework offers different ways to store model architecture and weights: Architecture and weights can be stored separately: defined_as: kipoi.model.KerasModel args: arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc The architecture can be stored together with the weights: defined_as: kipoi.model.KerasModel args: weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that is uploaded with the model architecture and weights. This file defines a dictionary containing custom Keras components called OBJECTS . These objects will be added to custom_objects when loading the model with keras.models.load_model . Example of a custom_keras_objects.py : from concise.layers import SplineT OBJECTS = {\"SplineT\": SplineT} Example of the corresponding model.yaml entry: defined_as: kipoi.model.KerasModel args: ... custom_objects: custom_keras_objects.py Here all the objects present in custom_keras_objects.py will be made available to Keras when loading the model.","title":"kipoi.model.KerasModel models"},{"location":"contributing/02_Writing_model.yaml/#kipoimodelpytorchmodel-models","text":"Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following args : weights , module_class , module_kwargs , module_obj . PyTorch models require python code in which the model is defined. The code that defines the model should not attempt load the weights, as this is done inside the PyTorchModel class in Kipoi using the model.load_state_dict(torch.load(weights)) command. For example the pytorch model definition could be in a file my_pytorch_model.py : from torch import nn class DummyModel(nn.Module): def __init__(self, x, y, z): super(DummyModel, self).__init__() # Some code here def forward(self, x): # some code here return x Assuming that the my_pytorch_model.py file lies in the same folder as the model.yaml , the default way for loading this model in Kiopi is then as follows: defined_as: kipoi.model.PyTorchModel args: module_class: my_pytorch_model.DummyModel module_kwargs: x: 1 y: 2 z: 3 weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If the module class does not have any arguments then module_kwargs can be omitted. If you use Sequential models ( torch.nn.Sequential ) or you generate a module instance in your my_sequential.py file, then you can use the module_obj in model.yaml to load that module: defined_as: kipoi.model.PyTorchModel args: module_obj: my_sequential.sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc where my_sequential.py for example contains: import torch sequential_model = torch.nn.Sequential(...) If you have trouble with the imports or if you would like to import a module from a parent directory you can explicitly specify the python file path: defined_as: kipoi.model.PyTorchModel args: module_file: ./my_sequential.py module_obj: sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If cuda is available on the system then the model will automatically be switched to cuda mode, so the user does not have to take care of that.","title":"kipoi.model.PyTorchModel models"},{"location":"contributing/02_Writing_model.yaml/#kipoimodelsklearnmodel-models","text":"SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: joblib.load(pkl_file) defined_as: kipoi.model.SklearnModel args: pkl_file: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc predict_method: predict_proba # Optional. predict by default. Available: predict, predict_proba, predict_log_proba","title":"kipoi.model.SklearnModel models"},{"location":"contributing/02_Writing_model.yaml/#kipoimodeltensorflowmodel-models","text":"Tensorflow models are expected to be stored by calling saver = tf.train.Saver(); saver.save(checkpoint_path) . The input_nodes argument is then a string, list of strings or dictionary of strings that define the input node names. The target_nodes argument is a string, list of strings or dictionary of strings that define the model target node names. defined_as: kipoi.model.TensorFlowModel args: input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: https://zenodo.org/path/to/my/model.tf md5: 1234567890abc If a model requires a constant feed of data which is not provided by the dataloader the const_feed_dict_pkl argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in const_feed_dict_pkl . defined_as: kipoi.model.TensorFlowModel args: ... const_feed_dict_pkl: url: https://zenodo.org/path/to/my/const_feed_dict.pkl md5: 1234567890abc","title":"kipoi.model.TensorFlowModel models"},{"location":"contributing/02_Writing_model.yaml/#custom-models","text":"It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined Model class must be a subclass of BaseModel defined in kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! If for example a custom model class definition ( MyModel ) is in a file my_model.py , then the model.yaml will contain: defined_as: my_model.MyModel Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of BaseModel , which in other words means that def predict_on_batch(self, x) has to be implemented. So if batch is for example what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to work. It is likely that MyModel will require additional files to work. The Kipoi way of using such files is by defining Model in the following way: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, external_file): self.data = read_my_file(external_file) #... The file will be downloaded from zenodo or figshare automatically and assigned to the external_file argument if the model.yaml contains: defined_as: my_model.MyModel args: external_file: default: url: https://zenodo.org/path/to/my/data md5: 1234567890abc","title":"custom models"},{"location":"contributing/02_Writing_model.yaml/#info","text":"The info field of a model.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the model. A short description of what it does and what it is designed for. license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the model and its use cases cite_as : Link to the journal, arXiv, ... trained_on : Description of the training dataset training_procedure : Description of the training procedure A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com - name: Second Author doc: My model description license: GNU tags: - TFBS - tag2 cite_as: http://www.the_journal.com/mypublication trained_on: The XXXX dataset from YYYY training_procedure: 10-fold cross validation","title":"info"},{"location":"contributing/02_Writing_model.yaml/#default_dataloader","text":"The default_dataloader defines the dataloader that should be used for the given model. It can either be defined by a package like kipoiseq or it can be defined by the contributor.","title":"default_dataloader"},{"location":"contributing/02_Writing_model.yaml/#using-a-pre-defined-dataloader","text":"If one of the ready-made dataloaders on kipoiseq fits the needs of your model, then please follow the instructions on kipoiseq . The default_dataloader in the model.yaml would then for example be: default_dataloader: defined_as: kipoiseq.dataloaders.SeqIntervalDl default_args: auto_resize_len: 1000 alphabet_axis: 0 dummy_axis: 1 dtype: float32","title":"Using a pre-defined dataloader"},{"location":"contributing/02_Writing_model.yaml/#using-a-custom-dataloader","text":"If you need a specialised dataloader you are encouraged to used as many methods and classes from within kipoiseq as possible as their functionality is tested. See more information on writing a dataloader and its companion yaml. Both of those files should lie in the same folder as the model.yaml . Then the default_dataloader entry in model.yaml is: default_dataloader: . It points to the location of the dataloader.yaml file. If dataloader.yaml lies in different subfolder then default_dataloader: path/to/folder would be used where dataloader.yaml would lie in folder .","title":"Using a custom dataloader"},{"location":"contributing/02_Writing_model.yaml/#schema","text":"Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are. schema contains two categories inputs and targets which each specify the shapes of the model input and model output. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A dictionary of numpy arrays as inputs or targets: schema: inputs: seq: shape: (1000,4) inp2: shape: (10)","title":"schema"},{"location":"contributing/02_Writing_model.yaml/#inputs","text":"The inputs fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input , not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction.","title":"inputs"},{"location":"contributing/02_Writing_model.yaml/#targets","text":"The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model input name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).","title":"targets"},{"location":"contributing/02_Writing_model.yaml/#how-model-types-handle-schemas","text":"The different model types handle those three different encapsulations of numpy arrays differently:","title":"How model types handle schemas"},{"location":"contributing/02_Writing_model.yaml/#kerasmodel-models","text":"","title":"KerasModel models"},{"location":"contributing/02_Writing_model.yaml/#input","text":"In case a Keras model is used the batch produced by the dataloader is passed on as it is to the model.predict_on_batch() function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to model.predict_on_batch() .","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output","text":"The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema.","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#pytorchmodel-models","text":"Pytorch needs torch.autograd.Variable instances to work. Hence all inputs are automatically converted into Variable objects and results are converted back into numpy arrays transparently. If cuda is available the model will automatically be used in cuda mode and also the input variables will be switched to cuda .","title":"PyTorchModel models"},{"location":"contributing/02_Writing_model.yaml/#input_1","text":"For prediction the following will happen to the tree different encapsulations of input arrays: A single array: Will be passed directly as the only argument to model call: model(Variable(from_numpy(x))) A list of arrays: The model will be called with the list of converted array as args (e.g.: model(*list_of_variables) ) A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: model(**dict_of_variables) )","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output_1","text":"The model return values will be converted back into encapsulations of numpy arrays, where: a single Variable object will be converted into a numpy arrays lists of Variable objects will be converted into a list of numpy arrays in the same order and","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#sklearnmodel-models","text":"The batch generated by the dataloader will be passed on directly to the SciKit-learn model using model.predict(x) , model.predict_proba(x) or model.predict_log_proba (depending on the predict_method argument).","title":"SklearnModel models"},{"location":"contributing/02_Writing_model.yaml/#tensorflowmodel-models","text":"","title":"TensorFlowModel models"},{"location":"contributing/02_Writing_model.yaml/#input_2","text":"The feed_dict for running a tensorflow session is generated by converting the batch samples into the feed_dict using input_nodes defined in the args section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays: If input_nodes is a single string the model will be fed with a dictionary {input_ops: x} If input_nodes is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that. If input_nodes is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that.","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output_2","text":"The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the schema field of model.yaml has to match that.","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#dependencies","text":"One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. If your model is a python-based model and you have not tested whether your model works in python 2 and python 3, then make sure that you also add the correct python version as a dependency e.g.: python=2.7 .","title":"dependencies"},{"location":"contributing/02_Writing_model.yaml/#conda","text":"Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used.","title":"conda"},{"location":"contributing/02_Writing_model.yaml/#pip","text":"Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"pip"},{"location":"contributing/02_Writing_model.yaml/#postprocessing","text":"The postprocessing section of a model.yaml is necessary to indicate that a model is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your model up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"postprocessing"},{"location":"contributing/03_Writing_dataloader.yaml/","text":"dataloader.yaml Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read Writing dataloader.py . To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example dataloader.yaml : defined_as: dataloader.MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: url: https://zenodo.org/path/to/example_files/features.csv # example file md5: 7a6s5d76as5d76a5sd7 targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: url: https://zenodo.org/path/to/example_files/targets.csv # example file md5: 76sd8f7687sd6fs68a67 optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: type: int doc: Just an example metadata column type The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values: PreloadedDataset Dataset BatchDataset SampleIterator SampleGenerator BatchIterator BatchGenerator defined_as defined_as indicates where the dataloader class can be found. It is a string value of file.ClassName where file refers to file file.py in the same directory as dataloader.yaml which contains the data-loader class ClassName . E.g.: dataloader.MyDataLoader . This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitly in args (see below). args A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.: args: reference_fasta: example: url: https://zenodo.org/path/to/example_files/chr22.fa md5: 765sadf876a argument_2: example: url: https://zenodo.org/path/to/example_files/example_input.txt md5: 786as8d7aasd An argument has the following fields: doc : A free text field describing the argument example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call kipoi test uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit the URLs of all necessary example files with the model. type : Optional: datatype of the argument ( str , bool , int , float ) default : This field is used to define external zenodo or figshare links that are automatically downloaded and assigned. See example below. optional : Optional: Boolean flag ( true / false ) for an argument if it is optional. If your dataloader requires an external data file at runtime which are not example/test files, you can specify these using the default attribute. default will override the default arguments of the dataloader init method (e.g. dataloader.MyDataloader.__init__ ). Example: defined_as: dataloader.MyDataset args: ... override_me: default: 10 essential_other_file: default: # download and replace with the path on the local filesystem url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a ... info The info field of a dataloader.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the dataloader. A short description of what it does. version : Version of the dataloader license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the dataloader and its use cases A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com doc: Datalaoder for my fancy model description version: 1.0 license: GNU tags: - TFBS - tag2 output_schema output_schema defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data. output_schema contains three categories inputs , targets and metadata . inputs and targets each specify the shapes of data generated for the model input and model. Offering the targets option enables the opportunity to possibly train models with the same dataloader. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: output_schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: output_schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A list of numpy arrays as inputs or targets: output_schema: inputs: seq: shape: (1000,4) inp2: shape: (10) inputs The inputs fields of output_schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input, not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction. associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated metadata field is called ranges then associated_metadata: ranges has to be set. targets The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model target name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line). metadata Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its associated_metadata flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are: doc : A free text description of the metadata element type : The datatype of the metadata field: str , int , float , array , GenomicRanges . Where the convenience class GenomicRanges is defined in kipoi.metadata , which is essentially an in-memory representation of a bed file. Definition of metadata is essential for postprocessing algorihms as variant effect prediction. Please refer to their detailed description for their requirements. An example of the defintion of dataloader.yaml with metadata can be seen here: output_schema: inputs: - name: seq shape: (1000,4) associated_metadata: my_ranges - name: inp2 shape: (10) ... metadata: my_ranges: type: GenomicRanges doc: Region from where inputs.seq was extracted dependencies One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. conda Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used. pip Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). postprocessing The postprocessing section of a dataloader.yaml is necessary to indicate that a dataloader is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your dataloader up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"dataloader.yaml"},{"location":"contributing/03_Writing_dataloader.yaml/#dataloaderyaml","text":"Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read Writing dataloader.py . To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example dataloader.yaml : defined_as: dataloader.MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: url: https://zenodo.org/path/to/example_files/features.csv # example file md5: 7a6s5d76as5d76a5sd7 targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: url: https://zenodo.org/path/to/example_files/targets.csv # example file md5: 76sd8f7687sd6fs68a67 optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: type: int doc: Just an example metadata column","title":"dataloader.yaml"},{"location":"contributing/03_Writing_dataloader.yaml/#type","text":"The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values: PreloadedDataset Dataset BatchDataset SampleIterator SampleGenerator BatchIterator BatchGenerator","title":"type"},{"location":"contributing/03_Writing_dataloader.yaml/#defined_as","text":"defined_as indicates where the dataloader class can be found. It is a string value of file.ClassName where file refers to file file.py in the same directory as dataloader.yaml which contains the data-loader class ClassName . E.g.: dataloader.MyDataLoader . This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitly in args (see below).","title":"defined_as"},{"location":"contributing/03_Writing_dataloader.yaml/#args","text":"A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.: args: reference_fasta: example: url: https://zenodo.org/path/to/example_files/chr22.fa md5: 765sadf876a argument_2: example: url: https://zenodo.org/path/to/example_files/example_input.txt md5: 786as8d7aasd An argument has the following fields: doc : A free text field describing the argument example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call kipoi test uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit the URLs of all necessary example files with the model. type : Optional: datatype of the argument ( str , bool , int , float ) default : This field is used to define external zenodo or figshare links that are automatically downloaded and assigned. See example below. optional : Optional: Boolean flag ( true / false ) for an argument if it is optional. If your dataloader requires an external data file at runtime which are not example/test files, you can specify these using the default attribute. default will override the default arguments of the dataloader init method (e.g. dataloader.MyDataloader.__init__ ). Example: defined_as: dataloader.MyDataset args: ... override_me: default: 10 essential_other_file: default: # download and replace with the path on the local filesystem url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a ...","title":"args"},{"location":"contributing/03_Writing_dataloader.yaml/#info","text":"The info field of a dataloader.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the dataloader. A short description of what it does. version : Version of the dataloader license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the dataloader and its use cases A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com doc: Datalaoder for my fancy model description version: 1.0 license: GNU tags: - TFBS - tag2","title":"info"},{"location":"contributing/03_Writing_dataloader.yaml/#output_schema","text":"output_schema defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data. output_schema contains three categories inputs , targets and metadata . inputs and targets each specify the shapes of data generated for the model input and model. Offering the targets option enables the opportunity to possibly train models with the same dataloader. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: output_schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: output_schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A list of numpy arrays as inputs or targets: output_schema: inputs: seq: shape: (1000,4) inp2: shape: (10)","title":"output_schema"},{"location":"contributing/03_Writing_dataloader.yaml/#inputs","text":"The inputs fields of output_schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input, not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction. associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated metadata field is called ranges then associated_metadata: ranges has to be set.","title":"inputs"},{"location":"contributing/03_Writing_dataloader.yaml/#targets","text":"The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model target name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).","title":"targets"},{"location":"contributing/03_Writing_dataloader.yaml/#metadata","text":"Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its associated_metadata flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are: doc : A free text description of the metadata element type : The datatype of the metadata field: str , int , float , array , GenomicRanges . Where the convenience class GenomicRanges is defined in kipoi.metadata , which is essentially an in-memory representation of a bed file. Definition of metadata is essential for postprocessing algorihms as variant effect prediction. Please refer to their detailed description for their requirements. An example of the defintion of dataloader.yaml with metadata can be seen here: output_schema: inputs: - name: seq shape: (1000,4) associated_metadata: my_ranges - name: inp2 shape: (10) ... metadata: my_ranges: type: GenomicRanges doc: Region from where inputs.seq was extracted","title":"metadata"},{"location":"contributing/03_Writing_dataloader.yaml/#dependencies","text":"One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future.","title":"dependencies"},{"location":"contributing/03_Writing_dataloader.yaml/#conda","text":"Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used.","title":"conda"},{"location":"contributing/03_Writing_dataloader.yaml/#pip","text":"Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"pip"},{"location":"contributing/03_Writing_dataloader.yaml/#postprocessing","text":"The postprocessing section of a dataloader.yaml is necessary to indicate that a dataloader is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your dataloader up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"postprocessing"},{"location":"contributing/04_Writing_dataloader.py/","text":"Dataloader Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys: inputs targets (optional) metadata (optional). As the names suggest, the inputs will get feeded to the model to make the predictions and targets could be used to train the model. The metadata field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models). In a batch of data returned by the dataloader, all three fields can be further nested - i.e. inputs can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays. Note that the inputs and targets have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the name fields in the keras.layers.Input layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array. Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield: individual samples batches of data whole dataset Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like bool , float , int , str . These will later get stacked into a one-dimensional numpy array. Dataloader types Specifically, a dataloader has to inherit from one of the following classes defined in kipoi.data : PreloadedDataset Function that returns the whole dataset as a nested dictionary/list of numpy arrays useful when: the dataset is expected to load quickly and fit into the memory Dataset Class that inherits from kipoi.data.Dataset and implements __len__ and __getitem__ methods. __getitem__ returns a single sample from the dataset. useful when: dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches BatchDataset Class that inherits from kipoi.data.BatchDataset and implements __len__ and __getitem__ methods. __getitem__ returns a single batch of samples from the dataset. useful when: dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches SampleIterator Class that inherits from kipoi.data.SampleIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches BatchIterator Class that inherits from kipoi.data.BatchIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single batch of samples sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches SampleGenerator A generator function that yields a single sample from the dataset and returns when all the samples were yielded. useful when: same as for SampleIterator , but can be typically implemented in fewer lines of code BatchGenerator A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded. useful when: same as for BatchIterator , but can be typically implemented in fewer lines of code Here is a table showing the (recommended) requirements for each dataloader type: Dataloader type Length known? Significant benefit from loading data in batches? Fits into memory and loads quickly? PreloadedDataset yes yes yes Dataset yes no no BatchDataset yes yes no SampleIterator no no no BatchIterator no yes no SampleGenerator no no no BatchGenerator no yes no Dataset example Here is an example dataloader that gets as input a fasta file and a bed file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges'). from __future__ import absolute_import, division, print_function import numpy as np from pybedtools import BedTool from genomelake.extractors import FastaExtractor from kipoi.data import Dataset from kipoi.metadata import GenomicRanges class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file): self.bt = BedTool(intervals_file) self.fasta_file = fasta_file self.fasta_extractor = None def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaExtractor(self.fasta_file) interval = self.bt[idx] seq = np.squeeze(self.fasta_extractor([interval]), axis=0) return { \"inputs\": seq, # lacks targets \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } } Since FastaExtractor is not multi-processing safe, we have initialized it on the first call of __getitem__ instead of __init__ . The reason for this is that when we use parallel dataloading, each process will get a copy of the SeqDataset(...) object. Upon the first call of __getitem__ the fasta_extractor and hence the underlying file-handle will be setup for each worker independently. Required static files If your dataloader requires an external data file as for example in tutorials/contributing_models , then the Kipoi way of automatically downloading and using that file is by adding an argument to the dataloader implementation: from __future__ import absolute_import, division, print_function from kipoi.data import Dataset class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file, essential_other_file): fh = open(essential_other_file, \"r\") ... Kipoi can automaticall download the required file from a zenodo or figshare url as if the url was defined as a default in the dataloader.yaml as follows: args: ... essential_other_file: default: url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a Further examples To see examples of other dataloaders, run kipoi init from the command-line and choose each time a different dataloader_type. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model ... Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: The generated model directory will contain a working implementation of a dataloader.","title":"dataloader.py"},{"location":"contributing/04_Writing_dataloader.py/#dataloader","text":"Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys: inputs targets (optional) metadata (optional). As the names suggest, the inputs will get feeded to the model to make the predictions and targets could be used to train the model. The metadata field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models). In a batch of data returned by the dataloader, all three fields can be further nested - i.e. inputs can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays. Note that the inputs and targets have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the name fields in the keras.layers.Input layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array. Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield: individual samples batches of data whole dataset Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like bool , float , int , str . These will later get stacked into a one-dimensional numpy array.","title":"Dataloader"},{"location":"contributing/04_Writing_dataloader.py/#dataloader-types","text":"Specifically, a dataloader has to inherit from one of the following classes defined in kipoi.data : PreloadedDataset Function that returns the whole dataset as a nested dictionary/list of numpy arrays useful when: the dataset is expected to load quickly and fit into the memory Dataset Class that inherits from kipoi.data.Dataset and implements __len__ and __getitem__ methods. __getitem__ returns a single sample from the dataset. useful when: dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches BatchDataset Class that inherits from kipoi.data.BatchDataset and implements __len__ and __getitem__ methods. __getitem__ returns a single batch of samples from the dataset. useful when: dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches SampleIterator Class that inherits from kipoi.data.SampleIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches BatchIterator Class that inherits from kipoi.data.BatchIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single batch of samples sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches SampleGenerator A generator function that yields a single sample from the dataset and returns when all the samples were yielded. useful when: same as for SampleIterator , but can be typically implemented in fewer lines of code BatchGenerator A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded. useful when: same as for BatchIterator , but can be typically implemented in fewer lines of code Here is a table showing the (recommended) requirements for each dataloader type: Dataloader type Length known? Significant benefit from loading data in batches? Fits into memory and loads quickly? PreloadedDataset yes yes yes Dataset yes no no BatchDataset yes yes no SampleIterator no no no BatchIterator no yes no SampleGenerator no no no BatchGenerator no yes no","title":"Dataloader types"},{"location":"contributing/04_Writing_dataloader.py/#dataset-example","text":"Here is an example dataloader that gets as input a fasta file and a bed file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges'). from __future__ import absolute_import, division, print_function import numpy as np from pybedtools import BedTool from genomelake.extractors import FastaExtractor from kipoi.data import Dataset from kipoi.metadata import GenomicRanges class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file): self.bt = BedTool(intervals_file) self.fasta_file = fasta_file self.fasta_extractor = None def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaExtractor(self.fasta_file) interval = self.bt[idx] seq = np.squeeze(self.fasta_extractor([interval]), axis=0) return { \"inputs\": seq, # lacks targets \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } } Since FastaExtractor is not multi-processing safe, we have initialized it on the first call of __getitem__ instead of __init__ . The reason for this is that when we use parallel dataloading, each process will get a copy of the SeqDataset(...) object. Upon the first call of __getitem__ the fasta_extractor and hence the underlying file-handle will be setup for each worker independently.","title":"Dataset example"},{"location":"contributing/04_Writing_dataloader.py/#required-static-files","text":"If your dataloader requires an external data file as for example in tutorials/contributing_models , then the Kipoi way of automatically downloading and using that file is by adding an argument to the dataloader implementation: from __future__ import absolute_import, division, print_function from kipoi.data import Dataset class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file, essential_other_file): fh = open(essential_other_file, \"r\") ... Kipoi can automaticall download the required file from a zenodo or figshare url as if the url was defined as a default in the dataloader.yaml as follows: args: ... essential_other_file: default: url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a","title":"Required static files"},{"location":"contributing/04_Writing_dataloader.py/#further-examples","text":"To see examples of other dataloaders, run kipoi init from the command-line and choose each time a different dataloader_type. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model ... Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: The generated model directory will contain a working implementation of a dataloader.","title":"Further examples"},{"location":"contributing/05_Writing_model.py/","text":"model.py Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type. Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of BaseModel defined in kipoi.model , which in other words means that def predict_on_batch(self, x) has to be implemented. So for example if batch is what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to run the model prediction on the given input. A very simple version of such a model definition that can be stored in for example model.py may be: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, file_path): ... self.model = load_model_parameters(file_path) # Execute model prediction for input data def predict_on_batch(self, x): return self.model.predict(x) This can then be integrated in the model.yaml in the following way: defined_as: model.MyModel args: file_path: # get model parameters from an url url: https://zenodo.org/path/to/my/architecture/file md5: .... ...","title":"model.py"},{"location":"contributing/05_Writing_model.py/#modelpy","text":"Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type. Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of BaseModel defined in kipoi.model , which in other words means that def predict_on_batch(self, x) has to be implemented. So for example if batch is what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to run the model prediction on the given input. A very simple version of such a model definition that can be stored in for example model.py may be: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, file_path): ... self.model = load_model_parameters(file_path) # Execute model prediction for input data def predict_on_batch(self, x): return self.model.predict(x) This can then be integrated in the model.yaml in the following way: defined_as: model.MyModel args: file_path: # get model parameters from an url url: https://zenodo.org/path/to/my/architecture/file md5: .... ...","title":"model.py"},{"location":"contributing/06_dumping_models_programatically/","text":"Contributing multiple very similar models To easily contribute model groups with multiple models of the same kind, you can specify two files describing all the models: model-template.yaml - template for model.yaml models.tsv - tab-separated files holding custom model variables First few lines of model-template.yaml : type: keras args: arch: url: {{ args_arch_url }} md5: {{ args_arch_md5 }} weights: url: {{ args_weights_url }} md5: {{ args_weights_md5 }} First few lines of models.tsv : model args_arch args_weights args_arch_md5 args_weights_md5 args_arch_url args_weights_url A549_ENCSR000DDI model_files/A549_ENCSR000DDI.json model_files/A549_ENCSR000DDI.h5 6d3a971ce766128ca444dd70ef76df70 f23198b146ad8e4d6755cb215fe75e0f https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI.h5?download=1 BE2C_ENCSR000DEB model_files/BE2C_ENCSR000DEB.json model_files/BE2C_ENCSR000DEB.h5 919b2f7f675bebb9217d95021d92af74 159ea3cb7985c08eab8f64151eb1799e https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB?download=1 https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB.h5?download=1 BJ_ENCSR000DEA model_files/BJ_ENCSR000DEA.json model_files/BJ_ENCSR000DEA.h5 6d3a971ce766128ca444dd70ef76df70 9ad8797caff0dd0e8274de6befded4e7 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/BJ_ENCSR000DEA.h5?download=1 CMK_ENCSR000DGJ model_files/CMK_ENCSR000DGJ.json model_files/CMK_ENCSR000DGJ.h5 6d3a971ce766128ca444dd70ef76df70 d5c0c9dd55f1056036cc300ec1f61e1d https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/CMK_ENCSR000DGJ.h5?download=1 One row in models.tsv will represent a single model and will be used to populate model-template.yaml and construct model.yaml using Jinja2 templating language . This allows you to even write if statements in model-template.yaml . See CpGenie model as an example.","title":"Multiple very similar models"},{"location":"contributing/06_dumping_models_programatically/#contributing-multiple-very-similar-models","text":"To easily contribute model groups with multiple models of the same kind, you can specify two files describing all the models: model-template.yaml - template for model.yaml models.tsv - tab-separated files holding custom model variables First few lines of model-template.yaml : type: keras args: arch: url: {{ args_arch_url }} md5: {{ args_arch_md5 }} weights: url: {{ args_weights_url }} md5: {{ args_weights_md5 }} First few lines of models.tsv : model args_arch args_weights args_arch_md5 args_weights_md5 args_arch_url args_weights_url A549_ENCSR000DDI model_files/A549_ENCSR000DDI.json model_files/A549_ENCSR000DDI.h5 6d3a971ce766128ca444dd70ef76df70 f23198b146ad8e4d6755cb215fe75e0f https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI.h5?download=1 BE2C_ENCSR000DEB model_files/BE2C_ENCSR000DEB.json model_files/BE2C_ENCSR000DEB.h5 919b2f7f675bebb9217d95021d92af74 159ea3cb7985c08eab8f64151eb1799e https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB?download=1 https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB.h5?download=1 BJ_ENCSR000DEA model_files/BJ_ENCSR000DEA.json model_files/BJ_ENCSR000DEA.h5 6d3a971ce766128ca444dd70ef76df70 9ad8797caff0dd0e8274de6befded4e7 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/BJ_ENCSR000DEA.h5?download=1 CMK_ENCSR000DGJ model_files/CMK_ENCSR000DGJ.json model_files/CMK_ENCSR000DGJ.h5 6d3a971ce766128ca444dd70ef76df70 d5c0c9dd55f1056036cc300ec1f61e1d https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/CMK_ENCSR000DGJ.h5?download=1 One row in models.tsv will represent a single model and will be used to populate model-template.yaml and construct model.yaml using Jinja2 templating language . This allows you to even write if statements in model-template.yaml . See CpGenie model as an example.","title":"Contributing multiple very similar models"},{"location":"tutorials/R-api/","text":"Generated from notebooks/R-api.ipynb Using Kipoi from R Thanks to the reticulate R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that. Make sure you have git-lfs and Kipoi correctly installed: Install git-lfs conda install -c conda-forge git-lfs && git lfs install (alternatively see https://git-lfs.github.com/ ) Install kipoi pip install kipoi Please read docs/using/getting started before going through this notebook. Install and load reticulate Make sure you have the reticulate R package installed # install.packages(\"reticulate\") library(reticulate) Reticulate quick intro In general, using Kipoi from R is almost the same as using it from Python: instead of using object.method() or object.attribute as in python, use $ : object$method() , object$attribute . # short reticulate example os <- import(\"os\") os$chdir(\"/tmp\") os$getcwd() '/tmp' Type mapping R <-> python Reticulate translates objects between R and python in the following way: R Python Examples Single-element vector Scalar 1 , 1L , TRUE , \"foo\" Multi-element vector List c(1.0, 2.0, 3.0) , c(1L, 2L, 3L) List of multiple types Tuple list(1L, TRUE, \"foo\") Named list Dict list(a = 1L, b = 2.0) , dict(x = x_data) Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2) Function Python function function(x) x + 1 NULL, TRUE, FALSE None, True, False NULL , TRUE , FALSE For more info on reticulate, please visit https://github.com/rstudio/reticulate/. Setup the python environment With reticulate::py_config() you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with use_condaenv(...) . This comes handy when using different models depending on different conda environments. reticulate::py_config() python: /home/avsec/bin/anaconda3/bin/python libpython: /home/avsec/bin/anaconda3/lib/libpython3.6m.so pythonhome: /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3 version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) [GCC 7.2.0] numpy: /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy numpy_version: 1.14.0 os: /home/avsec/bin/anaconda3/lib/python3.6/os.py python versions found: /home/avsec/bin/anaconda3/bin/python /usr/bin/python /usr/bin/python3 List all conda environments: reticulate::conda_list() Create a new conda environment for the model: $ kipoi env create HAL Use that environment in R: reticulate::use_condaenv(\"kipoi-HAL') Load kipoi kipoi <- import(\"kipoi\") List models kipoi$list_models()$head() source model version \\ 0 kipoi DeepSEAKeras 0.1 1 kipoi extended_coda 0.1 2 kipoi DeepCpG_DNA/Hou2016_mESC_dna 1.0.4 3 kipoi DeepCpG_DNA/Smallwood2014_2i_dna 1.0.4 4 kipoi DeepCpG_DNA/Hou2016_HepG2_dna 1.0.4 authors \\ 0 [Author(name='Jian Zhou', github=None, email=N... 1 [Author(name='Pang Wei Koh', github='kohpangwe... 2 [Author(name='Christof Angermueller', github='... 3 [Author(name='Christof Angermueller', github='... 4 [Author(name='Christof Angermueller', github='... contributors \\ 0 [Author(name='Lara Urban', github='LaraUrban',... 1 [Author(name='Johnny Israeli', github='jisrael... 2 [Author(name='Roman Kreuzhuber', github='krrom... 3 [Author(name='Roman Kreuzhuber', github='krrom... 4 [Author(name='Roman Kreuzhuber', github='krrom... doc type \\ 0 This CNN is based on the DeepSEA model from Zh... keras 1 Single bp resolution ChIP-seq denoising - http... keras 2 This is the extraction of the DNA-part of the ... keras 3 This is the extraction of the DNA-part of the ... keras 4 This is the extraction of the DNA-part of the ... keras inputs targets \\ 0 seq TFBS_DHS_probs 1 [H3K27AC_subsampled] [H3K27ac] 2 [dna] [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c... 3 [dna] [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ... 4 [dna] [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2... postproc_score_variants license \\ 0 True MIT 1 False MIT 2 True MIT 3 True MIT 4 True MIT cite_as \\ 0 https://doi.org/10.1038/nmeth.3547 1 https://doi.org/10.1093/bioinformatics/btx243 2 https://doi.org/10.1186/s13059-017-1189-z, htt... 3 https://doi.org/10.1186/s13059-017-1189-z, htt... 4 https://doi.org/10.1186/s13059-017-1189-z, htt... trained_on \\ 0 ENCODE and Roadmap Epigenomics chromatin profi... 1 Described in https://academic.oup.com/bioinfor... 2 scBS-seq and scRRBS-seq datasets, https://geno... 3 scBS-seq and scRRBS-seq datasets, https://geno... 4 scBS-seq and scRRBS-seq datasets, https://geno... training_procedure \\ 0 https://www.nature.com/articles/nmeth.3547#met... 1 Described in https://academic.oup.com/bioinfor... 2 Described in https://genomebiology.biomedcentr... 3 Described in https://genomebiology.biomedcentr... 4 Described in https://genomebiology.biomedcentr... tags 0 [Histone modification, DNA binding, DNA access... 1 [Histone modification] 2 [DNA methylation] 3 [DNA methylation] 4 [DNA methylation] reticulate currently doesn't support direct convertion from pandas.DataFrame to R's data.frame . Let's make a convenience function to create an R dataframe via matrix conversion. #' List models as an R data.frame kipoi_list_models <- function() { df_models <- kipoi$list_models() df <- data.frame(df_models$as_matrix()) colnames(df) = df_models$columns$tolist() return(df) } df <- kipoi_list_models() head(df, 2) source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags kipoi DeepSEAKeras 0.1 <environment: 0x556afc757e38> <environment: 0x556afbb0d538> This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras seq TFBS_DHS_probs TRUE MIT https://doi.org/10.1038/nmeth.3547 ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods https://www.nature.com/articles/nmeth.3547#methods <environment: 0x556afcddfd50> kipoi extended_coda 0.1 <environment: 0x556afc764260> <environment: 0x556afbaff708> Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda keras H3K27AC_subsampled H3K27ac FALSE MIT https://doi.org/10.1093/bioinformatics/btx243 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60> Get the kipoi model and make a prediction for the example files To run the following example, make sure you have all the dependencies installed. Run: kipoi$install_model_requirements(\"MaxEntScan/3prime\") from R or kipoi env install MaxEntScan/3prime from the command-line. This will install all the required dependencies for both, the model and the dataloader. kipoi$install_model_requirements(\"MaxEntScan/3prime\") model <- kipoi$get_model(\"MaxEntScan/3prime\") predictions <- model$pipeline$predict_example() head(predictions) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 -9.52033554891735 9.54342300799607 Use the model and dataloader independently # Get the dataloader setwd('~/.kipoi/models/MaxEntScan/3prime') dl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) it DataLoaderIter # Retrieve a batch of data batch <- iter_next(it) str(batch) List of 2 $ inputs : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\" $ metadata:List of 5 ..$ geneID : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\" ..$ biotype : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\" ..$ order : num [1:4(1d)] 0 0 1 2 ..$ ranges :List of 5 .. ..$ chr : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\" .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645 .. ..$ end : num [1:4(1d)] 16062813 16118933 16101494 16100668 .. ..$ id : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\" # make the prediction with a model model$predict_on_batch(batch$inputs) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 Troubleshooting Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain. Run the same code in python or CLI When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run $ kipoi test MaxEntScan/3prime --source=kipoi from the command-line first. Dependency issues It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run: $ kipoi env install MaxEntScan/3prime This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run $ kipoi env create MaxEntScan/3prime To use that environment in R, run: use_condaenv(\"kipoi-MaxEntScan__3prime\") Make sure you run that code snippet right after importing the reticulate library (i.e. make sure you run it before kipoi <- import('kipoi') ) Float/Double type issues When using a pytorch model: DeepSEA/predict kipoi$install_model_requirements(\"DeepSEA/predict\") # Get the dataloader setwd('~/.kipoi/models/DeepSEA/predict') model <- kipoi$get_model(\"DeepSEA/predict\") dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) # model$predict_on_batch(batch$inputs) We get an error: Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same This means that the feeded array is Double instead of Float. R arrays are by default converted to float64 numpy dtype: np <- import(\"numpy\", convert=FALSE) np$array(0.1)$dtype float64 np$array(batch$inputs)$dtype float64 To fix this, we need to explicitly convert them to float32 before passing the batch to the model: model$predict_on_batch(np$array(batch$inputs, dtype='float32')) 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651","title":"R API"},{"location":"tutorials/R-api/#using-kipoi-from-r","text":"Thanks to the reticulate R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that. Make sure you have git-lfs and Kipoi correctly installed: Install git-lfs conda install -c conda-forge git-lfs && git lfs install (alternatively see https://git-lfs.github.com/ ) Install kipoi pip install kipoi Please read docs/using/getting started before going through this notebook.","title":"Using Kipoi from R"},{"location":"tutorials/R-api/#install-and-load-reticulate","text":"Make sure you have the reticulate R package installed # install.packages(\"reticulate\") library(reticulate)","title":"Install and load reticulate"},{"location":"tutorials/R-api/#reticulate-quick-intro","text":"In general, using Kipoi from R is almost the same as using it from Python: instead of using object.method() or object.attribute as in python, use $ : object$method() , object$attribute . # short reticulate example os <- import(\"os\") os$chdir(\"/tmp\") os$getcwd() '/tmp'","title":"Reticulate quick intro"},{"location":"tutorials/R-api/#type-mapping-r-python","text":"Reticulate translates objects between R and python in the following way: R Python Examples Single-element vector Scalar 1 , 1L , TRUE , \"foo\" Multi-element vector List c(1.0, 2.0, 3.0) , c(1L, 2L, 3L) List of multiple types Tuple list(1L, TRUE, \"foo\") Named list Dict list(a = 1L, b = 2.0) , dict(x = x_data) Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2) Function Python function function(x) x + 1 NULL, TRUE, FALSE None, True, False NULL , TRUE , FALSE For more info on reticulate, please visit https://github.com/rstudio/reticulate/.","title":"Type mapping R &lt;-&gt; python"},{"location":"tutorials/R-api/#setup-the-python-environment","text":"With reticulate::py_config() you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with use_condaenv(...) . This comes handy when using different models depending on different conda environments. reticulate::py_config() python: /home/avsec/bin/anaconda3/bin/python libpython: /home/avsec/bin/anaconda3/lib/libpython3.6m.so pythonhome: /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3 version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) [GCC 7.2.0] numpy: /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy numpy_version: 1.14.0 os: /home/avsec/bin/anaconda3/lib/python3.6/os.py python versions found: /home/avsec/bin/anaconda3/bin/python /usr/bin/python /usr/bin/python3 List all conda environments: reticulate::conda_list() Create a new conda environment for the model: $ kipoi env create HAL Use that environment in R: reticulate::use_condaenv(\"kipoi-HAL')","title":"Setup the python environment"},{"location":"tutorials/R-api/#load-kipoi","text":"kipoi <- import(\"kipoi\")","title":"Load kipoi"},{"location":"tutorials/R-api/#list-models","text":"kipoi$list_models()$head() source model version \\ 0 kipoi DeepSEAKeras 0.1 1 kipoi extended_coda 0.1 2 kipoi DeepCpG_DNA/Hou2016_mESC_dna 1.0.4 3 kipoi DeepCpG_DNA/Smallwood2014_2i_dna 1.0.4 4 kipoi DeepCpG_DNA/Hou2016_HepG2_dna 1.0.4 authors \\ 0 [Author(name='Jian Zhou', github=None, email=N... 1 [Author(name='Pang Wei Koh', github='kohpangwe... 2 [Author(name='Christof Angermueller', github='... 3 [Author(name='Christof Angermueller', github='... 4 [Author(name='Christof Angermueller', github='... contributors \\ 0 [Author(name='Lara Urban', github='LaraUrban',... 1 [Author(name='Johnny Israeli', github='jisrael... 2 [Author(name='Roman Kreuzhuber', github='krrom... 3 [Author(name='Roman Kreuzhuber', github='krrom... 4 [Author(name='Roman Kreuzhuber', github='krrom... doc type \\ 0 This CNN is based on the DeepSEA model from Zh... keras 1 Single bp resolution ChIP-seq denoising - http... keras 2 This is the extraction of the DNA-part of the ... keras 3 This is the extraction of the DNA-part of the ... keras 4 This is the extraction of the DNA-part of the ... keras inputs targets \\ 0 seq TFBS_DHS_probs 1 [H3K27AC_subsampled] [H3K27ac] 2 [dna] [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c... 3 [dna] [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ... 4 [dna] [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2... postproc_score_variants license \\ 0 True MIT 1 False MIT 2 True MIT 3 True MIT 4 True MIT cite_as \\ 0 https://doi.org/10.1038/nmeth.3547 1 https://doi.org/10.1093/bioinformatics/btx243 2 https://doi.org/10.1186/s13059-017-1189-z, htt... 3 https://doi.org/10.1186/s13059-017-1189-z, htt... 4 https://doi.org/10.1186/s13059-017-1189-z, htt... trained_on \\ 0 ENCODE and Roadmap Epigenomics chromatin profi... 1 Described in https://academic.oup.com/bioinfor... 2 scBS-seq and scRRBS-seq datasets, https://geno... 3 scBS-seq and scRRBS-seq datasets, https://geno... 4 scBS-seq and scRRBS-seq datasets, https://geno... training_procedure \\ 0 https://www.nature.com/articles/nmeth.3547#met... 1 Described in https://academic.oup.com/bioinfor... 2 Described in https://genomebiology.biomedcentr... 3 Described in https://genomebiology.biomedcentr... 4 Described in https://genomebiology.biomedcentr... tags 0 [Histone modification, DNA binding, DNA access... 1 [Histone modification] 2 [DNA methylation] 3 [DNA methylation] 4 [DNA methylation] reticulate currently doesn't support direct convertion from pandas.DataFrame to R's data.frame . Let's make a convenience function to create an R dataframe via matrix conversion. #' List models as an R data.frame kipoi_list_models <- function() { df_models <- kipoi$list_models() df <- data.frame(df_models$as_matrix()) colnames(df) = df_models$columns$tolist() return(df) } df <- kipoi_list_models() head(df, 2) source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags kipoi DeepSEAKeras 0.1 <environment: 0x556afc757e38> <environment: 0x556afbb0d538> This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras seq TFBS_DHS_probs TRUE MIT https://doi.org/10.1038/nmeth.3547 ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods https://www.nature.com/articles/nmeth.3547#methods <environment: 0x556afcddfd50> kipoi extended_coda 0.1 <environment: 0x556afc764260> <environment: 0x556afbaff708> Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda keras H3K27AC_subsampled H3K27ac FALSE MIT https://doi.org/10.1093/bioinformatics/btx243 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60>","title":"List models"},{"location":"tutorials/R-api/#get-the-kipoi-model-and-make-a-prediction-for-the-example-files","text":"To run the following example, make sure you have all the dependencies installed. Run: kipoi$install_model_requirements(\"MaxEntScan/3prime\") from R or kipoi env install MaxEntScan/3prime from the command-line. This will install all the required dependencies for both, the model and the dataloader. kipoi$install_model_requirements(\"MaxEntScan/3prime\") model <- kipoi$get_model(\"MaxEntScan/3prime\") predictions <- model$pipeline$predict_example() head(predictions) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 -9.52033554891735 9.54342300799607","title":"Get the kipoi model and make a prediction for the example files"},{"location":"tutorials/R-api/#use-the-model-and-dataloader-independently","text":"# Get the dataloader setwd('~/.kipoi/models/MaxEntScan/3prime') dl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) it DataLoaderIter # Retrieve a batch of data batch <- iter_next(it) str(batch) List of 2 $ inputs : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\" $ metadata:List of 5 ..$ geneID : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\" ..$ biotype : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\" ..$ order : num [1:4(1d)] 0 0 1 2 ..$ ranges :List of 5 .. ..$ chr : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\" .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645 .. ..$ end : num [1:4(1d)] 16062813 16118933 16101494 16100668 .. ..$ id : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\" # make the prediction with a model model$predict_on_batch(batch$inputs) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451","title":"Use the model and dataloader independently"},{"location":"tutorials/R-api/#troubleshooting","text":"Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain.","title":"Troubleshooting"},{"location":"tutorials/R-api/#run-the-same-code-in-python-or-cli","text":"When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run $ kipoi test MaxEntScan/3prime --source=kipoi from the command-line first.","title":"Run the same code in python or CLI"},{"location":"tutorials/R-api/#dependency-issues","text":"It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run: $ kipoi env install MaxEntScan/3prime This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run $ kipoi env create MaxEntScan/3prime To use that environment in R, run: use_condaenv(\"kipoi-MaxEntScan__3prime\") Make sure you run that code snippet right after importing the reticulate library (i.e. make sure you run it before kipoi <- import('kipoi') )","title":"Dependency issues"},{"location":"tutorials/R-api/#floatdouble-type-issues","text":"When using a pytorch model: DeepSEA/predict kipoi$install_model_requirements(\"DeepSEA/predict\") # Get the dataloader setwd('~/.kipoi/models/DeepSEA/predict') model <- kipoi$get_model(\"DeepSEA/predict\") dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) # model$predict_on_batch(batch$inputs) We get an error: Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same This means that the feeded array is Double instead of Float. R arrays are by default converted to float64 numpy dtype: np <- import(\"numpy\", convert=FALSE) np$array(0.1)$dtype float64 np$array(batch$inputs)$dtype float64 To fix this, we need to explicitly convert them to float32 before passing the batch to the model: model$predict_on_batch(np$array(batch$inputs, dtype='float32')) 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651","title":"Float/Double type issues"},{"location":"tutorials/composing_models/","text":"Generated from notebooks/composing_models.ipynb Composing models by Ziga Avsec Composing models means that we take the predictions of some model and use it as input for another model like this: Three different scenarios can occur when we want to compose models from Kipoi: all models are written in the same framework (say Keras) models are written in different frameworks but can all be executed in the same python environment models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities All models in the same framework In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras. Let's first dump 4 dummy models: import keras.layers as kl from keras.models import Model from keras.models import load_model # create model 1 inp1 = kl.Input((3,), name=\"input1\") out1 = kl.Dense(4)(inp1) m1 = Model(inp1, out1) m1.save(\"/tmp/m1.h5\") # create model 2 inp2 = kl.Input((7,), name=\"input1_model1\") out2 = kl.Dense(3)(inp2) m2 = Model(inp2, out2) m2.save(\"/tmp/m2.h5\") # create model 3 inp3 = kl.Input((6,), name=\"input2\") out3 = kl.Dense(4)(inp3) m3 = Model(inp3, out3) m3.save(\"/tmp/m3.h5\") # create model 4 inp4 = kl.Input((7,), name=\"model2_model3\") out4 = kl.Dense(1)(inp4) m4 = Model(inp4, out4) m4.save(\"/tmp/m4.h5\") Next, we load the models back: ## Load models m1 = load_model(\"/tmp/m1.h5\") m2 = load_model(\"/tmp/m2.h5\") m3 = load_model(\"/tmp/m3.h5\") m4 = load_model(\"/tmp/m4.h5\") /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn('No training configuration found in save file: ' And compose them m2_in = kl.concatenate([m1.output, m1.input]) m2_out = m2(m2_in) m3_in = kl.concatenate([m2_out, m3.output]) out = m4(m3_in) m = Model(inputs=[m1.input, m3.input], outputs=out) from IPython.display import SVG from keras.utils.vis_utils import model_to_dot svg_img = model_to_dot(m, ).create(prog='dot', format='svg') SVG(svg_img) Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting m1.trainable = False . Contributing to Kipoi To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model. Models in different frameworks There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses keras=0.3 and and another one keras=2.0 ). Compatible dependencies To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model. def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1): m1 = kipoi.get_model(\"model1\") m2 = kipoi.get_model(\"model2\") m3 = kipoi.get_model(\"model3\") dl1 = m1.default_dataloader(**dl1_kwargs) dl2 = m1.default_dataloader(**dl2_kwargs) target_gen = get_target_gen(target_file) batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers) batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers) while True: batch1 = next(batch_it1)['inputs'] batch2 = next(batch_it2)['inputs'] targets, ids = next(target_gen) m1_pred = m1.predict_on_batch(batch1) m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1)) m3_pred = m3.predict_on_batch(batch2) yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \"targets\": targets, \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"], \"model3_id\": batch2[\"metadata\"][\"id\"], \"targets_id\": ids, } } # create model 4 inp2 = kl.Input((3,), name=\"model2\") inp3 = kl.Input((4,), name=\"model3\") x = kl.concatenate([inp2, inp3]) out4 = kl.Dense(1)(x) m4 = Model([inp2, inp3], out4) m4.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Train model4 def create_train_gen(**kwargs): while True: gen = new_dataloader(**kwargs) while True: batch = next(gen) yield (batch['inputs'], batch['targets']) train_gen = create_train_gen(...) m4.fit_generator(train_gen, ...) # Dump model4 m4.save(\"model_files/model.h5\") Incompatible dependencies Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk. Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake. Let's consider the following case: # Python part of the Snakefile import os import subprocess py_path = subprocess.check_output(['which', 'python']).decode().strip() env_paths = os.path.join(os.path.dirname(py_path), \"../envs\") def get_args(wildcards): \"\"\"Function returning a dictionary of dataloader kwargs for the corresponding model \"\"\" if wildcards.model == \"model3\": return {\"arg1\": 1} elif wildcards.model == \"model3\": return {\"\"} else: return {\"arg2\": 1} # Yaml part of the Snakefile rule all: inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"]) rule create_evironment: \"\"\"Create a new conda environment for each model\"\"\" output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\" rule run_predictions: \"\"\"Create a new conda environment for each model\"\"\" input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") output: \"predictions/{model}.h5\" params: dl_args: get_args batch_size: 15 threads: 8 shell: \"\"\" source activate kipoi-{wildcards.model} kipoi predict {wildcards.model} \\ -n {threads} \\ --dataloader_args='{params.dl_args}' \\ --batch_size={params.batch_size} \\ -f hdf5 \\ -o {output} \"\"\" This snakefile will generate the following hdf5 files predictions/model1.h5 predictions/model2.h5 To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions import deepdish def new_dataloader(model1_h5, model2_h5, target_file): d1 = deepdish.io.load(model1_h5) d2 = deepdish.io.load(model2_h5) targets = load_target_file(target_file) return { \"inputs\": { \"model1\": d1[\"predictions\"], \"model2\": d2[\"predictions\"], }, \"targets\": targets, \"metadata\": { \"model1_id\": d1[\"metdata\"][\"id\"], \"model2_id\": d2[\"metdata\"][\"id\"], } } # get the training data ... data_train = new_dataloader(\"predictions/model1.h5\" \"predictions/model1.h5\", \"target_file.h5\") # train the model... m4.fit(data_train['inputs'], data_train['targets']) # Dump the model m4.save(\"model_files/model.h5\") Uploading composite models to Kipoi Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so: All models in the same framework Dataloader: newly written, combines dataloaders Model: combines models by stitching them together in the framework Different frameworks, compatible dependencies Dataloader: newly written, combines dataloaders and models Model: final ensembling model (model 4) Different frameworks, in-compatible dependencies Dataloader: newly written, loads data from the hdf5 files containing model predictions Model: final ensembling model (model 4)","title":"Composing models"},{"location":"tutorials/composing_models/#composing-models","text":"by Ziga Avsec Composing models means that we take the predictions of some model and use it as input for another model like this: Three different scenarios can occur when we want to compose models from Kipoi: all models are written in the same framework (say Keras) models are written in different frameworks but can all be executed in the same python environment models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities","title":"Composing models"},{"location":"tutorials/composing_models/#all-models-in-the-same-framework","text":"In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras. Let's first dump 4 dummy models: import keras.layers as kl from keras.models import Model from keras.models import load_model # create model 1 inp1 = kl.Input((3,), name=\"input1\") out1 = kl.Dense(4)(inp1) m1 = Model(inp1, out1) m1.save(\"/tmp/m1.h5\") # create model 2 inp2 = kl.Input((7,), name=\"input1_model1\") out2 = kl.Dense(3)(inp2) m2 = Model(inp2, out2) m2.save(\"/tmp/m2.h5\") # create model 3 inp3 = kl.Input((6,), name=\"input2\") out3 = kl.Dense(4)(inp3) m3 = Model(inp3, out3) m3.save(\"/tmp/m3.h5\") # create model 4 inp4 = kl.Input((7,), name=\"model2_model3\") out4 = kl.Dense(1)(inp4) m4 = Model(inp4, out4) m4.save(\"/tmp/m4.h5\") Next, we load the models back: ## Load models m1 = load_model(\"/tmp/m1.h5\") m2 = load_model(\"/tmp/m2.h5\") m3 = load_model(\"/tmp/m3.h5\") m4 = load_model(\"/tmp/m4.h5\") /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn('No training configuration found in save file: ' And compose them m2_in = kl.concatenate([m1.output, m1.input]) m2_out = m2(m2_in) m3_in = kl.concatenate([m2_out, m3.output]) out = m4(m3_in) m = Model(inputs=[m1.input, m3.input], outputs=out) from IPython.display import SVG from keras.utils.vis_utils import model_to_dot svg_img = model_to_dot(m, ).create(prog='dot', format='svg') SVG(svg_img) Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting m1.trainable = False .","title":"All models in the same framework"},{"location":"tutorials/composing_models/#contributing-to-kipoi","text":"To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model.","title":"Contributing to Kipoi"},{"location":"tutorials/composing_models/#models-in-different-frameworks","text":"There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses keras=0.3 and and another one keras=2.0 ).","title":"Models in different frameworks"},{"location":"tutorials/composing_models/#compatible-dependencies","text":"To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model. def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1): m1 = kipoi.get_model(\"model1\") m2 = kipoi.get_model(\"model2\") m3 = kipoi.get_model(\"model3\") dl1 = m1.default_dataloader(**dl1_kwargs) dl2 = m1.default_dataloader(**dl2_kwargs) target_gen = get_target_gen(target_file) batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers) batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers) while True: batch1 = next(batch_it1)['inputs'] batch2 = next(batch_it2)['inputs'] targets, ids = next(target_gen) m1_pred = m1.predict_on_batch(batch1) m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1)) m3_pred = m3.predict_on_batch(batch2) yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \"targets\": targets, \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"], \"model3_id\": batch2[\"metadata\"][\"id\"], \"targets_id\": ids, } } # create model 4 inp2 = kl.Input((3,), name=\"model2\") inp3 = kl.Input((4,), name=\"model3\") x = kl.concatenate([inp2, inp3]) out4 = kl.Dense(1)(x) m4 = Model([inp2, inp3], out4) m4.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Train model4 def create_train_gen(**kwargs): while True: gen = new_dataloader(**kwargs) while True: batch = next(gen) yield (batch['inputs'], batch['targets']) train_gen = create_train_gen(...) m4.fit_generator(train_gen, ...) # Dump model4 m4.save(\"model_files/model.h5\")","title":"Compatible dependencies"},{"location":"tutorials/composing_models/#incompatible-dependencies","text":"Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk. Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake. Let's consider the following case: # Python part of the Snakefile import os import subprocess py_path = subprocess.check_output(['which', 'python']).decode().strip() env_paths = os.path.join(os.path.dirname(py_path), \"../envs\") def get_args(wildcards): \"\"\"Function returning a dictionary of dataloader kwargs for the corresponding model \"\"\" if wildcards.model == \"model3\": return {\"arg1\": 1} elif wildcards.model == \"model3\": return {\"\"} else: return {\"arg2\": 1} # Yaml part of the Snakefile rule all: inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"]) rule create_evironment: \"\"\"Create a new conda environment for each model\"\"\" output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\" rule run_predictions: \"\"\"Create a new conda environment for each model\"\"\" input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") output: \"predictions/{model}.h5\" params: dl_args: get_args batch_size: 15 threads: 8 shell: \"\"\" source activate kipoi-{wildcards.model} kipoi predict {wildcards.model} \\ -n {threads} \\ --dataloader_args='{params.dl_args}' \\ --batch_size={params.batch_size} \\ -f hdf5 \\ -o {output} \"\"\" This snakefile will generate the following hdf5 files predictions/model1.h5 predictions/model2.h5 To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions import deepdish def new_dataloader(model1_h5, model2_h5, target_file): d1 = deepdish.io.load(model1_h5) d2 = deepdish.io.load(model2_h5) targets = load_target_file(target_file) return { \"inputs\": { \"model1\": d1[\"predictions\"], \"model2\": d2[\"predictions\"], }, \"targets\": targets, \"metadata\": { \"model1_id\": d1[\"metdata\"][\"id\"], \"model2_id\": d2[\"metdata\"][\"id\"], } } # get the training data ... data_train = new_dataloader(\"predictions/model1.h5\" \"predictions/model1.h5\", \"target_file.h5\") # train the model... m4.fit(data_train['inputs'], data_train['targets']) # Dump the model m4.save(\"model_files/model.h5\")","title":"Incompatible dependencies"},{"location":"tutorials/composing_models/#uploading-composite-models-to-kipoi","text":"Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so: All models in the same framework Dataloader: newly written, combines dataloaders Model: combines models by stitching them together in the framework Different frameworks, compatible dependencies Dataloader: newly written, combines dataloaders and models Model: final ensembling model (model 4) Different frameworks, in-compatible dependencies Dataloader: newly written, loads data from the hdf5 files containing model predictions Model: final ensembling model (model 4)","title":"Uploading composite models to Kipoi"},{"location":"tutorials/contributing_models/","text":"Generated from notebooks/contributing_models.ipynb Contributing a model to the Kipoi model repository This notebook will show you how to contribute a model to the Kipoi model repository . For a simple 'model contribution checklist' see also http://kipoi.org/docs/contributing/01_Getting_started/ . Kipoi basics Contributing a model to Kipoi means writing a sub-folder with all the required files to the Kipoi model repository via pull request. Two main components of the model repository are model and dataloader . Model Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the predict_on_batch(x) method, where x is dictionary/list of numpy arrays. The model contributor needs to provide one of the following: Serialized Keras model Serialized Sklearn model Custom model inheriting from keras.model.BaseModel . all the required files, i.e. weights need to be loaded in the __init__ See http://kipoi.org/docs/contributing/02_Writing_model.yaml/ and http://kipoi.org/docs/contributing/05_Writing_model.py/ for more info. Dataloader Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. Before writing your own dataloader take a look at our kipoiseq repository to see whether your use-case is covered by the available dataloaders. Writing your own dataloader Technically, dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following: PreloadedDataset Dataset BatchDataset SampleIterator BatchIterator SampleGenerator BatchGenerator See http://kipoi.org/docs/contributing/04_Writing_dataloader.py/ for more info. Folder layout Here is an example folder structure of a Kipoi model: MyModel \u251c\u2500\u2500 dataloader.py # implements the dataloader (only necessary if you wrote your own dataloader) \u251c\u2500\u2500 dataloader.yaml # describes the dataloader (only necessary if you wrote your own dataloader) \u2514\u2500\u2500 model.yaml # describes the model The model.yaml and dataloader.yaml files a complete description about the model, the dataloader and the files they depend on. Contributing a simple Iris-classifier Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known Iris dataset. Outline Train the model Generate the model directory Store all data files required for the model and the dataloader in a temporary folder Write model.yaml Write dataloader.yaml Write dataloader.py Test with the model with $ kipoi test . Publish data files on zenodo Update model.yaml and dataloader.yaml to contain the links Test again Commit, push and generate a pull request 1. Train the model Load and pre-process the data import pandas as pd import os from sklearn.preprocessing import LabelBinarizer, StandardScaler from sklearn import datasets iris = datasets.load_iris() # view more info about the dataset # print(iris[\"DESCR\"]) # Data pre-processing y_transformer = LabelBinarizer().fit(iris[\"target\"]) x_transformer = StandardScaler().fit(iris[\"data\"]) x = x_transformer.transform(iris[\"data\"]) y = y_transformer.transform(iris[\"target\"]) x[:3] array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]]) y[:3] array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]) Train an example model Let's train a simple linear-regression model using Keras. from keras.models import Model import keras.layers as kl inp = kl.Input(shape=(4, ), name=\"features\") out = kl.Dense(units=3)(inp) model = Model(inp, out) model.compile(\"adam\", \"categorical_crossentropy\") model.fit(x, y, verbose=0) Using TensorFlow backend. WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead <keras.callbacks.History at 0x2ab58e8ba860> 2. Set the model directory up: In reality, you would also need to Fork the kipoi/models repository Clone your repository fork, ignoring all the git-lfs files $ git clone git@github.com:<your_username>/models.git Create a new folder <mynewmodel> 3. Store the files in a temporary directory All the data of the model will have to be published on zenodo or figshare before the pull request is performed. While setting the Kipoi model up, it is handy the keep the models in a temporary directory in the model folder, which we will delete prior to the pull request. # create the model directory !mkdir contribution_sample_model # create the temporary directory where we will keep the files that should later be published in zenodo or figshare !mkdir contribution_sample_model/tmp Now we can change the current working directory to the model directory: import os os.chdir(\"contribution_sample_model\") 3a. Static files for dataloader Since in our case here we require to write a new dataloader. The dataloader can use some trained transformer instances (here the LabelBinarizer and StandardScaler transformers form sklearn). These should be uploaded with the model files and then referenced correctly in the dataloader.yaml file. We will store the required files in the temporary folder: import pickle with open(\"tmp/y_transformer.pkl\", \"wb\") as f: pickle.dump(y_transformer, f, protocol=2) with open(\"tmp/x_transformer.pkl\", \"wb\") as f: pickle.dump(x_transformer, f, protocol=2) ! ls tmp x_transformer.pkl y_transformer.pkl 3b. Model definition / weights Now that we have the static files that are required by the dataloader, we also need to store the model architecture and weights: # Architecture with open(\"tmp/model.json\", \"w\") as f: f.write(model.to_json()) # Weights model.save_weights(\"tmp/weights.h5\") Alternatively if we would be using a scikit-learn model we would save the pickle file: # Alternatively, for the scikit-learn model we would save the pickle file from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier lr = OneVsRestClassifier(LogisticRegression()) lr.fit(x, y) with open(\"tmp/sklearn_model.pkl\", \"wb\") as f: pickle.dump(lr, f, protocol=2) 3c. Example files for the dataloader Every Kipoi dataloader has to provide a set of example files so that Kipoi can perform its automated tests and users can have an idea what the dataloader files have to look like. Again we will store the files in the temporary folder: # select first 20 rows of the iris dataset X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"]) y = pd.DataFrame({\"class\": iris[\"target\"][:20]}) # store the model input features and targets as csv files with column names: X.to_csv(\"tmp/example_features.csv\", index=False) y.to_csv(\"tmp/example_targets.csv\", index=False) 4 Write the model.yaml Now it is time to write the model.yaml in the model directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. model_yaml = \"\"\" defined_as: kipoi.model.KerasModel # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: tmp/model.json weights: tmp/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" \"\"\" with open(\"model.yaml\", \"w\") as ofh: ofh.write(model_yaml) 5 and 6 Write the dataloader.yaml and dataloader.py PLEASE REMEMBER: Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in kipoiseq and use those as explained in the Kipoi docs. Now it is time to write the dataloader.yaml . Since we defined the default_dataloader field in model.yaml as . Kipoi will expect that our dataloader.yaml file lies in the same directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. dataloader_yaml = \"\"\" type: Dataset defined_as: dataloader.MyDataset args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: tmp/example_features.csv # example files x_transformer: default: tmp/x_transformer.pkl #default: # url: https://github.com/kipoi/kipoi/raw/57734d716b8dedaffe460855e7cfe8f37ec2d48d/example/models/sklearn_iris/dataloader_files/x_transformer.pkl # md5: bc1bf3c61c418b2d07506a7d0521a893 y_transformer: default: tmp/y_transformer.pkl targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: tmp/example_targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3, ) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: doc: Just an example metadata column \"\"\" with open(\"dataloader.yaml\", \"w\") as ofh: ofh.write(dataloader_yaml) Since we have referred to the dataloader as dataloader.MyDataset we expect a dataloader.py file in the same directory as dataloader.yaml which has to contain the dataloader class, which is here MyDataset . Notice that external static files are arguments to the __init__ function! Their path was defined in the dataloader.yaml . import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } In order to elucidate what the Dataloader class does I will make a few function calls that are usually performed by the Kipoi API in order to generate model input: # instantiate the dataloader ds = MyDataset(\"tmp/example_features.csv\", \"tmp/example_targets.csv\", \"tmp/x_transformer.pkl\", \"tmp/y_transformer.pkl\") # call __getitem__ ds[5] {'inputs': {'features': array([-0.53717756, 1.95766909, -1.17067529, -1.05003079])}, 'targets': array([1, 0, 0]), 'metadata': {'example_row_number': 5}} it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2) next(it) {'inputs': {'features': array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]])}, 'targets': array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]), 'metadata': {'example_row_number': array([0, 1, 2])}} I will now store the code from above in a file so that we can test it: dataloader_py = \"\"\" import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } \"\"\" with open(\"dataloader.py\", \"w\") as ofh: ofh.write(dataloader_py) 7 Test the model Now it is time to test the model. !kipoi test . \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/opt/model-zoo/notebooks/contribution_sample_model/dataloader.MyDataset\u001b[0m Using TensorFlow backend. 2018-10-11 17:41:58.586759: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='tmp/model.json' mode='r' encoding='UTF-8'>\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from tmp/weights.h5\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 28.88it/s] \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.main]\u001b[0m Successfully ran test_predict\u001b[0m 8. Publish data on zenodo or figshare Now that the model works It is time to upload the data files onto zenodo or figshare. To do so follow the instructions on the website. It might be necessary to remove file suffixes in order to be able to load the respective files. 9 Update model.yaml and dataloader.yaml Now the local file paths in model.yaml and dataloader.yaml have to be replaced by the zenodo / figshare URLs in the following way. The entry: args: ... x_transformer: default: tmp/x_transformer.pkl would be replaced by: args: ... x_transformer: default: url: https://zenodo.org/path/to/example_files/x_transformer.pkl md5: 76a5sd76asd57 So every local path has to be replaced by the url and md5 combination. Where md5 is the md5 sum of the file. If you cannot find the the md5 sum on the zenodo / figshare website you can for example run curl https://zenodo.org/.../x_transformer.pkl | md5sum to calculate the md5 sum. Now after replacing all the files, test the setup again by running kipoi test . and then delete the tmp folder. Now the only file(s) remaining in the folder should be model.yaml (and in this case also: dataloader.py dataloader.yaml ). 9 Test again Now that you have deleted the temporary files, rerun the test to make sure everything works fine. 10 Commit and push Now commit the model.yaml and if needed (like in this example) also the dataloader.py and datalaoder.yaml files by running: git add model.yaml . Now you can push back to your fork ( git push ) and submit a pull request with kipoi/models to request adding your model to the Kipoi models. Accessing local models through kipoi In Kipoi it is not necessary to publish your model. You can leverage the full functionality of Kipoi also for local models. All you have to do is specify --source dir when using the CLI or setting source=\"dir\" in the python API. The model name is then the local path to the model folder. import kipoi m = kipoi.get_model(\".\", source=\"dir\") # See also python-sdk.ipynb m.pipeline.predict({\"features_file\": \"tmp/example_features.csv\", \"targets_file\": \"tmp/example_targets.csv\" })[:5] 0it [00:00, ?it/s]\u001b[A 1it [00:00, 19.03it/s]\u001b[A array([[ 3.2324865 , -0.29753828, 0.62135816], [ 2.8549244 , 0.4957999 , 0.6873083 ], [ 3.2744825 , 0.40906954, 0.99161 ], [ 3.1413555 , 0.58123374, 1.0272367 ], [ 3.416262 , -0.34901416, 0.76257455]], dtype=float32) m.info ModelInfo(authors=[Author(name='Your Name', github='your_github_username', email='your_email@host.org')], doc='Model predicting the Iris species', name=None, version='0.1', license='MIT', tags=[], contributors=[], cite_as='https://doi.org:/...', trained_on='Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris)', training_procedure=None) m.default_dataloader dataloader.MyDataset m.model <keras.engine.training.Model at 0x2ab5a3eff668> m.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x2ab5a2d75160>> Recap Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use kipoi init to setup a model directory. Make sure you have read the getting started guide for contributing models.","title":"Contributing models"},{"location":"tutorials/contributing_models/#contributing-a-model-to-the-kipoi-model-repository","text":"This notebook will show you how to contribute a model to the Kipoi model repository . For a simple 'model contribution checklist' see also http://kipoi.org/docs/contributing/01_Getting_started/ .","title":"Contributing a model to the Kipoi model repository"},{"location":"tutorials/contributing_models/#kipoi-basics","text":"Contributing a model to Kipoi means writing a sub-folder with all the required files to the Kipoi model repository via pull request. Two main components of the model repository are model and dataloader .","title":"Kipoi basics"},{"location":"tutorials/contributing_models/#model","text":"Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the predict_on_batch(x) method, where x is dictionary/list of numpy arrays. The model contributor needs to provide one of the following: Serialized Keras model Serialized Sklearn model Custom model inheriting from keras.model.BaseModel . all the required files, i.e. weights need to be loaded in the __init__ See http://kipoi.org/docs/contributing/02_Writing_model.yaml/ and http://kipoi.org/docs/contributing/05_Writing_model.py/ for more info.","title":"Model"},{"location":"tutorials/contributing_models/#dataloader","text":"Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. Before writing your own dataloader take a look at our kipoiseq repository to see whether your use-case is covered by the available dataloaders.","title":"Dataloader"},{"location":"tutorials/contributing_models/#writing-your-own-dataloader","text":"Technically, dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following: PreloadedDataset Dataset BatchDataset SampleIterator BatchIterator SampleGenerator BatchGenerator See http://kipoi.org/docs/contributing/04_Writing_dataloader.py/ for more info.","title":"Writing your own dataloader"},{"location":"tutorials/contributing_models/#folder-layout","text":"Here is an example folder structure of a Kipoi model: MyModel \u251c\u2500\u2500 dataloader.py # implements the dataloader (only necessary if you wrote your own dataloader) \u251c\u2500\u2500 dataloader.yaml # describes the dataloader (only necessary if you wrote your own dataloader) \u2514\u2500\u2500 model.yaml # describes the model The model.yaml and dataloader.yaml files a complete description about the model, the dataloader and the files they depend on.","title":"Folder layout"},{"location":"tutorials/contributing_models/#contributing-a-simple-iris-classifier","text":"Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known Iris dataset.","title":"Contributing a simple Iris-classifier"},{"location":"tutorials/contributing_models/#outline","text":"Train the model Generate the model directory Store all data files required for the model and the dataloader in a temporary folder Write model.yaml Write dataloader.yaml Write dataloader.py Test with the model with $ kipoi test . Publish data files on zenodo Update model.yaml and dataloader.yaml to contain the links Test again Commit, push and generate a pull request","title":"Outline"},{"location":"tutorials/contributing_models/#1-train-the-model","text":"","title":"1. Train the model"},{"location":"tutorials/contributing_models/#load-and-pre-process-the-data","text":"import pandas as pd import os from sklearn.preprocessing import LabelBinarizer, StandardScaler from sklearn import datasets iris = datasets.load_iris() # view more info about the dataset # print(iris[\"DESCR\"]) # Data pre-processing y_transformer = LabelBinarizer().fit(iris[\"target\"]) x_transformer = StandardScaler().fit(iris[\"data\"]) x = x_transformer.transform(iris[\"data\"]) y = y_transformer.transform(iris[\"target\"]) x[:3] array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]]) y[:3] array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])","title":"Load and pre-process the data"},{"location":"tutorials/contributing_models/#train-an-example-model","text":"Let's train a simple linear-regression model using Keras. from keras.models import Model import keras.layers as kl inp = kl.Input(shape=(4, ), name=\"features\") out = kl.Dense(units=3)(inp) model = Model(inp, out) model.compile(\"adam\", \"categorical_crossentropy\") model.fit(x, y, verbose=0) Using TensorFlow backend. WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead <keras.callbacks.History at 0x2ab58e8ba860>","title":"Train an example model"},{"location":"tutorials/contributing_models/#2-set-the-model-directory-up","text":"In reality, you would also need to Fork the kipoi/models repository Clone your repository fork, ignoring all the git-lfs files $ git clone git@github.com:<your_username>/models.git Create a new folder <mynewmodel>","title":"2. Set the model directory up:"},{"location":"tutorials/contributing_models/#3-store-the-files-in-a-temporary-directory","text":"All the data of the model will have to be published on zenodo or figshare before the pull request is performed. While setting the Kipoi model up, it is handy the keep the models in a temporary directory in the model folder, which we will delete prior to the pull request. # create the model directory !mkdir contribution_sample_model # create the temporary directory where we will keep the files that should later be published in zenodo or figshare !mkdir contribution_sample_model/tmp Now we can change the current working directory to the model directory: import os os.chdir(\"contribution_sample_model\")","title":"3. Store the files in a temporary directory"},{"location":"tutorials/contributing_models/#3a-static-files-for-dataloader","text":"Since in our case here we require to write a new dataloader. The dataloader can use some trained transformer instances (here the LabelBinarizer and StandardScaler transformers form sklearn). These should be uploaded with the model files and then referenced correctly in the dataloader.yaml file. We will store the required files in the temporary folder: import pickle with open(\"tmp/y_transformer.pkl\", \"wb\") as f: pickle.dump(y_transformer, f, protocol=2) with open(\"tmp/x_transformer.pkl\", \"wb\") as f: pickle.dump(x_transformer, f, protocol=2) ! ls tmp x_transformer.pkl y_transformer.pkl","title":"3a. Static files for dataloader"},{"location":"tutorials/contributing_models/#3b-model-definition-weights","text":"Now that we have the static files that are required by the dataloader, we also need to store the model architecture and weights: # Architecture with open(\"tmp/model.json\", \"w\") as f: f.write(model.to_json()) # Weights model.save_weights(\"tmp/weights.h5\") Alternatively if we would be using a scikit-learn model we would save the pickle file: # Alternatively, for the scikit-learn model we would save the pickle file from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier lr = OneVsRestClassifier(LogisticRegression()) lr.fit(x, y) with open(\"tmp/sklearn_model.pkl\", \"wb\") as f: pickle.dump(lr, f, protocol=2)","title":"3b. Model definition / weights"},{"location":"tutorials/contributing_models/#3c-example-files-for-the-dataloader","text":"Every Kipoi dataloader has to provide a set of example files so that Kipoi can perform its automated tests and users can have an idea what the dataloader files have to look like. Again we will store the files in the temporary folder: # select first 20 rows of the iris dataset X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"]) y = pd.DataFrame({\"class\": iris[\"target\"][:20]}) # store the model input features and targets as csv files with column names: X.to_csv(\"tmp/example_features.csv\", index=False) y.to_csv(\"tmp/example_targets.csv\", index=False)","title":"3c. Example files for the dataloader"},{"location":"tutorials/contributing_models/#4-write-the-modelyaml","text":"Now it is time to write the model.yaml in the model directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. model_yaml = \"\"\" defined_as: kipoi.model.KerasModel # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: tmp/model.json weights: tmp/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" \"\"\" with open(\"model.yaml\", \"w\") as ofh: ofh.write(model_yaml)","title":"4 Write the model.yaml"},{"location":"tutorials/contributing_models/#5-and-6-write-the-dataloaderyaml-and-dataloaderpy","text":"PLEASE REMEMBER: Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in kipoiseq and use those as explained in the Kipoi docs. Now it is time to write the dataloader.yaml . Since we defined the default_dataloader field in model.yaml as . Kipoi will expect that our dataloader.yaml file lies in the same directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. dataloader_yaml = \"\"\" type: Dataset defined_as: dataloader.MyDataset args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: tmp/example_features.csv # example files x_transformer: default: tmp/x_transformer.pkl #default: # url: https://github.com/kipoi/kipoi/raw/57734d716b8dedaffe460855e7cfe8f37ec2d48d/example/models/sklearn_iris/dataloader_files/x_transformer.pkl # md5: bc1bf3c61c418b2d07506a7d0521a893 y_transformer: default: tmp/y_transformer.pkl targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: tmp/example_targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3, ) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: doc: Just an example metadata column \"\"\" with open(\"dataloader.yaml\", \"w\") as ofh: ofh.write(dataloader_yaml) Since we have referred to the dataloader as dataloader.MyDataset we expect a dataloader.py file in the same directory as dataloader.yaml which has to contain the dataloader class, which is here MyDataset . Notice that external static files are arguments to the __init__ function! Their path was defined in the dataloader.yaml . import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } In order to elucidate what the Dataloader class does I will make a few function calls that are usually performed by the Kipoi API in order to generate model input: # instantiate the dataloader ds = MyDataset(\"tmp/example_features.csv\", \"tmp/example_targets.csv\", \"tmp/x_transformer.pkl\", \"tmp/y_transformer.pkl\") # call __getitem__ ds[5] {'inputs': {'features': array([-0.53717756, 1.95766909, -1.17067529, -1.05003079])}, 'targets': array([1, 0, 0]), 'metadata': {'example_row_number': 5}} it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2) next(it) {'inputs': {'features': array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]])}, 'targets': array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]), 'metadata': {'example_row_number': array([0, 1, 2])}} I will now store the code from above in a file so that we can test it: dataloader_py = \"\"\" import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } \"\"\" with open(\"dataloader.py\", \"w\") as ofh: ofh.write(dataloader_py)","title":"5 and 6 Write the dataloader.yaml and dataloader.py"},{"location":"tutorials/contributing_models/#7-test-the-model","text":"Now it is time to test the model. !kipoi test . \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/opt/model-zoo/notebooks/contribution_sample_model/dataloader.MyDataset\u001b[0m Using TensorFlow backend. 2018-10-11 17:41:58.586759: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='tmp/model.json' mode='r' encoding='UTF-8'>\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from tmp/weights.h5\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 28.88it/s] \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.main]\u001b[0m Successfully ran test_predict\u001b[0m","title":"7 Test the model"},{"location":"tutorials/contributing_models/#8-publish-data-on-zenodo-or-figshare","text":"Now that the model works It is time to upload the data files onto zenodo or figshare. To do so follow the instructions on the website. It might be necessary to remove file suffixes in order to be able to load the respective files.","title":"8. Publish data on zenodo or figshare"},{"location":"tutorials/contributing_models/#9-update-modelyaml-and-dataloaderyaml","text":"Now the local file paths in model.yaml and dataloader.yaml have to be replaced by the zenodo / figshare URLs in the following way. The entry: args: ... x_transformer: default: tmp/x_transformer.pkl would be replaced by: args: ... x_transformer: default: url: https://zenodo.org/path/to/example_files/x_transformer.pkl md5: 76a5sd76asd57 So every local path has to be replaced by the url and md5 combination. Where md5 is the md5 sum of the file. If you cannot find the the md5 sum on the zenodo / figshare website you can for example run curl https://zenodo.org/.../x_transformer.pkl | md5sum to calculate the md5 sum. Now after replacing all the files, test the setup again by running kipoi test . and then delete the tmp folder. Now the only file(s) remaining in the folder should be model.yaml (and in this case also: dataloader.py dataloader.yaml ).","title":"9 Update model.yaml and dataloader.yaml"},{"location":"tutorials/contributing_models/#9-test-again","text":"Now that you have deleted the temporary files, rerun the test to make sure everything works fine.","title":"9 Test again"},{"location":"tutorials/contributing_models/#10-commit-and-push","text":"Now commit the model.yaml and if needed (like in this example) also the dataloader.py and datalaoder.yaml files by running: git add model.yaml . Now you can push back to your fork ( git push ) and submit a pull request with kipoi/models to request adding your model to the Kipoi models.","title":"10 Commit and push"},{"location":"tutorials/contributing_models/#accessing-local-models-through-kipoi","text":"In Kipoi it is not necessary to publish your model. You can leverage the full functionality of Kipoi also for local models. All you have to do is specify --source dir when using the CLI or setting source=\"dir\" in the python API. The model name is then the local path to the model folder. import kipoi m = kipoi.get_model(\".\", source=\"dir\") # See also python-sdk.ipynb m.pipeline.predict({\"features_file\": \"tmp/example_features.csv\", \"targets_file\": \"tmp/example_targets.csv\" })[:5] 0it [00:00, ?it/s]\u001b[A 1it [00:00, 19.03it/s]\u001b[A array([[ 3.2324865 , -0.29753828, 0.62135816], [ 2.8549244 , 0.4957999 , 0.6873083 ], [ 3.2744825 , 0.40906954, 0.99161 ], [ 3.1413555 , 0.58123374, 1.0272367 ], [ 3.416262 , -0.34901416, 0.76257455]], dtype=float32) m.info ModelInfo(authors=[Author(name='Your Name', github='your_github_username', email='your_email@host.org')], doc='Model predicting the Iris species', name=None, version='0.1', license='MIT', tags=[], contributors=[], cite_as='https://doi.org:/...', trained_on='Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris)', training_procedure=None) m.default_dataloader dataloader.MyDataset m.model <keras.engine.training.Model at 0x2ab5a3eff668> m.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x2ab5a2d75160>>","title":"Accessing local models through kipoi"},{"location":"tutorials/contributing_models/#recap","text":"Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use kipoi init to setup a model directory. Make sure you have read the getting started guide for contributing models.","title":"Recap"},{"location":"tutorials/python-api/","text":"Generated from notebooks/python-api.ipynb Kipoi python API Quick start There are three basic building blocks in kipoi: Source - provides Models and DataLoaders. Model - makes the prediction given the numpy arrays. Dataloader - loads the data from raw files and transforms them into a form that is directly consumable by the Model List of main commands Get/list sources - kipoi.list_sources() - kipoi.get_source() List models/dataloaders - kipoi.list_models() - kipoi.list_dataloaders() Get model/dataloader - kipoi.get_model() - kipoi.get_dataloader_factory() Load only model/dataloader description from the yaml file without loading the model kipoi.get_model_descr() kipoi.get_dataloader_descr() Install the dependencies - kipoi.install_model_dependencies() - kipoi.install_dataloader_dependencies() import kipoi Source Available sources are specified in the config file located at: ~/.kipoi/config.yaml . Here is an example config file: model_sources: kipoi: # default type: git-lfs # git repository with large file storage (git-lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: ~/.kipoi/models/ # local storage path gl: type: git-lfs # custom model remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git local_path: /s/project/model-zoo There are three different model sources possible: git-lfs - git repository with source files tracked normally by git and all the binary files like model weights (located in files* directories) are tracked by git-lfs . Requires git-lfs to be installed. git - all the files including weights (not recommended) local - local directory containing models defined in subdirectories For git-lfs source type, larger files tracked by git-lfs will be downloaded into the specified directory local_path only after the model has been requested (when invoking kipoi.get_model() ). Note A particular model/dataloader is defined by its source (say kipoi or my_git_models ) and the relative path of the desired model directory from the model source root (say rbp/ ). A directory is considered a model if it contains a model.yaml file. import kipoi import warnings warnings.filterwarnings('ignore') import logging logging.disable(1000) kipoi.list_sources() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source type location local_size n_models n_dataloaders 0 kipoi git-lfs /home/avsec/.kipoi/mo... 1,2G 780 780 s = kipoi.get_source(\"kipoi\") s GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') kipoi.list_models().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags 0 kipoi DeepSEAKeras 0.1 [Author(name='Jian Zh... [Author(name='Lara Ur... This CNN is based on ... keras seq TFBS_DHS_probs True MIT https://doi.org/10.10... ENCODE and Roadmap Ep... https://www.nature.co... [Histone modification... 1 kipoi extended_coda 0.1 [Author(name='Pang We... [Author(name='Johnny ... Single bp resolution ... keras [H3K27AC_subsampled] [H3K27ac] False MIT https://doi.org/10.10... Described in https://... Described in https://... [Histone modification] 2 kipoi DeepCpG_DNA/Hou2016_m... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/mESC1, cpg/mESC2... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 3 kipoi DeepCpG_DNA/Smallwood... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/BS24_1_2I, cpg/B... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 4 kipoi DeepCpG_DNA/Hou2016_H... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/HepG21, cpg/HepG... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] Model Let's choose to use the rbp_eclip/UPF1 model from kipoi MODEL = \"rbp_eclip/UPF1\" NOTE: If you are using python2, use a different model like MaxEntScan/3prime to following this example. # Note. Install all the dependencies for that model: # add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow) !kipoi env install {MODEL} model = kipoi.get_model(MODEL) Available fields: Model type args info authors name version tags doc schema inputs targets default_dataloader - loaded dataloader class predict_on_batch() source source_dir pipeline predict() predict_example() predict_generator() Dataloader type defined_as args info (same as for the model) output_schema inputs targets metadata source source_dir example_kwargs init_example() batch_iter() batch_train_iter() batch_predict_iter() load_all() model <kipoi.model.KerasModel at 0x7f95b27af2b0> model.type 'keras' Info model.info ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\' ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017 ', training_procedure='Single task training with ADAM') model.info.version '0.1' Schema dict(model.schema.inputs) {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None), 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None), 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None), 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None), 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)} model.schema.targets ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None) Default dataloader Model already has the default dataloder present. To use it, specify model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1' model.default_dataloader dataloader.SeqDistDataset model.default_dataloader.info Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks - tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end ', name=None, version='0.1', license='MIT', tags=[]) Predict_on_batch model.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>> Others # Model source model.source GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') # model location directory model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1' DataLoader DataLoader = kipoi.get_dataloader_factory(MODEL) # same as DataLoader = model.default_dataloader A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed: # Display information about the dataloader print(DataLoader.__doc__) Args: intervals_file: file path; tsv file Assumes bed-like `chrom start end id score strand` format. fasta_file: file path; Genome sequence gtf_file: file path; Genome annotation GTF file. filter_protein_coding: Considering genomic landmarks only for protein coding genes preproc_transformer: file path; tranformer used for pre-processing. target_file: file path; path to the targets batch_size: int # Alternatively the dataloader keyword arguments can be displayed using the function: kipoi.print_dl_kwargs(DataLoader) Keyword argument: `intervals_file` doc: bed6 file with `chrom start end id score strand` columns type: str optional: False example: example_files/intervals.bed Keyword argument: `fasta_file` doc: Reference genome sequence type: str optional: False example: example_files/hg38_chr22.fa Keyword argument: `gtf_file` doc: file path; Genome annotation GTF file type: str optional: False example: example_files/gencode.v24.annotation_chr22.gtf Keyword argument: `filter_protein_coding` doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark. type: str optional: True example: True Keyword argument: `target_file` doc: path to the targets (txt) file type: str optional: True example: example_files/targets.tsv Keyword argument: `use_linecache` doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows type: str optional: True -------------------------------------------------------------------------------- Example keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'} Run dataloader on some examples # each dataloader already provides example files which can be used to illustrate its use: DataLoader.example_kwargs {'fasta_file': 'example_files/hg38_chr22.fa', 'filter_protein_coding': True, 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'intervals_file': 'example_files/intervals.bed', 'target_file': 'example_files/targets.tsv'} import os # cd into the source directory os.chdir(DataLoader.source_dir) !tree . \u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py \u251c\u2500\u2500 dataloader_files \u2502 \u2514\u2500\u2500 position_transformer.pkl \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u251c\u2500\u2500 model.yaml -> ../template/model.yaml \u2514\u2500\u2500 __pycache__ \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc \u2514\u2500\u2500 dataloader.cpython-36.pyc 4 directories, 8 files dl = DataLoader(**DataLoader.example_kwargs) # could be also done with DataLoader.init_example() # This particular dataloader is of type Dataset # i.e. it implements the __getitem__ method: dl[0].keys() dict_keys(['inputs', 'targets', 'metadata']) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) len(dl) 14 Get the whole dataset whole_data = dl.load_all() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.24it/s] whole_data.keys() dict_keys(['inputs', 'targets', 'metadata']) whole_data[\"inputs\"][\"seq\"].shape (14, 101, 4) Get the iterator to run predictions it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False) next(it)[\"inputs\"][\"seq\"].shape (1, 101, 4) model.predict_on_batch(next(it)[\"inputs\"]) array([[0.1351]], dtype=float32) Pipeline Pipeline object will take the dataloader arguments and run the whole pipeline directly: dataloader arguments --Dataloader--> numpy arrays --Model--> prediction example_kwargs = model.default_dataloader.example_kwargs preds = model.pipeline.predict_example() preds 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.78it/s] array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) model.pipeline.predict(example_kwargs) 1it [00:01, 1.56s/it] array([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208], dtype=float32) next(model.pipeline.predict_generator(example_kwargs, batch_size=2)) array([[0.4208], [0.0005]], dtype=float32) from kipoi.data_utils import numpy_collate numpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs))) array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) Re-train the Keras model Keras model is stored under the .model attribute. model.model.compile(\"adam\", \"binary_crossentropy\") train_it = dl.batch_train_iter(batch_size=2) # model.model.summary() model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1) Epoch 1/1 3/3 [==============================] - 1s 291ms/step - loss: 1.3592 <keras.callbacks.History at 0x7f95b0095fd0>","title":"Python API"},{"location":"tutorials/python-api/#kipoi-python-api","text":"","title":"Kipoi python API"},{"location":"tutorials/python-api/#quick-start","text":"There are three basic building blocks in kipoi: Source - provides Models and DataLoaders. Model - makes the prediction given the numpy arrays. Dataloader - loads the data from raw files and transforms them into a form that is directly consumable by the Model","title":"Quick start"},{"location":"tutorials/python-api/#list-of-main-commands","text":"Get/list sources - kipoi.list_sources() - kipoi.get_source() List models/dataloaders - kipoi.list_models() - kipoi.list_dataloaders() Get model/dataloader - kipoi.get_model() - kipoi.get_dataloader_factory() Load only model/dataloader description from the yaml file without loading the model kipoi.get_model_descr() kipoi.get_dataloader_descr() Install the dependencies - kipoi.install_model_dependencies() - kipoi.install_dataloader_dependencies() import kipoi","title":"List of main commands"},{"location":"tutorials/python-api/#source","text":"Available sources are specified in the config file located at: ~/.kipoi/config.yaml . Here is an example config file: model_sources: kipoi: # default type: git-lfs # git repository with large file storage (git-lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: ~/.kipoi/models/ # local storage path gl: type: git-lfs # custom model remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git local_path: /s/project/model-zoo There are three different model sources possible: git-lfs - git repository with source files tracked normally by git and all the binary files like model weights (located in files* directories) are tracked by git-lfs . Requires git-lfs to be installed. git - all the files including weights (not recommended) local - local directory containing models defined in subdirectories For git-lfs source type, larger files tracked by git-lfs will be downloaded into the specified directory local_path only after the model has been requested (when invoking kipoi.get_model() ).","title":"Source"},{"location":"tutorials/python-api/#note","text":"A particular model/dataloader is defined by its source (say kipoi or my_git_models ) and the relative path of the desired model directory from the model source root (say rbp/ ). A directory is considered a model if it contains a model.yaml file. import kipoi import warnings warnings.filterwarnings('ignore') import logging logging.disable(1000) kipoi.list_sources() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source type location local_size n_models n_dataloaders 0 kipoi git-lfs /home/avsec/.kipoi/mo... 1,2G 780 780 s = kipoi.get_source(\"kipoi\") s GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') kipoi.list_models().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags 0 kipoi DeepSEAKeras 0.1 [Author(name='Jian Zh... [Author(name='Lara Ur... This CNN is based on ... keras seq TFBS_DHS_probs True MIT https://doi.org/10.10... ENCODE and Roadmap Ep... https://www.nature.co... [Histone modification... 1 kipoi extended_coda 0.1 [Author(name='Pang We... [Author(name='Johnny ... Single bp resolution ... keras [H3K27AC_subsampled] [H3K27ac] False MIT https://doi.org/10.10... Described in https://... Described in https://... [Histone modification] 2 kipoi DeepCpG_DNA/Hou2016_m... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/mESC1, cpg/mESC2... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 3 kipoi DeepCpG_DNA/Smallwood... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/BS24_1_2I, cpg/B... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 4 kipoi DeepCpG_DNA/Hou2016_H... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/HepG21, cpg/HepG... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation]","title":"Note"},{"location":"tutorials/python-api/#model","text":"Let's choose to use the rbp_eclip/UPF1 model from kipoi MODEL = \"rbp_eclip/UPF1\" NOTE: If you are using python2, use a different model like MaxEntScan/3prime to following this example. # Note. Install all the dependencies for that model: # add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow) !kipoi env install {MODEL} model = kipoi.get_model(MODEL)","title":"Model"},{"location":"tutorials/python-api/#available-fields","text":"","title":"Available fields:"},{"location":"tutorials/python-api/#model_1","text":"type args info authors name version tags doc schema inputs targets default_dataloader - loaded dataloader class predict_on_batch() source source_dir pipeline predict() predict_example() predict_generator()","title":"Model"},{"location":"tutorials/python-api/#dataloader","text":"type defined_as args info (same as for the model) output_schema inputs targets metadata source source_dir example_kwargs init_example() batch_iter() batch_train_iter() batch_predict_iter() load_all() model <kipoi.model.KerasModel at 0x7f95b27af2b0> model.type 'keras'","title":"Dataloader"},{"location":"tutorials/python-api/#info","text":"model.info ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\' ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017 ', training_procedure='Single task training with ADAM') model.info.version '0.1'","title":"Info"},{"location":"tutorials/python-api/#schema","text":"dict(model.schema.inputs) {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None), 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None), 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None), 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None), 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)} model.schema.targets ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None)","title":"Schema"},{"location":"tutorials/python-api/#default-dataloader","text":"Model already has the default dataloder present. To use it, specify model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1' model.default_dataloader dataloader.SeqDistDataset model.default_dataloader.info Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks - tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end ', name=None, version='0.1', license='MIT', tags=[])","title":"Default dataloader"},{"location":"tutorials/python-api/#predict_on_batch","text":"model.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>>","title":"Predict_on_batch"},{"location":"tutorials/python-api/#others","text":"# Model source model.source GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') # model location directory model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1'","title":"Others"},{"location":"tutorials/python-api/#dataloader_1","text":"DataLoader = kipoi.get_dataloader_factory(MODEL) # same as DataLoader = model.default_dataloader A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed: # Display information about the dataloader print(DataLoader.__doc__) Args: intervals_file: file path; tsv file Assumes bed-like `chrom start end id score strand` format. fasta_file: file path; Genome sequence gtf_file: file path; Genome annotation GTF file. filter_protein_coding: Considering genomic landmarks only for protein coding genes preproc_transformer: file path; tranformer used for pre-processing. target_file: file path; path to the targets batch_size: int # Alternatively the dataloader keyword arguments can be displayed using the function: kipoi.print_dl_kwargs(DataLoader) Keyword argument: `intervals_file` doc: bed6 file with `chrom start end id score strand` columns type: str optional: False example: example_files/intervals.bed Keyword argument: `fasta_file` doc: Reference genome sequence type: str optional: False example: example_files/hg38_chr22.fa Keyword argument: `gtf_file` doc: file path; Genome annotation GTF file type: str optional: False example: example_files/gencode.v24.annotation_chr22.gtf Keyword argument: `filter_protein_coding` doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark. type: str optional: True example: True Keyword argument: `target_file` doc: path to the targets (txt) file type: str optional: True example: example_files/targets.tsv Keyword argument: `use_linecache` doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows type: str optional: True -------------------------------------------------------------------------------- Example keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}","title":"DataLoader"},{"location":"tutorials/python-api/#run-dataloader-on-some-examples","text":"# each dataloader already provides example files which can be used to illustrate its use: DataLoader.example_kwargs {'fasta_file': 'example_files/hg38_chr22.fa', 'filter_protein_coding': True, 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'intervals_file': 'example_files/intervals.bed', 'target_file': 'example_files/targets.tsv'} import os # cd into the source directory os.chdir(DataLoader.source_dir) !tree . \u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py \u251c\u2500\u2500 dataloader_files \u2502 \u2514\u2500\u2500 position_transformer.pkl \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u251c\u2500\u2500 model.yaml -> ../template/model.yaml \u2514\u2500\u2500 __pycache__ \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc \u2514\u2500\u2500 dataloader.cpython-36.pyc 4 directories, 8 files dl = DataLoader(**DataLoader.example_kwargs) # could be also done with DataLoader.init_example() # This particular dataloader is of type Dataset # i.e. it implements the __getitem__ method: dl[0].keys() dict_keys(['inputs', 'targets', 'metadata']) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) len(dl) 14","title":"Run dataloader on some examples"},{"location":"tutorials/python-api/#get-the-whole-dataset","text":"whole_data = dl.load_all() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.24it/s] whole_data.keys() dict_keys(['inputs', 'targets', 'metadata']) whole_data[\"inputs\"][\"seq\"].shape (14, 101, 4)","title":"Get the whole dataset"},{"location":"tutorials/python-api/#get-the-iterator-to-run-predictions","text":"it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False) next(it)[\"inputs\"][\"seq\"].shape (1, 101, 4) model.predict_on_batch(next(it)[\"inputs\"]) array([[0.1351]], dtype=float32)","title":"Get the iterator to run predictions"},{"location":"tutorials/python-api/#pipeline","text":"Pipeline object will take the dataloader arguments and run the whole pipeline directly: dataloader arguments --Dataloader--> numpy arrays --Model--> prediction example_kwargs = model.default_dataloader.example_kwargs preds = model.pipeline.predict_example() preds 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.78it/s] array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) model.pipeline.predict(example_kwargs) 1it [00:01, 1.56s/it] array([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208], dtype=float32) next(model.pipeline.predict_generator(example_kwargs, batch_size=2)) array([[0.4208], [0.0005]], dtype=float32) from kipoi.data_utils import numpy_collate numpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs))) array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32)","title":"Pipeline"},{"location":"tutorials/python-api/#re-train-the-keras-model","text":"Keras model is stored under the .model attribute. model.model.compile(\"adam\", \"binary_crossentropy\") train_it = dl.batch_train_iter(batch_size=2) # model.model.summary() model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1) Epoch 1/1 3/3 [==============================] - 1s 291ms/step - loss: 1.3592 <keras.callbacks.History at 0x7f95b0095fd0>","title":"Re-train the Keras model"},{"location":"tutorials/tf_binding_models/","text":"Generated from notebooks/tf_binding_models.ipynb Model benchmarking with Kipoi This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example). Load software tools Let's start by loading software for this tutorial: the kipoi model zoo, import kipoi import numpy as np from sklearn.metrics import roc_auc_score Prepare data files Next, we introduce a labeled BED-format interval file and a genome fasta file intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv' fasta_file = 'example_data/hg19_chr22.fa' dl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file} Let's look at the first few lines in the intervals file !head $intervals_file chr22 20208963 20209064 0 chr22 29673572 29673673 0 chr22 28193720 28193821 0 chr22 43864274 43864375 0 chr22 18261550 18261651 0 chr22 7869409 7869510 0 chr22 49798024 49798125 0 chr22 43088594 43088695 0 chr22 35147671 35147772 0 chr22 49486843 49486944 0 The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives. Let's load the labels from the last column: labels = np.loadtxt(intervals_file, usecols=(3,)) Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file. Install DeepBind model software requirements deepbind_model_name = \"DeepBind/D00776.005\" kipoi.install_model_requirements(deepbind_model_name) # Use `$ kipoi env install DeepBind/D00776.005 --gpu` from the command-line to install the gpu version of the dependencies Conda dependencies to be installed: ['python=2.7', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow==1.4', 'keras==2.1.4'] Requirement already satisfied: tensorflow==1.4 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.1.4 Using cached Keras-2.1.4-py2.py3-none-any.whl Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow==1.4) Installing collected packages: keras Found existing installation: Keras 2.0.4 Uninstalling Keras-2.0.4: Successfully uninstalled Keras-2.0.4 Successfully installed keras-2.1.4 Load DeepBind model deepbind_model = kipoi.get_model(deepbind_model_name) /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Get DeepBind predictions deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000) Evaluate DeepBind predictions Let's check the auROC of deepbind predictions: roc_auc_score(labels, deepbind_predictions) 0.808138 Load, run, and evaluate a HOCOMOCO PWM model pwm_model_name = \"pwm_HOCOMOCO/human/JUND\" kipoi.install_model_requirements(pwm_model_name) # Use `$ kipoi env install pwm_HOCOMOCO/human/JUND --gpu` from the command-line to install the gpu version of the dependencies pwm_model = kipoi.get_model(pwm_model_name) pwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000) print(\"PWM auROC:\") roc_auc_score(labels, pwm_predictions) Conda dependencies to be installed: ['python=3.5', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow', 'keras==2.0.4'] Requirement already satisfied: tensorflow in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.0.4 Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: theano in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/Theano-1.0.1-py2.7.egg (from keras==2.0.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.0.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from theano->keras==2.0.4) Installing collected packages: keras Found existing installation: Keras 2.1.4 Uninstalling Keras-2.1.4: Successfully uninstalled Keras-2.1.4 Successfully installed keras-2.0.4 /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. custom_objects=custom_objects) PWM auROC: 0.6431155 In this example, DeepBind's auROC of 80.8% outperforms the HOCOMOCO PWM auROC of 64.3%","title":"Comparing models"},{"location":"tutorials/tf_binding_models/#model-benchmarking-with-kipoi","text":"This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example).","title":"Model benchmarking with Kipoi"},{"location":"tutorials/tf_binding_models/#load-software-tools","text":"Let's start by loading software for this tutorial: the kipoi model zoo, import kipoi import numpy as np from sklearn.metrics import roc_auc_score","title":"Load software tools"},{"location":"tutorials/tf_binding_models/#prepare-data-files","text":"Next, we introduce a labeled BED-format interval file and a genome fasta file intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv' fasta_file = 'example_data/hg19_chr22.fa' dl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file} Let's look at the first few lines in the intervals file !head $intervals_file chr22 20208963 20209064 0 chr22 29673572 29673673 0 chr22 28193720 28193821 0 chr22 43864274 43864375 0 chr22 18261550 18261651 0 chr22 7869409 7869510 0 chr22 49798024 49798125 0 chr22 43088594 43088695 0 chr22 35147671 35147772 0 chr22 49486843 49486944 0 The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives. Let's load the labels from the last column: labels = np.loadtxt(intervals_file, usecols=(3,)) Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file.","title":"Prepare data files"},{"location":"tutorials/tf_binding_models/#install-deepbind-model-software-requirements","text":"deepbind_model_name = \"DeepBind/D00776.005\" kipoi.install_model_requirements(deepbind_model_name) # Use `$ kipoi env install DeepBind/D00776.005 --gpu` from the command-line to install the gpu version of the dependencies Conda dependencies to be installed: ['python=2.7', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow==1.4', 'keras==2.1.4'] Requirement already satisfied: tensorflow==1.4 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.1.4 Using cached Keras-2.1.4-py2.py3-none-any.whl Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow==1.4) Installing collected packages: keras Found existing installation: Keras 2.0.4 Uninstalling Keras-2.0.4: Successfully uninstalled Keras-2.0.4 Successfully installed keras-2.1.4","title":"Install DeepBind model software requirements"},{"location":"tutorials/tf_binding_models/#load-deepbind-model","text":"deepbind_model = kipoi.get_model(deepbind_model_name) /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend.","title":"Load DeepBind model"},{"location":"tutorials/tf_binding_models/#get-deepbind-predictions","text":"deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000)","title":"Get DeepBind predictions"},{"location":"tutorials/tf_binding_models/#evaluate-deepbind-predictions","text":"Let's check the auROC of deepbind predictions: roc_auc_score(labels, deepbind_predictions) 0.808138","title":"Evaluate DeepBind predictions"},{"location":"tutorials/tf_binding_models/#load-run-and-evaluate-a-hocomoco-pwm-model","text":"pwm_model_name = \"pwm_HOCOMOCO/human/JUND\" kipoi.install_model_requirements(pwm_model_name) # Use `$ kipoi env install pwm_HOCOMOCO/human/JUND --gpu` from the command-line to install the gpu version of the dependencies pwm_model = kipoi.get_model(pwm_model_name) pwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000) print(\"PWM auROC:\") roc_auc_score(labels, pwm_predictions) Conda dependencies to be installed: ['python=3.5', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow', 'keras==2.0.4'] Requirement already satisfied: tensorflow in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.0.4 Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: theano in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/Theano-1.0.1-py2.7.egg (from keras==2.0.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.0.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from theano->keras==2.0.4) Installing collected packages: keras Found existing installation: Keras 2.1.4 Uninstalling Keras-2.1.4: Successfully uninstalled Keras-2.1.4 Successfully installed keras-2.0.4 /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. custom_objects=custom_objects) PWM auROC: 0.6431155 In this example, DeepBind's auROC of 80.8% outperforms the HOCOMOCO PWM auROC of 64.3%","title":"Load, run, and evaluate a HOCOMOCO PWM model"},{"location":"using/01_Getting_started/","text":"Using Kipoi - Getting started Steps 1. Install and setup Kipoi First install Kipoi and select a model you want to work with using the instructions . 2. Use the model You can use the model from: Python Command-line interface R (via the reticulate package) Python - quick start See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi List all models kipoi.list_models() Get the model Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"rbp_eclip/UPF1\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink') Access information about the model In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model Test the model Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example() Get predictions for the raw files For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the rbp_eclip/UPF1 model, you would run the following: # Make sure we are in the directory containing the example files import os os.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1')) # Run the prediction model.pipeline.predict({'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}) Setup the dataloader If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\") Predict for a single batch Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs']) Re-train the model it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10) Command-line interface - quick start Show help For the command line interface the help command should explain most functionality kipoi -h List all models kipoi ls Get information on how the required dataloader keyword arguments kipoi info -i --source kipoi rbp_eclip/UPF1 Run model prediction cd ~/.kipoi/models/rbp_eclip/UPF1/example_files kipoi predict rbp_eclip/UPF1 \\ --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\ -o '/tmp/rbp_eclip__UPF1.example_pred.tsv' # check the results head '/tmp/rbp_eclip__UPF1.example_pred.tsv' Test a model Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files Install all model dependencies kipoi env install rbp_eclip/UPF1 Create a new conda environment for the model kipoi env create rbp_eclip/UPF1 source activate kipoi-rbp_eclip__UPF List all Kipoi environments kipoi env list Use source activate <env> or conda activate <env> to activate the environment. Score variants kipoi postproc score_variant rbp_eclip/UPF1 \\ --batch_size=16 \\ -v input.vcf \\ -o output.vcf R - quick start See tutorials/R-api .","title":"01 Getting started"},{"location":"using/01_Getting_started/#using-kipoi-getting-started","text":"","title":"Using Kipoi - Getting started"},{"location":"using/01_Getting_started/#steps","text":"","title":"Steps"},{"location":"using/01_Getting_started/#1-install-and-setup-kipoi","text":"First install Kipoi and select a model you want to work with using the instructions .","title":"1. Install and setup Kipoi"},{"location":"using/01_Getting_started/#2-use-the-model","text":"You can use the model from: Python Command-line interface R (via the reticulate package)","title":"2. Use the model"},{"location":"using/01_Getting_started/#python-quick-start","text":"See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi","title":"Python - quick start"},{"location":"using/01_Getting_started/#list-all-models","text":"kipoi.list_models()","title":"List all models"},{"location":"using/01_Getting_started/#get-the-model","text":"Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"rbp_eclip/UPF1\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink')","title":"Get the model"},{"location":"using/01_Getting_started/#access-information-about-the-model","text":"In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model","title":"Access information about the model"},{"location":"using/01_Getting_started/#test-the-model","text":"Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example()","title":"Test the model"},{"location":"using/01_Getting_started/#get-predictions-for-the-raw-files","text":"For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the rbp_eclip/UPF1 model, you would run the following: # Make sure we are in the directory containing the example files import os os.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1')) # Run the prediction model.pipeline.predict({'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'})","title":"Get predictions for the raw files"},{"location":"using/01_Getting_started/#setup-the-dataloader","text":"If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\")","title":"Setup the dataloader"},{"location":"using/01_Getting_started/#predict-for-a-single-batch","text":"Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs'])","title":"Predict for a single batch"},{"location":"using/01_Getting_started/#re-train-the-model","text":"it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)","title":"Re-train the model"},{"location":"using/01_Getting_started/#command-line-interface-quick-start","text":"","title":"Command-line interface - quick start"},{"location":"using/01_Getting_started/#show-help","text":"For the command line interface the help command should explain most functionality kipoi -h","title":"Show help"},{"location":"using/01_Getting_started/#list-all-models_1","text":"kipoi ls","title":"List all models"},{"location":"using/01_Getting_started/#get-information-on-how-the-required-dataloader-keyword-arguments","text":"kipoi info -i --source kipoi rbp_eclip/UPF1","title":"Get information on how the required dataloader keyword arguments"},{"location":"using/01_Getting_started/#run-model-prediction","text":"cd ~/.kipoi/models/rbp_eclip/UPF1/example_files kipoi predict rbp_eclip/UPF1 \\ --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\ -o '/tmp/rbp_eclip__UPF1.example_pred.tsv' # check the results head '/tmp/rbp_eclip__UPF1.example_pred.tsv'","title":"Run model prediction"},{"location":"using/01_Getting_started/#test-a-model","text":"Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files","title":"Test a model"},{"location":"using/01_Getting_started/#install-all-model-dependencies","text":"kipoi env install rbp_eclip/UPF1","title":"Install all model dependencies"},{"location":"using/01_Getting_started/#create-a-new-conda-environment-for-the-model","text":"kipoi env create rbp_eclip/UPF1 source activate kipoi-rbp_eclip__UPF","title":"Create a new conda environment for the model"},{"location":"using/01_Getting_started/#list-all-kipoi-environments","text":"kipoi env list Use source activate <env> or conda activate <env> to activate the environment.","title":"List all Kipoi environments"},{"location":"using/01_Getting_started/#score-variants","text":"kipoi postproc score_variant rbp_eclip/UPF1 \\ --batch_size=16 \\ -v input.vcf \\ -o output.vcf","title":"Score variants"},{"location":"using/01_Getting_started/#r-quick-start","text":"See tutorials/R-api .","title":"R - quick start"},{"location":"using/03_Model_sources/","text":"~/.kipoi/config.yaml kipoi package has a config file located at ~/.kipoi/config.yaml . By default, it will look like this (without comments): model_sources: kipoi: # source name type: git-lfs # git repository with large file storage (lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: /home/avsec/.kipoi/models/ # local storage path # special model source storing models accessed via github permalinks github-permalink: type: github-permalink local_path: /home/avsec/.kipoi/github-permalink/ model_sources defines all the places where kipoi will search for models and pull them to a local directory. By default, it contains the model-zoo from github.com/kipoi/models . It is not a normal git repository, since bigger files are stored with git large file storage (git-lfs) . This repository will be stored locally under local_path . Advantage of using git-lfs is that only the files tracked by git will be downloaded first. Larger files stored in git-lfs will be downloaded individually for each model upon request (say when a user invokes a kipoi predict command). All possible model source types In addition to the default kipoi source, you can modify ~/.kipoi/config.yaml and add additional (private or public) model sources. Available model source types are: git-lfs - As for kipoi model source. Model weights will get downloaded upon request (say when running kipoi predict ). git - Normal git repository, all the files will be downloaded on checkout. local - Local directory. Example: model_sources: kipoi: type: git-lfs remote_url: git@github.com:kipoi/models.git local_path: /home/avsec/.kipoi/models/ my_git_models: type: git remote_url: git@github.com:asd/other_models.git local_path: ~/.kipoi/other_models/ my_local_models: type: local local_path: /data/mymodels/ About model definition A particular model is defined by its source (key under model_sources , say kipoi ) and the relative path of the desired model directory from the model source root (say rbp_eclip/UPF1 ). A directory is considered a model if it contains a model.yaml file.","title":"Private and public model sources"},{"location":"using/03_Model_sources/#kipoiconfigyaml","text":"kipoi package has a config file located at ~/.kipoi/config.yaml . By default, it will look like this (without comments): model_sources: kipoi: # source name type: git-lfs # git repository with large file storage (lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: /home/avsec/.kipoi/models/ # local storage path # special model source storing models accessed via github permalinks github-permalink: type: github-permalink local_path: /home/avsec/.kipoi/github-permalink/ model_sources defines all the places where kipoi will search for models and pull them to a local directory. By default, it contains the model-zoo from github.com/kipoi/models . It is not a normal git repository, since bigger files are stored with git large file storage (git-lfs) . This repository will be stored locally under local_path . Advantage of using git-lfs is that only the files tracked by git will be downloaded first. Larger files stored in git-lfs will be downloaded individually for each model upon request (say when a user invokes a kipoi predict command).","title":"~/.kipoi/config.yaml"},{"location":"using/03_Model_sources/#all-possible-model-source-types","text":"In addition to the default kipoi source, you can modify ~/.kipoi/config.yaml and add additional (private or public) model sources. Available model source types are: git-lfs - As for kipoi model source. Model weights will get downloaded upon request (say when running kipoi predict ). git - Normal git repository, all the files will be downloaded on checkout. local - Local directory. Example: model_sources: kipoi: type: git-lfs remote_url: git@github.com:kipoi/models.git local_path: /home/avsec/.kipoi/models/ my_git_models: type: git remote_url: git@github.com:asd/other_models.git local_path: ~/.kipoi/other_models/ my_local_models: type: local local_path: /data/mymodels/","title":"All possible model source types"},{"location":"using/03_Model_sources/#about-model-definition","text":"A particular model is defined by its source (key under model_sources , say kipoi ) and the relative path of the desired model directory from the model source root (say rbp_eclip/UPF1 ). A directory is considered a model if it contains a model.yaml file.","title":"About model definition"},{"location":"using/04_Installing_on_OSX/","text":"Using Kipoi - Installing on OSX Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is. For Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda. We have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. kipoi where you install Kipoi. Steps Make sure you have python installed: You can try by just execting python in your Terminal, if nothing is found you will want to install python (not pythonw ). There are some good explanations on how python 2 can be installed on OSX Sierra and if you are using High Sierra and you prefer python 3 you can follow this . After completing the steps and installing conda or miniconda please procede as described in getting started .","title":"Installing on OSX"},{"location":"using/04_Installing_on_OSX/#using-kipoi-installing-on-osx","text":"Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is. For Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda. We have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. kipoi where you install Kipoi.","title":"Using Kipoi - Installing on OSX"},{"location":"using/04_Installing_on_OSX/#steps","text":"","title":"Steps"},{"location":"using/04_Installing_on_OSX/#make-sure-you-have-python-installed","text":"You can try by just execting python in your Terminal, if nothing is found you will want to install python (not pythonw ). There are some good explanations on how python 2 can be installed on OSX Sierra and if you are using High Sierra and you prefer python 3 you can follow this . After completing the steps and installing conda or miniconda please procede as described in getting started .","title":"Make sure you have python installed:"},{"location":"using/R/","text":"R You can use Kipoi from R via the reticulate package. For a more complete example, see https://github.com/kipoi/kipoi/blob/master/notebooks/R-api.ipynb . Installation Install Kipoi. See how Install R Install the reticulate package. From R, run: install.packages(\"reticulate\") Make sure reticulate is using python from the miniconda/anaconda installation (same as Kipoi): library(reticulate) reticulate::py_config() Usage Use a specific conda environment library(reticulate) reticulate::use_condaenv(\"kipoi-Basset) or install the dependencies from R: kipoi$install_model_requirements(\"Basset\") Get the model: kipoi <- import('kipoi') model <- kipoi$get_model('Basset') Make a prediction for example files predictions <- model$pipeline$predict_example() Use dataloader and model separately # Get the dataloader setwd('~/.kipoi/models/Basset') dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) model$predict_on_batch(batch$inputs) Make predictions for custom files directly: pred <- model$pipeline$predict(dl_kwargs, batch_size=4)","title":"R"},{"location":"using/R/#r","text":"You can use Kipoi from R via the reticulate package. For a more complete example, see https://github.com/kipoi/kipoi/blob/master/notebooks/R-api.ipynb .","title":"R"},{"location":"using/R/#installation","text":"Install Kipoi. See how Install R Install the reticulate package. From R, run: install.packages(\"reticulate\") Make sure reticulate is using python from the miniconda/anaconda installation (same as Kipoi): library(reticulate) reticulate::py_config()","title":"Installation"},{"location":"using/R/#usage","text":"Use a specific conda environment library(reticulate) reticulate::use_condaenv(\"kipoi-Basset) or install the dependencies from R: kipoi$install_model_requirements(\"Basset\") Get the model: kipoi <- import('kipoi') model <- kipoi$get_model('Basset') Make a prediction for example files predictions <- model$pipeline$predict_example() Use dataloader and model separately # Get the dataloader setwd('~/.kipoi/models/Basset') dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) model$predict_on_batch(batch$inputs) Make predictions for custom files directly: pred <- model$pipeline$predict(dl_kwargs, batch_size=4)","title":"Usage"},{"location":"using/cli/","text":"Command-line interface For the command line interface the help command should explain most functionality kipoi -h ls List all models kipoi ls info Get information on how the required dataloader keyword arguments kipoi info -i --source kipoi rbp_eclip/UPF1 predict Run model prediction cd ~/.kipoi/models/rbp_eclip/UPF1/example_files kipoi predict rbp_eclip/UPF1 \\ --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\ -o '/tmp/rbp_eclip__UPF1.example_pred.tsv' # check the results head '/tmp/rbp_eclip__UPF1.example_pred.tsv' test Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files env install Install model dependencies kipoi env install rbp_eclip/UPF1 create Create a new conda environment for the model kipoi env create rbp_eclip/UPF1 source activate kipoi-rbp_eclip__UPF list List all environments kipoi env list Use source activate <env> or conda activate <env> to activate the environment.","title":"CLI"},{"location":"using/cli/#command-line-interface","text":"For the command line interface the help command should explain most functionality kipoi -h","title":"Command-line interface"},{"location":"using/cli/#ls","text":"List all models kipoi ls","title":"ls"},{"location":"using/cli/#info","text":"Get information on how the required dataloader keyword arguments kipoi info -i --source kipoi rbp_eclip/UPF1","title":"info"},{"location":"using/cli/#predict","text":"Run model prediction cd ~/.kipoi/models/rbp_eclip/UPF1/example_files kipoi predict rbp_eclip/UPF1 \\ --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\ -o '/tmp/rbp_eclip__UPF1.example_pred.tsv' # check the results head '/tmp/rbp_eclip__UPF1.example_pred.tsv'","title":"predict"},{"location":"using/cli/#test","text":"Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files","title":"test"},{"location":"using/cli/#env","text":"","title":"env"},{"location":"using/cli/#install","text":"Install model dependencies kipoi env install rbp_eclip/UPF1","title":"install"},{"location":"using/cli/#create","text":"Create a new conda environment for the model kipoi env create rbp_eclip/UPF1 source activate kipoi-rbp_eclip__UPF","title":"create"},{"location":"using/cli/#list","text":"List all environments kipoi env list Use source activate <env> or conda activate <env> to activate the environment.","title":"list"},{"location":"using/plugins/","text":"Plugins To enable additional functionality beyond just running model predictions, there are two additional plugins available: kipoi-veff: variant effect prediction github , docs kipoi-interpret: model interpretation. github , docs Kipoi-veff Kipoi-veff is a plugin specific to genomics. Models trained to predict various molecular phenotypes from DNA sequence can be used to assess the impact of genetic mutations or variants. The veff plugin allows you to take the VCF file\u200a\u2014\u200acanonical file format for storing genetic variants\u200a\u2014\u200aand obtain changes in model predictions due to the genetic variants/mutations changing the DNA sequence. Kipoi-interpret Kipoi-interpret is a general (genomics agnostic) plugin and allows to compute the feature importance scores like saliency maps or DeepLift for Kipoi models. import kipoi from kipoi_interpret.importance_scores.gradient import GradientXInput model = kipoi.get_model(\"DeepBind/Homo_sapiens/TF/D00765.001_ChIP-seq_GATA1\") val = GradientXInput(model).score(seq_array)[0] seqlogo_heatmap(val, val.T)","title":"Plugins"},{"location":"using/plugins/#plugins","text":"To enable additional functionality beyond just running model predictions, there are two additional plugins available: kipoi-veff: variant effect prediction github , docs kipoi-interpret: model interpretation. github , docs","title":"Plugins"},{"location":"using/plugins/#kipoi-veff","text":"Kipoi-veff is a plugin specific to genomics. Models trained to predict various molecular phenotypes from DNA sequence can be used to assess the impact of genetic mutations or variants. The veff plugin allows you to take the VCF file\u200a\u2014\u200acanonical file format for storing genetic variants\u200a\u2014\u200aand obtain changes in model predictions due to the genetic variants/mutations changing the DNA sequence.","title":"Kipoi-veff"},{"location":"using/plugins/#kipoi-interpret","text":"Kipoi-interpret is a general (genomics agnostic) plugin and allows to compute the feature importance scores like saliency maps or DeepLift for Kipoi models. import kipoi from kipoi_interpret.importance_scores.gradient import GradientXInput model = kipoi.get_model(\"DeepBind/Homo_sapiens/TF/D00765.001_ChIP-seq_GATA1\") val = GradientXInput(model).score(seq_array)[0] seqlogo_heatmap(val, val.T)","title":"Kipoi-interpret"},{"location":"using/python/","text":"Python API See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi List all models kipoi.list_models() Get the model Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"rbp_eclip/UPF1\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink') Access information about the model In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model Test the model Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example() Get predictions for the raw files For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the rbp_eclip/UPF1 model, you would run the following: # Make sure we are in the directory containing the example files import os os.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1')) # Run the prediction model.pipeline.predict({'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}) Setup the dataloader If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\") Predict for a single batch Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs']) Re-train the model it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)","title":"Python"},{"location":"using/python/#python-api","text":"See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi","title":"Python API"},{"location":"using/python/#list-all-models","text":"kipoi.list_models()","title":"List all models"},{"location":"using/python/#get-the-model","text":"Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"rbp_eclip/UPF1\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink')","title":"Get the model"},{"location":"using/python/#access-information-about-the-model","text":"In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model","title":"Access information about the model"},{"location":"using/python/#test-the-model","text":"Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example()","title":"Test the model"},{"location":"using/python/#get-predictions-for-the-raw-files","text":"For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the rbp_eclip/UPF1 model, you would run the following: # Make sure we are in the directory containing the example files import os os.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1')) # Run the prediction model.pipeline.predict({'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'})","title":"Get predictions for the raw files"},{"location":"using/python/#setup-the-dataloader","text":"If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\")","title":"Setup the dataloader"},{"location":"using/python/#predict-for-a-single-batch","text":"Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs'])","title":"Predict for a single batch"},{"location":"using/python/#re-train-the-model","text":"it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)","title":"Re-train the model"}]}