{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kipoi: Model zoo for genomics This repository implements a python package and a command-line interface (CLI) to access and use models from Kipoi-compatible model zoo's. Links kipoi.org - Main website kipoi.org/docs - Documentation github.com/kipoi/models - Model zoo for genomics maintained by the Kipoi team biorxiv preprint - Kipoi: accelerating the community exchange and reuse of predictive models for genomics blog post introducing Kipoi , blog post describing 0.6 release github.com/kipoi/examples - Use-case oriented tutorials Installation Kipoi requires conda to manage model dependencies. Make sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see Installing python on OSX . Maintained python versions: >=3.6<=3.10. Install Kipoi using pip : pip install kipoi Known issue: h5py For systems using python 3.6 and 3.7, pretrained kipoi models of type kipoi.model.KerasModel and kipoi.model.TensorflowModel which were saved with h5py <3. are incompatible with h5py >= 3. . Please downgrade h5py after installing kipoi pip install h5py==2.10.0 This is not a problem with systems using python >=3.8<=3.10. More information available here For systems using python >=3.8<=3.10, it is necessary to install hdf5 and pkgconfig prior to installing kipoi. conda install --yes -c conda-forge hdf5 pkgconfig Quick start Explore available models on https://kipoi.org/groups/ . Use-case oriented tutorials are available at https://github.com/kipoi/examples . Installing all required model dependencies Use kipoi env create <model> to create a new conda environment for the model. You can use the following two commands to create common environments suitable for multiple models. kipoi env create shared/envs/kipoi-py3-keras2-tf1 kipoi env create shared/envs/kipoi-py3-keras2-tf2 kipoi env create shared/envs/kipoi-py3-keras1.2 Before using a model in any way, activate the right conda enviroment: source activate $(kipoi env get <model>) Using pre-made containers Alternatively, you can use the Singularity or Docker containers with all dependencies installed. Singularity containers can be seamlessly used with the CLI by adding the --singularity flag to kipoi predict commands. For example: Look at the sigularity tab under this . Alternatively, you can use the docker containers directly. For more information: Look at the docker tab under any model web page on kipoi.org such as this . A note about installing singularity Singularity has been renamed to Apptainer . However, it is also possible to use SingularityCE from Sylabs . Current versions of kipoi containers are compatible with the latest version of Apptainer (1.0.2) and SingularityCE 3.9. Install Apptainer from here or SingularityCE from here . Python Before using a model from python in any way, activate the right conda enviroment: source activate $(kipoi env get <model>) import kipoi kipoi.list_models() # list available models model = kipoi.get_model(\"Basset\") # load the model model = kipoi.get_model( # load the model from a past commit \"https://github.com/kipoi/models/tree/<commit>/<model>\", source='github-permalink' ) # main attributes model.model # wrapped model (say keras.models.Model) model.default_dataloader # dataloader model.info # description, authors, paper link, ... # main methods model.predict_on_batch(x) # implemented by all the models regardless of the framework model.pipeline.predict(dict(fasta_file=\"hg19.fa\", intervals_file=\"intervals.bed\")) # runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions For more information see: notebooks/python-api.ipynb and docs/using/python Command-line $ kipoi usage: kipoi <command> [-h] ... # Kipoi model-zoo command line tool. Available sub-commands: # - using models: ls List all the available models list_plugins List all the available plugins info Print dataloader keyword argument info get-example Download example files predict Run the model prediction pull Download the directory associated with the model preproc Run the dataloader and save the results to an hdf5 array env Tools for managing Kipoi conda environments # - contributing models: init Initialize a new Kipoi model test Runs a set of unit-tests for the model test-source Runs a set of unit-tests for many/all models in a source # - plugin commands: interpret Model interpretation using feature importance scores like ISM, grad*input or DeepLIFT # Run model predictions and save the results # sequentially into an HDF5 file kipoi predict <Model> --dataloader_args='{ \"intervals_file\": \"intervals.bed\", \"fasta_file\": \"hg38.fa\"}' \\ --singularity \\ -o '<Model>.preds.h5' Explore the CLI usage by running kipoi <command> -h . Also, see docs/using/cli/ for more information. Configure Kipoi in .kipoi/config.yaml You can add your own (private) model sources. See docs/using/03_Model_sources/ . Contributing models See docs/contributing getting started and docs/tutorials/contributing/models for more information. Plugins Kipoi supports plug-ins which are published as additional python packages. Currently available plug-in is: kipoi_interpret Model interpretation plugin for Kipoi. Allows to use feature importance scores like in-silico mutagenesis (ISM), saliency maps or DeepLift with a wide range of Kipoi models. example notebook pip install kipoi_interpret Variant effect prediction with a subset of Kipoi models Variant effect prediction allows to annotate a vcf file using model predictions for the reference and alternative alleles. The output is written to a new tsv file. For more information see https://github.com/kipoi/kipoi-veff2 . Documentation kipoi.org/docs Tutorials https://github.com/kipoi/examples - Use-case oriented tutorials notebooks Citing Kipoi If you use Kipoi for your research, please cite the publication of the model you are using (see model's cite_as entry) and the paper describing Kipoi: https://doi.org/10.1038/s41587-019-0140-0. @article{kipoi, title={The Kipoi repository accelerates community exchange and reuse of predictive models for genomics}, author={Avsec, Ziga and Kreuzhuber, Roman and Israeli, Johnny and Xu, Nancy and Cheng, Jun and Shrikumar, Avanti and Banerjee, Abhimanyu and Kim, Daniel S and Beier, Thorsten and Urban, Lara and others}, journal={Nature biotechnology}, pages={1}, year={2019}, publisher={Nature Publishing Group} } Development If you want to help with the development of Kipoi, you are more than welcome to join in! For the local setup for development, you should install all required dependencies using one of the provided dev-requirements(-py<36|37>).yml files For systems using python 3.6/3.7: conda env create -f dev-requirements-py36.yml --experimental-solver=libmamba or conda env create -f dev-requirements-py37.yml --experimental-solver=libmamba conda activate kipoi-dev pip install -e . git lfs install For systems using python >=3.8<=3.10: conda create --name kipoi-dev python=3.8 (or 3.9, 3.10) conda activate kipoi-dev conda env update --name kipoi-dev --file dev-requirements.yml --experimental-solver=libmamba pip install -e . conda install -c bioconda cyvcf2 pybigwig git lfs install A note about cyvcf2 and pybigwig For python >= 3.10, cyvcf2 and pybigwig are not available in conda yet. Install them from source like here and here instead. I will recommend against installing them using pip as it may lead to unexpected inconsistencies. You can test the package by running py.test . If you wish to run tests in parallel, run py.test -n 6 . License Kipoi is MIT-style licensed, as found in the LICENSE file.","title":"Home"},{"location":"#kipoi-model-zoo-for-genomics","text":"This repository implements a python package and a command-line interface (CLI) to access and use models from Kipoi-compatible model zoo's.","title":"Kipoi: Model zoo for genomics"},{"location":"#links","text":"kipoi.org - Main website kipoi.org/docs - Documentation github.com/kipoi/models - Model zoo for genomics maintained by the Kipoi team biorxiv preprint - Kipoi: accelerating the community exchange and reuse of predictive models for genomics blog post introducing Kipoi , blog post describing 0.6 release github.com/kipoi/examples - Use-case oriented tutorials","title":"Links"},{"location":"#installation","text":"Kipoi requires conda to manage model dependencies. Make sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see Installing python on OSX . Maintained python versions: >=3.6<=3.10. Install Kipoi using pip : pip install kipoi","title":"Installation"},{"location":"#known-issue-h5py","text":"For systems using python 3.6 and 3.7, pretrained kipoi models of type kipoi.model.KerasModel and kipoi.model.TensorflowModel which were saved with h5py <3. are incompatible with h5py >= 3. . Please downgrade h5py after installing kipoi pip install h5py==2.10.0 This is not a problem with systems using python >=3.8<=3.10. More information available here For systems using python >=3.8<=3.10, it is necessary to install hdf5 and pkgconfig prior to installing kipoi. conda install --yes -c conda-forge hdf5 pkgconfig","title":"Known issue: h5py"},{"location":"#quick-start","text":"Explore available models on https://kipoi.org/groups/ . Use-case oriented tutorials are available at https://github.com/kipoi/examples .","title":"Quick start"},{"location":"#installing-all-required-model-dependencies","text":"Use kipoi env create <model> to create a new conda environment for the model. You can use the following two commands to create common environments suitable for multiple models. kipoi env create shared/envs/kipoi-py3-keras2-tf1 kipoi env create shared/envs/kipoi-py3-keras2-tf2 kipoi env create shared/envs/kipoi-py3-keras1.2 Before using a model in any way, activate the right conda enviroment: source activate $(kipoi env get <model>)","title":"Installing all required model dependencies"},{"location":"#using-pre-made-containers","text":"Alternatively, you can use the Singularity or Docker containers with all dependencies installed. Singularity containers can be seamlessly used with the CLI by adding the --singularity flag to kipoi predict commands. For example: Look at the sigularity tab under this . Alternatively, you can use the docker containers directly. For more information: Look at the docker tab under any model web page on kipoi.org such as this .","title":"Using pre-made containers"},{"location":"#a-note-about-installing-singularity","text":"Singularity has been renamed to Apptainer . However, it is also possible to use SingularityCE from Sylabs . Current versions of kipoi containers are compatible with the latest version of Apptainer (1.0.2) and SingularityCE 3.9. Install Apptainer from here or SingularityCE from here .","title":"A note about installing singularity"},{"location":"#python","text":"Before using a model from python in any way, activate the right conda enviroment: source activate $(kipoi env get <model>) import kipoi kipoi.list_models() # list available models model = kipoi.get_model(\"Basset\") # load the model model = kipoi.get_model( # load the model from a past commit \"https://github.com/kipoi/models/tree/<commit>/<model>\", source='github-permalink' ) # main attributes model.model # wrapped model (say keras.models.Model) model.default_dataloader # dataloader model.info # description, authors, paper link, ... # main methods model.predict_on_batch(x) # implemented by all the models regardless of the framework model.pipeline.predict(dict(fasta_file=\"hg19.fa\", intervals_file=\"intervals.bed\")) # runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions For more information see: notebooks/python-api.ipynb and docs/using/python","title":"Python"},{"location":"#command-line","text":"$ kipoi usage: kipoi <command> [-h] ... # Kipoi model-zoo command line tool. Available sub-commands: # - using models: ls List all the available models list_plugins List all the available plugins info Print dataloader keyword argument info get-example Download example files predict Run the model prediction pull Download the directory associated with the model preproc Run the dataloader and save the results to an hdf5 array env Tools for managing Kipoi conda environments # - contributing models: init Initialize a new Kipoi model test Runs a set of unit-tests for the model test-source Runs a set of unit-tests for many/all models in a source # - plugin commands: interpret Model interpretation using feature importance scores like ISM, grad*input or DeepLIFT # Run model predictions and save the results # sequentially into an HDF5 file kipoi predict <Model> --dataloader_args='{ \"intervals_file\": \"intervals.bed\", \"fasta_file\": \"hg38.fa\"}' \\ --singularity \\ -o '<Model>.preds.h5' Explore the CLI usage by running kipoi <command> -h . Also, see docs/using/cli/ for more information.","title":"Command-line"},{"location":"#configure-kipoi-in-kipoiconfigyaml","text":"You can add your own (private) model sources. See docs/using/03_Model_sources/ .","title":"Configure Kipoi in .kipoi/config.yaml"},{"location":"#contributing-models","text":"See docs/contributing getting started and docs/tutorials/contributing/models for more information.","title":"Contributing models"},{"location":"#plugins","text":"Kipoi supports plug-ins which are published as additional python packages. Currently available plug-in is:","title":"Plugins"},{"location":"#kipoi_interpret","text":"Model interpretation plugin for Kipoi. Allows to use feature importance scores like in-silico mutagenesis (ISM), saliency maps or DeepLift with a wide range of Kipoi models. example notebook pip install kipoi_interpret","title":"kipoi_interpret"},{"location":"#variant-effect-prediction-with-a-subset-of-kipoi-models","text":"Variant effect prediction allows to annotate a vcf file using model predictions for the reference and alternative alleles. The output is written to a new tsv file. For more information see https://github.com/kipoi/kipoi-veff2 .","title":"Variant effect prediction with a subset of Kipoi models"},{"location":"#documentation","text":"kipoi.org/docs","title":"Documentation"},{"location":"#tutorials","text":"https://github.com/kipoi/examples - Use-case oriented tutorials notebooks","title":"Tutorials"},{"location":"#citing-kipoi","text":"If you use Kipoi for your research, please cite the publication of the model you are using (see model's cite_as entry) and the paper describing Kipoi: https://doi.org/10.1038/s41587-019-0140-0. @article{kipoi, title={The Kipoi repository accelerates community exchange and reuse of predictive models for genomics}, author={Avsec, Ziga and Kreuzhuber, Roman and Israeli, Johnny and Xu, Nancy and Cheng, Jun and Shrikumar, Avanti and Banerjee, Abhimanyu and Kim, Daniel S and Beier, Thorsten and Urban, Lara and others}, journal={Nature biotechnology}, pages={1}, year={2019}, publisher={Nature Publishing Group} }","title":"Citing Kipoi"},{"location":"#development","text":"If you want to help with the development of Kipoi, you are more than welcome to join in! For the local setup for development, you should install all required dependencies using one of the provided dev-requirements(-py<36|37>).yml files For systems using python 3.6/3.7: conda env create -f dev-requirements-py36.yml --experimental-solver=libmamba or conda env create -f dev-requirements-py37.yml --experimental-solver=libmamba conda activate kipoi-dev pip install -e . git lfs install For systems using python >=3.8<=3.10: conda create --name kipoi-dev python=3.8 (or 3.9, 3.10) conda activate kipoi-dev conda env update --name kipoi-dev --file dev-requirements.yml --experimental-solver=libmamba pip install -e . conda install -c bioconda cyvcf2 pybigwig git lfs install","title":"Development"},{"location":"#a-note-about-cyvcf2-and-pybigwig","text":"For python >= 3.10, cyvcf2 and pybigwig are not available in conda yet. Install them from source like here and here instead. I will recommend against installing them using pip as it may lead to unexpected inconsistencies. You can test the package by running py.test . If you wish to run tests in parallel, run py.test -n 6 .","title":"A note about cyvcf2 and pybigwig"},{"location":"#license","text":"Kipoi is MIT-style licensed, as found in the LICENSE file.","title":"License"},{"location":"faq/","text":"What type of models are suitable for the Kipoi model repository? The Kipoi model repository is restricted to trained models with application in genomics,. Specifically, we request at least one input data modality to be derived from DNA sequence (which includes amino acid sequences) or from a functional genomics assay such as ChIP-seq or protein mass-spectrometry. Moreover, models must be able to satisfy the specifications of the API ( model and dataloader ). Please contact us if the model you would like to share a model that doesn\u2019t fit the scope. We would be happy to help you instantiate a new model repository for a different domain (say imaging). What models don't go into Kipoi? Basically, models that don't fit the above requirements . These are for example models that require training before they can be used (say imputation models that need to be trained on the specific dataset prior to application). What licenses are allowed? Any license that allows the redistribution model of files uploaded to file-sharing services like Zenodo or Figshare. We encourage users to use one of the standard open-source software licenses such as MIT, BSD License, GNU Public License or Apache License ( Comparison of free and open-source software licenses ). Please contact us if you would like to host the files on your own servers. We note that it is the users' responsibility not to break copy rights when (re-)using models that are available in the Kipoi model zoo. License is specified either in the LICENSE file present in the model directory or the license type is specified in model.yaml . Versioning of models We do not version models in the model.yaml. Instead, if substantial changes to the model were made we encourage the author/contributor to create a new model. For example, if KipoiModel/model.yaml gets extended, the author should could create new models: KipoiModel/v1/model.yaml and KipoiModel/v2/model.yaml and keep a softlink to the most recent version in KipoiModel/v2/model.yaml -> KipoiModel/model.yaml . Micro-changes like updating the model description are also tracked using Git, hence a particular model version can be always referred to using the commit hash. Can the model be a binary executable? Yes if the binary is compiled and distributed through Bioconda or Conda-Forge conda channels. Lsgkm-SVM is one such example. See its model.py for the implementation details. Trouble with system-wide libraries? If you have trouble executing kipoi because of system-wide installed libraries you can execute the commands within our singularity containers: After installing [singularity], just add the --singularity argument to your kipoi predict command. Is it possible to perform transfer learning across machine learning frameworks? It depends. Kipoi allows you to pre-compute the activations of the frozen part of the network and save them to a file. These activations can be used as input features for a model written in an arbitrary framework. See this notebook on how to do this. If you wish to fine-tune the the whole model in a different framework you would need to convert the model parameters yourself (we recommend using ONNX to do so). In the future, we plan to convert all the models to the ONNX format which will allow porting models across different frameworks. If you are interested or would like to get involved - here is the issue tracking this feature: https://github.com/kipoi/kipoi/issues/405. Does Kipoi support Windows? For Windows users we suggest to use the docker container: https://hub.docker.com/r/kipoi/models/. Installing conda environments for the Kipoi models is not fully supported on Windows due to unavailability of certain conda packages for Windows. Specifically, cyvcf2 is not readily available for windows via Bioconda, neither is htslib/tabix. htslib has been enabled for Windows from version 1.6, but not on conda. We hope that we will be able to enable support for Windows when those packages become Windows-compatible.","title":"FAQ"},{"location":"faq/#what-type-of-models-are-suitable-for-the-kipoi-model-repository","text":"The Kipoi model repository is restricted to trained models with application in genomics,. Specifically, we request at least one input data modality to be derived from DNA sequence (which includes amino acid sequences) or from a functional genomics assay such as ChIP-seq or protein mass-spectrometry. Moreover, models must be able to satisfy the specifications of the API ( model and dataloader ). Please contact us if the model you would like to share a model that doesn\u2019t fit the scope. We would be happy to help you instantiate a new model repository for a different domain (say imaging).","title":"What type of models are suitable for the Kipoi model repository?"},{"location":"faq/#what-models-dont-go-into-kipoi","text":"Basically, models that don't fit the above requirements . These are for example models that require training before they can be used (say imputation models that need to be trained on the specific dataset prior to application).","title":"What models don't go into Kipoi?"},{"location":"faq/#what-licenses-are-allowed","text":"Any license that allows the redistribution model of files uploaded to file-sharing services like Zenodo or Figshare. We encourage users to use one of the standard open-source software licenses such as MIT, BSD License, GNU Public License or Apache License ( Comparison of free and open-source software licenses ). Please contact us if you would like to host the files on your own servers. We note that it is the users' responsibility not to break copy rights when (re-)using models that are available in the Kipoi model zoo. License is specified either in the LICENSE file present in the model directory or the license type is specified in model.yaml .","title":"What licenses are allowed?"},{"location":"faq/#versioning-of-models","text":"We do not version models in the model.yaml. Instead, if substantial changes to the model were made we encourage the author/contributor to create a new model. For example, if KipoiModel/model.yaml gets extended, the author should could create new models: KipoiModel/v1/model.yaml and KipoiModel/v2/model.yaml and keep a softlink to the most recent version in KipoiModel/v2/model.yaml -> KipoiModel/model.yaml . Micro-changes like updating the model description are also tracked using Git, hence a particular model version can be always referred to using the commit hash.","title":"Versioning of models"},{"location":"faq/#can-the-model-be-a-binary-executable","text":"Yes if the binary is compiled and distributed through Bioconda or Conda-Forge conda channels. Lsgkm-SVM is one such example. See its model.py for the implementation details.","title":"Can the model be a binary executable?"},{"location":"faq/#trouble-with-system-wide-libraries","text":"If you have trouble executing kipoi because of system-wide installed libraries you can execute the commands within our singularity containers: After installing [singularity], just add the --singularity argument to your kipoi predict command.","title":"Trouble with system-wide libraries?"},{"location":"faq/#is-it-possible-to-perform-transfer-learning-across-machine-learning-frameworks","text":"It depends. Kipoi allows you to pre-compute the activations of the frozen part of the network and save them to a file. These activations can be used as input features for a model written in an arbitrary framework. See this notebook on how to do this. If you wish to fine-tune the the whole model in a different framework you would need to convert the model parameters yourself (we recommend using ONNX to do so). In the future, we plan to convert all the models to the ONNX format which will allow porting models across different frameworks. If you are interested or would like to get involved - here is the issue tracking this feature: https://github.com/kipoi/kipoi/issues/405.","title":"Is it possible to perform transfer learning across machine learning frameworks?"},{"location":"faq/#does-kipoi-support-windows","text":"For Windows users we suggest to use the docker container: https://hub.docker.com/r/kipoi/models/. Installing conda environments for the Kipoi models is not fully supported on Windows due to unavailability of certain conda packages for Windows. Specifically, cyvcf2 is not readily available for windows via Bioconda, neither is htslib/tabix. htslib has been enabled for Windows from version 1.6, but not on conda. We hope that we will be able to enable support for Windows when those packages become Windows-compatible.","title":"Does Kipoi support Windows?"},{"location":"api/dataloader/","text":"get_dataloader_descr get_dataloader_descr(dataloader, source='kipoi') Get dataloder description Arguments datalaoder : dataloader's relative path/name in the source. 2nd column in the kipoi.list_dataloader() pd.DataFrame`. source : Model source. 1st column in the kipoi.list_models() pd.DataFrame . get_dataloader get_dataloader(dataloader, source='kipoi') Loads the dataloader Arguments dataloader (str) : dataloader name source (str) : source name Returns Instance of class inheriting from kipoi.data.BaseDataLoader (like kipoi.data.Dataset ) decorated with additional attributes. Methods batch_iter(batch_size, num_workers, **kwargs) Arguments batch_size : batch size num_workers : Number of workers to use in parallel. **kwargs : Other kwargs specific to each dataloader Yields dict with \"inputs\" , \"targets\" and \"metadata\" batch_train_iter(cycle=True, **kwargs) Arguments cycle : if True, cycle indefinitely **kwargs : Kwargs passed to batch_iter() like batch_size Yields tuple of (\"inputs\", \"targets\") from the usual dict returned by batch_iter() batch_predict_iter(**kwargs) Arguments **kwargs : Kwargs passed to batch_iter() like batch_size Yields \"inputs\" field from the usual dict returned by batch_iter() load_all(**kwargs) - load the whole dataset into memory Arguments **kwargs : Kwargs passed to batch_iter() like batch_size Returns dict with \"inputs\" , \"targets\" and \"metadata\" init_example() - instantiate the dataloader with example kwargs print_args() - print information about the required arguments Appended attributes type (str): dataloader type (class name) defined_as (str): path and dataloader name args (list of kipoi.specs.DataLoaderArgument): datalaoder argument description info (kipoi.specs.Info): general information about the dataloader schema (kipoi.specs.DataloaderSchema): information about the input/output data modalities dependencies (kipoi.specs.Dependencies): class specifying the dependencies. (implements install method for running the installation) name (str): model name source (str): model source source_dir (str): local path to model source storage writers (dict): dictionary of arguments for writers example_kwargs (dict): kwargs for running the provided example","title":"Dataloader"},{"location":"api/dataloader/#get_dataloader_descr","text":"get_dataloader_descr(dataloader, source='kipoi') Get dataloder description Arguments datalaoder : dataloader's relative path/name in the source. 2nd column in the kipoi.list_dataloader() pd.DataFrame`. source : Model source. 1st column in the kipoi.list_models() pd.DataFrame .","title":"get_dataloader_descr"},{"location":"api/dataloader/#get_dataloader","text":"get_dataloader(dataloader, source='kipoi') Loads the dataloader Arguments dataloader (str) : dataloader name source (str) : source name Returns Instance of class inheriting from kipoi.data.BaseDataLoader (like kipoi.data.Dataset ) decorated with additional attributes. Methods batch_iter(batch_size, num_workers, **kwargs) Arguments batch_size : batch size num_workers : Number of workers to use in parallel. **kwargs : Other kwargs specific to each dataloader Yields dict with \"inputs\" , \"targets\" and \"metadata\" batch_train_iter(cycle=True, **kwargs) Arguments cycle : if True, cycle indefinitely **kwargs : Kwargs passed to batch_iter() like batch_size Yields tuple of (\"inputs\", \"targets\") from the usual dict returned by batch_iter() batch_predict_iter(**kwargs) Arguments **kwargs : Kwargs passed to batch_iter() like batch_size Yields \"inputs\" field from the usual dict returned by batch_iter() load_all(**kwargs) - load the whole dataset into memory Arguments **kwargs : Kwargs passed to batch_iter() like batch_size Returns dict with \"inputs\" , \"targets\" and \"metadata\" init_example() - instantiate the dataloader with example kwargs print_args() - print information about the required arguments Appended attributes type (str): dataloader type (class name) defined_as (str): path and dataloader name args (list of kipoi.specs.DataLoaderArgument): datalaoder argument description info (kipoi.specs.Info): general information about the dataloader schema (kipoi.specs.DataloaderSchema): information about the input/output data modalities dependencies (kipoi.specs.Dependencies): class specifying the dependencies. (implements install method for running the installation) name (str): model name source (str): model source source_dir (str): local path to model source storage writers (dict): dictionary of arguments for writers example_kwargs (dict): kwargs for running the provided example","title":"get_dataloader"},{"location":"api/install_/","text":"install_model_requirements install_model_requirements(model, source='kipoi', and_dataloaders=True) Install model dependencies Arguments model (str) : model name source (str) : model source and_dataloaders (bool) : if True, install also the dependencies for the default dataloader .. deprecated :: 0.6.8 This will be removed in 0.7.0. installing packages in a running python env is error prone. Use command line interface of kipoi to install packages. install_dataloader_requirements install_dataloader_requirements(dataloader, source='kipoi') Install dataloader dependencies Arguments datalaoder (str) : dataloader name source (str) : model source .. deprecated :: 0.6.8 This will be removed in 0.7.0. installing packages in a running python env is error prone. Use command line interface of kipoi to install packages.","title":"install"},{"location":"api/install_/#install_model_requirements","text":"install_model_requirements(model, source='kipoi', and_dataloaders=True) Install model dependencies Arguments model (str) : model name source (str) : model source and_dataloaders (bool) : if True, install also the dependencies for the default dataloader .. deprecated :: 0.6.8 This will be removed in 0.7.0. installing packages in a running python env is error prone. Use command line interface of kipoi to install packages.","title":"install_model_requirements"},{"location":"api/install_/#install_dataloader_requirements","text":"install_dataloader_requirements(dataloader, source='kipoi') Install dataloader dependencies Arguments datalaoder (str) : dataloader name source (str) : model source .. deprecated :: 0.6.8 This will be removed in 0.7.0. installing packages in a running python env is error prone. Use command line interface of kipoi to install packages.","title":"install_dataloader_requirements"},{"location":"api/list_/","text":"list_models list_models(sources=None) List models Arguments sources : list of model sources to use. If None, use all Returns pandas.DataFrame list_dataloaders list_dataloaders(sources=None) List dataloaders Arguments sources : list of model sources to use. If None, use all Returns pandas.DataFrame","title":"list"},{"location":"api/list_/#list_models","text":"list_models(sources=None) List models Arguments sources : list of model sources to use. If None, use all Returns pandas.DataFrame","title":"list_models"},{"location":"api/list_/#list_dataloaders","text":"list_dataloaders(sources=None) List dataloaders Arguments sources : list of model sources to use. If None, use all Returns pandas.DataFrame","title":"list_dataloaders"},{"location":"api/metadata/","text":"kipoi.metadata Module defining different metadata classes compatible with dataloaders All classes inherit from collections.abc.Mapping which allows to use kipoi.data_utils.numpy_collate on them (e.g. they behave as a dictionary). GenomicRanges GenomicRanges(self, chr, start, end, id, strand='*') Container for genomic interval(s) All fields can be either a single values (str or int) or a numpy array of values. Arguments chr (str or np.array) : Chromosome(s) start (int or np.array) : Interval start (0-based coordinates) end (int or np.array) : Interval end (0-based coordinates) id (str or np.array) : Interval id strand (str or np.array) : Interval strand (\"+\", \"-\", or \"*\") from_interval GenomicRanges.from_interval(interval) Create the ranges object from pybedtools.Interval Arguments interval : pybedtools.Interval instance to_interval GenomicRanges.to_interval(self) Convert GenomicRanges object to a Interval object Returns (pybedtools.Interval or list[pybedtools.Interval])","title":"metadata"},{"location":"api/metadata/#kipoimetadata","text":"Module defining different metadata classes compatible with dataloaders All classes inherit from collections.abc.Mapping which allows to use kipoi.data_utils.numpy_collate on them (e.g. they behave as a dictionary).","title":"kipoi.metadata"},{"location":"api/metadata/#genomicranges","text":"GenomicRanges(self, chr, start, end, id, strand='*') Container for genomic interval(s) All fields can be either a single values (str or int) or a numpy array of values. Arguments chr (str or np.array) : Chromosome(s) start (int or np.array) : Interval start (0-based coordinates) end (int or np.array) : Interval end (0-based coordinates) id (str or np.array) : Interval id strand (str or np.array) : Interval strand (\"+\", \"-\", or \"*\")","title":"GenomicRanges"},{"location":"api/metadata/#from_interval","text":"GenomicRanges.from_interval(interval) Create the ranges object from pybedtools.Interval Arguments interval : pybedtools.Interval instance","title":"from_interval"},{"location":"api/metadata/#to_interval","text":"GenomicRanges.to_interval(self) Convert GenomicRanges object to a Interval object Returns (pybedtools.Interval or list[pybedtools.Interval])","title":"to_interval"},{"location":"api/model/","text":"get_model_descr get_model_descr(model, source='kipoi') Get model description Arguments model : model's relative path/name in the source. 2nd column in the kipoi.list_models() pd.DataFrame`. source : Model source. 1st column in the kipoi.list_models() pd.DataFrame . get_model get_model(model, source='kipoi', with_dataloader=True, **kwargs) Load the model from source , as well as the default dataloder to model.default_dataloder. Arguments model (str) : model name source (str) : source name with_dataloader (bool) : if True, the default dataloader is loaded to model.default_dataloader and the pipeline at model.pipeline enabled. Returns Instance of class inheriting from kipoi.models.BaseModel (like kipoi.models.KerasModel ) decorated with additional attributes. Methods predict_on_batch(x) : Make model predictions given a batch of data x Appended attributes type ( str ): model type (class name) args ( dict ): model args used to instantiate the model class info ( kipoi.specs.Info ): information about the author (etc) schema ( kipoi.specs.ModelSchema ): information about the input/outputdata modalities dependencies ( kipoi.specs.Dependencies ): class specifying the dependencies. (implements install method for running the installation) default_dataloader (class inheriting from kipoi.data.BaseDataLoader ): default dataloader. None if with_dataloader=False was used. name ( str ): model name source ( str ): model source source_dir ( str ): local path to model source storage writers (dict): dictionary of arguments for writers pipeline ( kipoi.pipeline.Pipeline ): handle to a Pipeline object","title":"Model"},{"location":"api/model/#get_model_descr","text":"get_model_descr(model, source='kipoi') Get model description Arguments model : model's relative path/name in the source. 2nd column in the kipoi.list_models() pd.DataFrame`. source : Model source. 1st column in the kipoi.list_models() pd.DataFrame .","title":"get_model_descr"},{"location":"api/model/#get_model","text":"get_model(model, source='kipoi', with_dataloader=True, **kwargs) Load the model from source , as well as the default dataloder to model.default_dataloder. Arguments model (str) : model name source (str) : source name with_dataloader (bool) : if True, the default dataloader is loaded to model.default_dataloader and the pipeline at model.pipeline enabled. Returns Instance of class inheriting from kipoi.models.BaseModel (like kipoi.models.KerasModel ) decorated with additional attributes. Methods predict_on_batch(x) : Make model predictions given a batch of data x Appended attributes type ( str ): model type (class name) args ( dict ): model args used to instantiate the model class info ( kipoi.specs.Info ): information about the author (etc) schema ( kipoi.specs.ModelSchema ): information about the input/outputdata modalities dependencies ( kipoi.specs.Dependencies ): class specifying the dependencies. (implements install method for running the installation) default_dataloader (class inheriting from kipoi.data.BaseDataLoader ): default dataloader. None if with_dataloader=False was used. name ( str ): model name source ( str ): model source source_dir ( str ): local path to model source storage writers (dict): dictionary of arguments for writers pipeline ( kipoi.pipeline.Pipeline ): handle to a Pipeline object","title":"get_model"},{"location":"api/pipeline/","text":"Pipeline Pipeline(self, model, dataloader_cls) Runs model predictions from raw files: raw files --(dataloader)--> data batches --(model)--> prediction Arguments model : model returned by kipoi.get_model dataloader_cls : dataloader class returned by kipoi.get_dataloader_factory of kipoi.get_model().default_dataloader predict_example Pipeline.predict_example(self, batch_size=32, output_file=None, **kwargs) Run model prediction for the example file Arguments batch_size : batch_size output_file : if not None, inputs and predictions are stored to output_file path **kwargs : Further arguments passed to batch_iter predict Pipeline.predict(self, dataloader_kwargs, batch_size=32, **kwargs) Arguments dataloader_kwargs : Keyword arguments passed to the pre-processor **kwargs : Further arguments passed to batch_iter Returns np.array, dict, list : Predict the whole array predict_generator Pipeline.predict_generator(self, dataloader_kwargs, batch_size=32, layer=None, **kwargs) Prediction generator Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Size of batches produced by the dataloader layer : If not None activation of specified layer will be returned. Only possible for models that are a subclass of LayerActivationMixin . **kwargs : Further arguments passed to batch_iter Yields dict : model batch prediction predict_to_file Pipeline.predict_to_file(self, output_file, dataloader_kwargs, batch_size=32, keep_inputs=False, keep_metadata=False, **kwargs) Make predictions and write them iteratively to a file Arguments output_file : output file path. File format is inferred from the file path ending. Available file formats are: 'bed', 'h5', 'hdf5', 'tsv' dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader keep_inputs : if True, inputs and targets will also be written to the output file. keep_metadata : if True, metadata will also be written to the output file. **kwargs : Further arguments passed to batch_iter input_grad Pipeline.input_grad(self, dataloader_kwargs, batch_size=32, filter_idx=None, avg_func=None, layer=None, final_layer=True, selected_fwd_node=None, pre_nonlinearity=False, **kwargs) Get input gradients Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader filter_idx : filter index of layer for which the gradient should be returned avg_func : String name of averaging function to be applied across filters in layer layer layer : layer from which backwards the gradient should be calculated final_layer : Use the final (classification) layer as layer selected_fwd_node : None - not supported by KerasModel at the moment pre_nonlinearity : Try to use the layer output prior to activation (will not always be possible in an automatic way) **kwargs : Further arguments passed to input_grad Returns dict : A dictionary of all model inputs and the gradients. Gradients are stored in key 'grads' input_grad_generator Pipeline.input_grad_generator(self, dataloader_kwargs, batch_size=32, filter_idx=None, avg_func=None, layer=None, final_layer=True, selected_fwd_node=None, pre_nonlinearity=False, **kwargs) Get input gradients Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader filter_idx : filter index of layer for which the gradient should be returned avg_func : String name of averaging function to be applied across filters in layer layer layer : layer from which backwards the gradient should be calculated final_layer : Use the final (classification) layer as layer selected_fwd_node : None - not supported by KerasModel at the moment pre_nonlinearity : Try to use the layer output prior to activation (will not always be possible in an automatic way) **kwargs : Further arguments passed to input_grad Yields dict : A dictionary of all model inputs and the gradients. Gradients are stored in key 'grads'","title":"Pipeline"},{"location":"api/pipeline/#pipeline","text":"Pipeline(self, model, dataloader_cls) Runs model predictions from raw files: raw files --(dataloader)--> data batches --(model)--> prediction Arguments model : model returned by kipoi.get_model dataloader_cls : dataloader class returned by kipoi.get_dataloader_factory of kipoi.get_model().default_dataloader","title":"Pipeline"},{"location":"api/pipeline/#predict_example","text":"Pipeline.predict_example(self, batch_size=32, output_file=None, **kwargs) Run model prediction for the example file Arguments batch_size : batch_size output_file : if not None, inputs and predictions are stored to output_file path **kwargs : Further arguments passed to batch_iter","title":"predict_example"},{"location":"api/pipeline/#predict","text":"Pipeline.predict(self, dataloader_kwargs, batch_size=32, **kwargs) Arguments dataloader_kwargs : Keyword arguments passed to the pre-processor **kwargs : Further arguments passed to batch_iter Returns np.array, dict, list : Predict the whole array","title":"predict"},{"location":"api/pipeline/#predict_generator","text":"Pipeline.predict_generator(self, dataloader_kwargs, batch_size=32, layer=None, **kwargs) Prediction generator Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Size of batches produced by the dataloader layer : If not None activation of specified layer will be returned. Only possible for models that are a subclass of LayerActivationMixin . **kwargs : Further arguments passed to batch_iter Yields dict : model batch prediction","title":"predict_generator"},{"location":"api/pipeline/#predict_to_file","text":"Pipeline.predict_to_file(self, output_file, dataloader_kwargs, batch_size=32, keep_inputs=False, keep_metadata=False, **kwargs) Make predictions and write them iteratively to a file Arguments output_file : output file path. File format is inferred from the file path ending. Available file formats are: 'bed', 'h5', 'hdf5', 'tsv' dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader keep_inputs : if True, inputs and targets will also be written to the output file. keep_metadata : if True, metadata will also be written to the output file. **kwargs : Further arguments passed to batch_iter","title":"predict_to_file"},{"location":"api/pipeline/#input_grad","text":"Pipeline.input_grad(self, dataloader_kwargs, batch_size=32, filter_idx=None, avg_func=None, layer=None, final_layer=True, selected_fwd_node=None, pre_nonlinearity=False, **kwargs) Get input gradients Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader filter_idx : filter index of layer for which the gradient should be returned avg_func : String name of averaging function to be applied across filters in layer layer layer : layer from which backwards the gradient should be calculated final_layer : Use the final (classification) layer as layer selected_fwd_node : None - not supported by KerasModel at the moment pre_nonlinearity : Try to use the layer output prior to activation (will not always be possible in an automatic way) **kwargs : Further arguments passed to input_grad Returns dict : A dictionary of all model inputs and the gradients. Gradients are stored in key 'grads'","title":"input_grad"},{"location":"api/pipeline/#input_grad_generator","text":"Pipeline.input_grad_generator(self, dataloader_kwargs, batch_size=32, filter_idx=None, avg_func=None, layer=None, final_layer=True, selected_fwd_node=None, pre_nonlinearity=False, **kwargs) Get input gradients Arguments dataloader_kwargs : Keyword arguments passed to the dataloader batch_size : Batch size used for the dataloader filter_idx : filter index of layer for which the gradient should be returned avg_func : String name of averaging function to be applied across filters in layer layer layer : layer from which backwards the gradient should be calculated final_layer : Use the final (classification) layer as layer selected_fwd_node : None - not supported by KerasModel at the moment pre_nonlinearity : Try to use the layer output prior to activation (will not always be possible in an automatic way) **kwargs : Further arguments passed to input_grad Yields dict : A dictionary of all model inputs and the gradients. Gradients are stored in key 'grads'","title":"input_grad_generator"},{"location":"api/readers/","text":"kipoi.readers Readers useful for creating new dataloaders HDF5Reader HDF5Reader HDF5Reader(self, file_path) Read the HDF5 file. Convenience wrapper around h5py.File Arguments file_path : File path to an HDF5 file ls HDF5Reader.ls(self) Recursively list the arrays load_all HDF5Reader.load_all(self, unflatten=True) Load the whole file Arguments unflatten : if True, nest/unflatten the keys. e.g. an entry f['/foo/bar'] would need to be accessed using two nested get call : f['foo']['bar'] batch_iter HDF5Reader.batch_iter(self, batch_size=16, **kwargs) Create a batch iterator over the whole file Arguments batch_size : batch size **kwargs : ignored argument. Used for consistency with other dataloaders open HDF5Reader.open(self) Open the file close HDF5Reader.close(self) Close the file load HDF5Reader.load(file_path, unflatten=True) Load the data all at once (classmethod). Arguments file_path : HDF5 file path unflatten : see load_all ZarrReader ZarrReader(self, file_path) Read the Zarr file. Convenience wrapper around zarr.group Arguments file_path : File path to an Zarr file ls ZarrReader.ls(self) Recursively list the arrays load_all ZarrReader.load_all(self, unflatten=True) Load the whole file Arguments unflatten : if True, nest/unflatten the keys. e.g. an entry f['/foo/bar'] would need to be accessed using two nested get call : f['foo']['bar'] batch_iter ZarrReader.batch_iter(self, batch_size=16, **kwargs) Create a batch iterator over the whole file Arguments batch_size : batch size **kwargs : ignored argument. Used for consistency with other dataloaders open ZarrReader.open(self) Open the file close ZarrReader.close(self) Close the file load ZarrReader.load(file_path, unflatten=True) Load the data all at once (classmethod). Arguments file_path : Zarr file path unflatten : see load_all","title":"readers"},{"location":"api/readers/#kipoireaders","text":"Readers useful for creating new dataloaders HDF5Reader","title":"kipoi.readers"},{"location":"api/readers/#hdf5reader","text":"HDF5Reader(self, file_path) Read the HDF5 file. Convenience wrapper around h5py.File Arguments file_path : File path to an HDF5 file","title":"HDF5Reader"},{"location":"api/readers/#ls","text":"HDF5Reader.ls(self) Recursively list the arrays","title":"ls"},{"location":"api/readers/#load_all","text":"HDF5Reader.load_all(self, unflatten=True) Load the whole file Arguments unflatten : if True, nest/unflatten the keys. e.g. an entry f['/foo/bar'] would need to be accessed using two nested get call : f['foo']['bar']","title":"load_all"},{"location":"api/readers/#batch_iter","text":"HDF5Reader.batch_iter(self, batch_size=16, **kwargs) Create a batch iterator over the whole file Arguments batch_size : batch size **kwargs : ignored argument. Used for consistency with other dataloaders","title":"batch_iter"},{"location":"api/readers/#open","text":"HDF5Reader.open(self) Open the file","title":"open"},{"location":"api/readers/#close","text":"HDF5Reader.close(self) Close the file","title":"close"},{"location":"api/readers/#load","text":"HDF5Reader.load(file_path, unflatten=True) Load the data all at once (classmethod). Arguments file_path : HDF5 file path unflatten : see load_all","title":"load"},{"location":"api/readers/#zarrreader","text":"ZarrReader(self, file_path) Read the Zarr file. Convenience wrapper around zarr.group Arguments file_path : File path to an Zarr file","title":"ZarrReader"},{"location":"api/readers/#ls_1","text":"ZarrReader.ls(self) Recursively list the arrays","title":"ls"},{"location":"api/readers/#load_all_1","text":"ZarrReader.load_all(self, unflatten=True) Load the whole file Arguments unflatten : if True, nest/unflatten the keys. e.g. an entry f['/foo/bar'] would need to be accessed using two nested get call : f['foo']['bar']","title":"load_all"},{"location":"api/readers/#batch_iter_1","text":"ZarrReader.batch_iter(self, batch_size=16, **kwargs) Create a batch iterator over the whole file Arguments batch_size : batch size **kwargs : ignored argument. Used for consistency with other dataloaders","title":"batch_iter"},{"location":"api/readers/#open_1","text":"ZarrReader.open(self) Open the file","title":"open"},{"location":"api/readers/#close_1","text":"ZarrReader.close(self) Close the file","title":"close"},{"location":"api/readers/#load_1","text":"ZarrReader.load(file_path, unflatten=True) Load the data all at once (classmethod). Arguments file_path : Zarr file path unflatten : see load_all","title":"load"},{"location":"api/sources/","text":"get_source get_source(source) Get the source object Arguments source (str) : source string Returns Source child class instance : like kipoi.sources.GitLFSSource list_sources list_sources() Returns a pandas.DataFrame of possible sources","title":"sources"},{"location":"api/sources/#get_source","text":"get_source(source) Get the source object Arguments source (str) : source string Returns Source child class instance : like kipoi.sources.GitLFSSource","title":"get_source"},{"location":"api/sources/#list_sources","text":"list_sources() Returns a pandas.DataFrame of possible sources","title":"list_sources"},{"location":"api/writers/","text":"kipoi.writers Writers used in kipoi predict TsvBatchWriter BedBatchWriter HDF5BatchWriter RegionWriter BedGraphWriter BigWigWriter TsvBatchWriter TsvBatchWriter(self, file_path, nested_sep='/') Tab-separated file writer Arguments file_path (str) : File path of the output tsv file nested_sep : What separator to use for flattening the nested dictionary structure into a single key batch_write TsvBatchWriter.batch_write(self, batch) Write a batch of data Arguments batch : batch of data. Either a single np.array or a list/dict thereof. BedBatchWriter BedBatchWriter(self, file_path, metadata_schema, header=True) Bed-file writer Arguments file_path (str) : File path of the output tsv file dataloader_schema : Schema of the dataloader. Used to find the ranges object nested_sep : What separator to use for flattening the nested dictionary structure into a single key batch_write BedBatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof. HDF5BatchWriter HDF5BatchWriter(self, file_path, chunk_size=10000, compression='gzip') HDF5 file writer Arguments file_path (str) : File path of the output .h5 file chunk_size (str) : Chunk size for storing the files nested_sep : What separator to use for flattening the nested dictionary structure into a single key compression (str) : default compression to use for the hdf5 datasets. see also : http://docs.h5py.org/en/latest/high/dataset.html#dataset-compression batch_write HDF5BatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof. close HDF5BatchWriter.close(self) Close the file handle dump HDF5BatchWriter.dump(file_path, batch) In a single shot write the batch/data to a file and close the file. Arguments file_path : file path batch : batch of data. Either a single np.array or a list/dict thereof. ZarrBatchWriter ZarrBatchWriter(self, file_path, chunk_size=10000, store=None, string_dtype=None, compressor=None) Zarr file writer Arguments file_path (str) : File path of the output zarr file chunk_size (str) : Chunk size for storing the files store : zarr.storage. If not specified, it's inferred from the file-name. For example : .lmdb.zarr uses LMDB, .zip.zarr uses Zip, and no special suffix uses DirectoryStore compressor (str) : Zarr compressor from numcodecs. Example: from numcodecs import Blosc compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE) string_dtype : how to encode the string. If None, variable length is used batch_write ZarrBatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof. close ZarrBatchWriter.close(self) Close the file handle dump ZarrBatchWriter.dump(file_path, batch) In a single shot write the batch/data to a file and close the file. Arguments file_path : file path batch : batch of data. Either a single np.array or a list/dict thereof. BedGraphWriter BedGraphWriter(self, file_path) Arguments file_path (str) : File path of the output bedgraph file region_write BedGraphWriter.region_write(self, region, data) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first. write_entry BedGraphWriter.write_entry(self, chr, start, end, value) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first. close BedGraphWriter.close(self) Close the file BigWigWriter BigWigWriter(self, file_path, genome_file=None, chrom_sizes=None, is_sorted=True) Arguments file_path (str) : File path of the output BigWig file genome_file : genome file containing chromosome sizes. Can be None. Can be overriden by chrom_sizes . chrom_sizes : a list of tuples containing chromosome sizes. If not None, it overrided genome_file . is_sorted : if True, the provided entries need to be sorted beforehand Note : One of genome_file or chrom_sizes shouldn't be None. region_write BigWigWriter.region_write(self, region, data) Write region to file. Note: the written regions need to be sorted beforehand. Arguments region : a kipoi.metadata.GenomicRanges , pybedtools.Interval or a dictionary with at least keys: \"chr\", \"start\", \"end\" and list-values. Example : {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D-array of values to be written - where the 0th entry is at 0-based \"start\" close BigWigWriter.close(self) Close the file","title":"writers"},{"location":"api/writers/#kipoiwriters","text":"Writers used in kipoi predict TsvBatchWriter BedBatchWriter HDF5BatchWriter RegionWriter BedGraphWriter BigWigWriter","title":"kipoi.writers"},{"location":"api/writers/#tsvbatchwriter","text":"TsvBatchWriter(self, file_path, nested_sep='/') Tab-separated file writer Arguments file_path (str) : File path of the output tsv file nested_sep : What separator to use for flattening the nested dictionary structure into a single key","title":"TsvBatchWriter"},{"location":"api/writers/#batch_write","text":"TsvBatchWriter.batch_write(self, batch) Write a batch of data Arguments batch : batch of data. Either a single np.array or a list/dict thereof.","title":"batch_write"},{"location":"api/writers/#bedbatchwriter","text":"BedBatchWriter(self, file_path, metadata_schema, header=True) Bed-file writer Arguments file_path (str) : File path of the output tsv file dataloader_schema : Schema of the dataloader. Used to find the ranges object nested_sep : What separator to use for flattening the nested dictionary structure into a single key","title":"BedBatchWriter"},{"location":"api/writers/#batch_write_1","text":"BedBatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof.","title":"batch_write"},{"location":"api/writers/#hdf5batchwriter","text":"HDF5BatchWriter(self, file_path, chunk_size=10000, compression='gzip') HDF5 file writer Arguments file_path (str) : File path of the output .h5 file chunk_size (str) : Chunk size for storing the files nested_sep : What separator to use for flattening the nested dictionary structure into a single key compression (str) : default compression to use for the hdf5 datasets. see also : http://docs.h5py.org/en/latest/high/dataset.html#dataset-compression","title":"HDF5BatchWriter"},{"location":"api/writers/#batch_write_2","text":"HDF5BatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof.","title":"batch_write"},{"location":"api/writers/#close","text":"HDF5BatchWriter.close(self) Close the file handle","title":"close"},{"location":"api/writers/#dump","text":"HDF5BatchWriter.dump(file_path, batch) In a single shot write the batch/data to a file and close the file. Arguments file_path : file path batch : batch of data. Either a single np.array or a list/dict thereof.","title":"dump"},{"location":"api/writers/#zarrbatchwriter","text":"ZarrBatchWriter(self, file_path, chunk_size=10000, store=None, string_dtype=None, compressor=None) Zarr file writer Arguments file_path (str) : File path of the output zarr file chunk_size (str) : Chunk size for storing the files store : zarr.storage. If not specified, it's inferred from the file-name. For example : .lmdb.zarr uses LMDB, .zip.zarr uses Zip, and no special suffix uses DirectoryStore compressor (str) : Zarr compressor from numcodecs. Example: from numcodecs import Blosc compressor = Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE) string_dtype : how to encode the string. If None, variable length is used","title":"ZarrBatchWriter"},{"location":"api/writers/#batch_write_3","text":"ZarrBatchWriter.batch_write(self, batch) Write a batch of data to bed file Arguments batch : batch of data. Either a single np.array or a list/dict thereof.","title":"batch_write"},{"location":"api/writers/#close_1","text":"ZarrBatchWriter.close(self) Close the file handle","title":"close"},{"location":"api/writers/#dump_1","text":"ZarrBatchWriter.dump(file_path, batch) In a single shot write the batch/data to a file and close the file. Arguments file_path : file path batch : batch of data. Either a single np.array or a list/dict thereof.","title":"dump"},{"location":"api/writers/#bedgraphwriter","text":"BedGraphWriter(self, file_path) Arguments file_path (str) : File path of the output bedgraph file","title":"BedGraphWriter"},{"location":"api/writers/#region_write","text":"BedGraphWriter.region_write(self, region, data) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first.","title":"region_write"},{"location":"api/writers/#write_entry","text":"BedGraphWriter.write_entry(self, chr, start, end, value) Write region to file. Arguments region : Defines the region that will be written position by position. Example: {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D or 2D numpy array vector that has length \"end\" - \"start\". if 2D array is passed then data.sum(axis=1) is performed on it first.","title":"write_entry"},{"location":"api/writers/#close_2","text":"BedGraphWriter.close(self) Close the file","title":"close"},{"location":"api/writers/#bigwigwriter","text":"BigWigWriter(self, file_path, genome_file=None, chrom_sizes=None, is_sorted=True) Arguments file_path (str) : File path of the output BigWig file genome_file : genome file containing chromosome sizes. Can be None. Can be overriden by chrom_sizes . chrom_sizes : a list of tuples containing chromosome sizes. If not None, it overrided genome_file . is_sorted : if True, the provided entries need to be sorted beforehand Note : One of genome_file or chrom_sizes shouldn't be None.","title":"BigWigWriter"},{"location":"api/writers/#region_write_1","text":"BigWigWriter.region_write(self, region, data) Write region to file. Note: the written regions need to be sorted beforehand. Arguments region : a kipoi.metadata.GenomicRanges , pybedtools.Interval or a dictionary with at least keys: \"chr\", \"start\", \"end\" and list-values. Example : {\"chr\":\"chr1\", \"start\":0, \"end\":4} . data : a 1D-array of values to be written - where the 0th entry is at 0-based \"start\"","title":"region_write"},{"location":"api/writers/#close_3","text":"BigWigWriter.close(self) Close the file","title":"close"},{"location":"contributing/01_Getting_started/","text":"Contributing models - Getting started Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the kipoi/models github repository. The minimum requirement for a model is that a model.yaml file is available in the model folder, which defines the type of the model, file paths / URLs, the dataloader, description, software dependencies, etc. We have compiled some of the standard use-cases of model contribution here. Please specify: // Definition of dynamic content var model_class = {\"keras\": \"kipoi.model.KerasModel\", \"tensorflow\": \"kipoi.model.TensorFlowModel\", \"pytorch\": \"kipoi.model.PyTorchModel\", \"scikitlearn\": \"kipoi.model.SklearnModel\", \"other\": \"my_model.MyModel # MyModel class defined in my_model.py\"}; var model_args = {\"keras\": `args: # arguments of kipoi.model.KerasModel arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc`, \"tensorflow\": `args: # arguments of kipoi.model.TensorFlowModel input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: https://zenodo.org/path/to/my/model.tf md5: 1234567890abc`, \"pytorch\": `args: # arguments of kipoi.model.PyTorchModel module_class: my_model.DummyModel # DummyModel defined in ./my_model.py module_kwargs: # Optional kwargs for the DummyModel initialisation x: 1 y: 2 z: 3 weights: # Path to the file containing the state_dict url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc`, \"scikitlearn\": `args: # arguments of kipoi.model.SklearnModel pkl_file: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc predict_method: predict_proba`, \"other\":`args: # Optional. Arguments to be passed to the model initialisation. file_path: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc my_param: 42`, }; var model_template_args = {\"keras\": `args: # arguments of kipoi.model.KerasModel arch: url: {{ model_arch_url }} # refers to models.tsv md5: {{ model_arch_md5 }} weights: url: {{ model_weights_url }} md5: {{ model_weights_md5 }}`, \"tensorflow\": `args: # arguments of kipoi.model.TensorFlowModel input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: {{ model_checkpoint_url }} # refers to models.tsv md5: {{ model_checkpoint_md5 }}`, \"pytorch\": `args: # arguments of kipoi.model.PyTorchModel module_class: my_model.DummyModel # DummyModel defined in my_model.py module_kwargs: # Optional kwargs for the DummyModel initialisation x: 1 y: 2 z: 3 weights: # Path to the file containing the state_dict url: {{ model_weights_url }} # refers to models.tsv md5: {{ model_weights_md5 }}`, \"scikitlearn\": `args: # arguments of kipoi.model.SklearnModel pkl_file: url: {{ model_pkl_url }} # refers to models.tsv md5: {{ model_pkl_md5 }} predict_method: predict_proba`, \"other\":`args: # Optional. Arguments to be passed to the model initialisation. file_path: url: {{ model_file_url }} # refers to models.tsv md5: {{ model_file_md5 }} my_param: 42`, } var models_tsv = {\"keras\": `model\\tmodel_arch_url\\tmodel_arch_md5\\tmodel_weights_url\\tmodel_weights_md5 my_model_1\\thttps://zenodo.org/path/to/my/architecture/file1\\t1234567890abc\\thttps://zenodo.org/path/to/my/model/weights1.h5\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/architecture/file2\\t1234567890abc\\thttps://zenodo.org/path/to/my/model/weights2.h5\\t1234567890abc`, \"tensorflow\": `model\\tmodel_checkpoint_url\\tmodel_checkpoint_md5 my_model_1\\thttps://zenodo.org/path/to/my/model1.tf\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model2.tf\\t1234567890abc`, \"pytorch\": `model\\tmodel_weights_url\\tmodel_weights_md5 my_model_1\\thttps://zenodo.org/path/to/my/model/weights1.pth\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model/weights2.pth\\t1234567890abc`, \"scikitlearn\": `model\\tmodel_pkl_url\\tmodel_pkl_md5 my_model_1\\thttps://zenodo.org/path/to/my/model1.pkl\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model2.pkl\\t1234567890abc`, \"other\":`model\\tmodel_file_url\\tmodel_file_md5 my_model_1\\thttps://zenodo.org/path/to/my/model1.pkl\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model2.pkl\\t1234567890abc`, }; var model_yaml_dl_entry ={ \"dna\":` defined_as: kipoiseq.dataloaders.SeqIntervalDl default_args: # Optional arguments to the SeqIntervalDl dataloader # See also https://kipoi.org/kipoiseq/dataloaders/#seqintervaldl auto_resize_len: 100 # Automatically resize sequence intervals alphabet_axis: 1 dummy_axis: 2 # Add a dummy axis. Omit in order not to create dummy_axis. alphabet: \"ACGT\" # Order of letters in 1-hot encoding ignore_targets: False # if True, dont return any target variables`, \"dnaAdditional\":`. #Refer to dataloader.yaml in the same folder as this file.`, \"splicing\":` defined_as: kipoiseq.dataloaders.MMSpliceDl default_args: # Optional arguments to the MMSpliceDl dataloader intron5prime_len: 100 # 5' intronic sequence length to take. intron3prime_len: 100 # 3' intronic sequence length to take.` }; var model_yaml = `defined_as: {{ model_class }} {{ model_args }} default_dataloader: {{ model_yaml_dl_entry }} info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting X cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Dataset Y. held-out chromosomes chr8, chr9 and chr22. license: MIT # Software License - if not set defaults to MIT # You can also specify the license in the LICENSE file dependencies: conda: # install via conda - python - h5py - pip pip: # install via pip - keras&gt;=2.0.4 - tensorflow&gt;=1.0 schema: # Model schema. The schema defintion is essential for kipoi plug-ins to work. inputs: # input = single numpy array shape: (100,4) # array shape of a single sample (omitting the batch dimension) doc: input feature description # inputs: # input = dictionary of fields # seq: # shape: (100,4) # doc: input feature description # other_track: # shape: (50,) # doc: input feature description targets: shape: (3,) doc: model prediction description `; var model_py = `from kipoi.model import BaseModel class MyModel(BaseModel): # Implement your Kipoi model def __init__(self, file_path, my_param): ... self.model = load_model_parameters(file_path) # Execute model prediction for input data def predict_on_batch(self, x): # The bare minimum that has to be defined return self.model.predict(x)`; var dataloader_yaml = `defined_as: dataloader.MyDataset # MyDataset impolemented in dataloader.py args: # MyDataset.__init__ argument description features_file: doc: intervals_file: bed3 file containing intervals # Test file URL's example: url: https://raw.githubusercontent.com/../intervals_51bp.tsv md5: a76e47b3df87fd514860cf27fdc10eb4 targets_file: doc: Reference genome FASTA file path. example: url: https://raw.githubusercontent.com/../hg38_chr22_32000000_32300000.fa md5: 01320157a250a3d2eea63e89ecf79eba ignore_targets: doc: if True, don't return any target variables optional: True # if not present, the \"targets\" will not be present info: authors: - name: Your Name github: your_github_account email: your_email@host.org doc: Data-loader returning one-hot encoded sequences given genome intervals dependencies: conda: - python - bioconda::pybedtools - bioconda::pysam - bioconda::pyfaidx - numpy - pandas - pip pip: - kipoiseq output_schema: # Define the dataloader output schema according to the returned values inputs: seq: shape: (100, 4) doc: One-hot encoded DNA sequence other_track: shape: (50,) doc: dummy track targets: shape: (None,) doc: (optional) values following the bed-entry metadata: # additional information about the samples ranges: type: GenomicRanges doc: Ranges describing inputs.seq`; var dataloader_py = `import numpy as np from kipoi.data import Dataset from kipoi.metadata import GenomicRanges from kipoiseq.dataloaders.sequence import BedDataset from kipoiseq.extractors import FastaStringExtractor from kipoiseq.transforms import OneHot class MyDataset(Dataset): \"\"\"Example re-implementation of kipoiseq.dataloaders.SeqIntervalDl Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file, ignore_targets=True): self.bt = BedDataset(intervals_file, bed_columns=3, ignore_targets=ignore_targets) self.fasta_file = fasta_file self.fasta_extractor = None self.transform = OneHot() # one-hot encode DNA sequence def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaStringExtractor(self.fasta_file) # get the intervals interval, targets = self.bed[idx] # resize to 100bp interval = resize_interval(interval, 100, anchor='center') # extract the sequence seq = self.fasta_extractors.extract(interval) # one-hot encode the sequence seq_onehot = self.transform(seq) return { \"inputs\": { \"seq\": seq_onehot, \"other_track\": np.ones((50, )) }, \"targets\": targets, # (optional field) \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } }`; which input data your model requires: Select... DNA sequence (one-hot encoded or string) DNA sequence with additional tracks DNA sequence splicing model Other model input in which framework your model is implemented: Select... Keras TensorFlow PyTorch Sci-Kit learn other whether you want to contribute a: Select... single model set of highly similar models (say models for different TFs) set of models that logically belong together, but may not be very similar .cond:{ visibility: hidden; } .hidden:{ visibility: hidden; } Preparation Before you start, make sure you have installed kipoi . Setting up your model For this example let's assume the model you want to submit is called MyModel . To submit your model you will have create the folder MyModel in you Kipoi model folder (default: ~/.kipoi/models ). In this folder you will have to create the following file(s): If you have trained multiple models that logically belong into one model-group as they are similar in function, but they individually require different preprocessing code then you are right here. To submit your model you will have to: Create a new local folder named after your model, e.g.: mkdir MyModel and within this folder create a folder structure so that every individual trained model has its own folder. Every folder that contains a model.yaml is then interpreted as an individual model by Kipoi. To make this clearer take a look at how FactorNet is structured: FactorNet . If you have files that are re-used in multiple models you can use symbolic links ( ln -s ) relative within the folder structure of your model group. For your selection the following files have to exist in every sub-folder that should act as an individual model: model.yaml model-template.yaml model.py models.tsv dataloader.yaml dataloader.py For this example let's assume the model you want to submit is called MyModel . To submit your model you will have to: Create a new local folder named like your model, e.g.: mkdir MyModel In the MyModel folder you will have to crate a model.yaml file: The model.yaml files acts as a configuration file for Kipoi. For an example take a look at Divergent421/model.yaml . For this example let's assume you have trained one model architecture on multiple similar datasets and can use the same preprocessing code for all models. Let's assume you want to call the model-group MyModel . To submit your model you will have to: Create a new local folder named after your model, e.g.: mkdir MyModel In the MyModel folder you will have to crate a model-template.yaml file: The model-template.yaml files acts as a configuration file for Kipoi. For an example take a look at CpGenie/model-template.yaml . As you can see instead of putting urls and parameters directly in the .yaml file you need to put {{ parameter_name }} in the yaml file. The values are then automatically loaded from a tab -delimited file called models.tsv that you also have to provide. For the previous example this would be: CpGenie/models.tsv . Using kipoi those models are then accessible by the model group name and the model name defined in the models.tsv . Model names may contain / s. In the model definition yaml file you see the defined_as keyword: Since your model is a Keras model, set it to kipoi.model.KerasModel . In the model definition yaml file you see the args keyword, which can be set the following way: KerasModel definition In the model definition yaml file you see the defined_as keyword: Since your model is a TensorFlow model, set it to kipoi.model.TensorFlowModel . In the model definition yaml file you see the args keyword, which can be set the following way: TensorFlowModel definition In the model definition yaml file you see the defined_as keyword: Since your model is a PyTorch model, set it to kipoi.model.PyTorchModel . In the model definition yaml file you see the args keyword, which can be set the following way: PyTorchModel definition In the model definition yaml file you see the defined_as keyword: Since your model is a scikit-learn model, set it to kipoi.model.SklearnModel . In the model definition yaml file you see the args keyword, which can be set the following way: SklearnModel definition Your model is not implemented in Keras , TensorFlow , PyTorch , nor sci-kit learn , so you will have to implement a custom python class inheriting from kipoi.model.Model . In the defined_as keyword of the model.yaml you will then have to refer to your definition by my_model_def.MyModel if the MyModel class is defined in the my_model_def.py that lies in the same folder as model.yaml . For details please see: defining custom models in model.yaml and writing a model.py file . Now set the software requirements correctly. This happens in the dependencies section of the model .yaml file. As you can see in the example the dependencies are split by conda and pip . Ideally you define the ranges of the versions of packages your model supports - otherwise it may fail at some point in future. If you need to specify a conda channel use the <channel>::<package> notation for conda dependencies. As you have seen in the presented example and in the model definition links it is necessary that prior to model contribution you have published all model files (except for python scripts and other configuration files) on zenodo or figshare to ensure functionality and versioning of models. If you want to test your model(s) locally before publishing them on zenodo or figshare you can replace the pair of url and md5 tags in the model definition yaml by the local path on your filesystem, e.g.: args: arch: path/to/my/arch.json But keep in mind that local paths are only good for testing and for models that you want to keep only locally. Setting up your dataloader Sice your model uses DNA sequence input the kipoiseq dataloaders are recommended to be used, as shown in the above example model definition .yaml file, which could for example be defined like this: default_dataloader : defined_as : kipoiseq .dataloaders .SeqIntervalDl default_args : auto_resize_len : 1001 alphabet_axis : 0 dummy_axis : 1 To see all the parameters and functions of the off-the-shelf dataloaders please take a look at kipoiseq . Since your model uses DNA sequence and additional annotation you have to define your own dataloader function or class. Depending on your use-case you may find some of the data-loader implementations of exiting models in the model zoo helpful. You may find the rbp_eclip dataloader or one of the FactorNet dataloaders relevant. Also consider taking advantage of elements implemented in the kipoiseq package. For you implementation you have to: set default_dataloader: . in the model.yaml file write a dataloader.yaml file as defined in writing dataloader.yaml . An example is this one . implement the dataloader in a dataloader.py file as defined in writing dataloader.py . An example is this one . put the dataloader.yaml and the dataloader.py in the same folder as model.yaml . Since your model uses input other than what is covered by the default data-loaders you have to define your own dataloader function or class. Depending on your use-case you may find some of the data-loader implementations of exiting models in the model zoo helpful. You may find the rbp_eclip dataloader or one of the FactorNet dataloaders relevant. Also consider taking advantage of elements implemented in the kipoiseq package. For you implementation you have to: set default_dataloader: . in the model.yaml file write a dataloader.yaml file as defined in writing dataloader.yaml . An example is this one . implement the dataloader in a dataloader.py file as defined in writing dataloader.py . An example is this one . put the dataloader.yaml and the dataloader.py in the same folder as model.yaml . Since your model is specialised in predicting properties of splice sites you are encouraged to take a look at the dataloaders implemented for the kipoi models tagged as RNA splicing models, such as HAL , labranchor , or MMSplice . If the MMSplice dataloader in the above example does not fit your needs, you have to: set default_dataloader: . in the model.yaml file write a dataloader.yaml file as defined in writing dataloader.yaml . implement the dataloader in a dataloader.py file as defined in writing dataloader.py . put the dataloader.yaml and the dataloader.py in the same folder as model.yaml . Info and model schema Please update the model description, the authors and the data it the model was trained in the info section of the model .yaml file. Please explain explicitly what your model does etc. Think what you would want to know if you didn't know anything about the model. Now fillout the model schema ( schema tag) as explained here: model schema . License Please make sure that the license that is defined in the license: tag in the yaml file is correct. Also only contribute models for which you have the rights to do so and only contribute models that permit redistribution. Testing Now it is time to test your model. If you are in the model directory run the command: kipoi test . in your model folder to test whether the general setup is correct. When this was successful run kipoi test-source dir --all to test whether all the software dependencies of the model are setup correctly and the automated tests will pass. Testing Now it is time to test your models. For the following let's assume your model group is called MyModel and your have two models in the group, which are MyModel/ModelA and MyModel/ModelB then you should should make sure you are in the MyModel folder and run the commands kipoi test ./ModelA and kipoi test ./ModelB . When this was successful run kipoi test-source dir --all to test whether all the software dependencies of the model and dataloader are setup correctly. Forking and submitting Make sure your model repository is up to date: git pull Commit your changes git add MyModel/ git commit -m \"Added <MyModel>\" Fork the https://github.com/kipoi/models repo on github (click on the Fork button) Add your fork as a git remote to ~/.kipoi/models git remote add fork https://github.com/<username>/models.git Push to your fork git push fork master Submit a pull-request On github click the New pull request button on your github fork - https://github.com/<username>/models> get_model_yaml_code = function(){ var sel_inp = $('#sel_inp').val(); var sel_fw = $('#sel_fw').val(); return Mustache.render(model_yaml, {model_class: model_class[sel_fw], model_args: model_args[sel_fw], model_yaml_dl_entry: model_yaml_dl_entry[sel_inp]}); } get_model_template_yaml_code = function(){ var sel_inp = $('#sel_inp').val(); var sel_fw = $('#sel_fw').val(); return Mustache.render(model_yaml, {model_class: model_class[sel_fw], model_args: model_template_args[sel_fw], model_yaml_dl_entry: model_yaml_dl_entry[sel_inp]}); } get_models_tsv_code = function(){ var sel_fw = $('#sel_fw').val(); return models_tsv[sel_fw]; } get_model_py_code = function(){ return model_py; } get_dataloader_py_code = function(){ return dataloader_py; } get_dataloader_yaml_code = function(){ return dataloader_yaml; } function copyToClipboard(text){ //https://stackoverflow.com/questions/33855641/copy-output-of-a-javascript-variable-to-the-clipboard var dummy = document.createElement(\"input\"); document.body.appendChild(dummy); dummy.setAttribute('value', unescape(text)); dummy.select(); document.execCommand(\"copy\"); document.body.removeChild(dummy); } insert_code_data = function(){ var sel_inp = $('#sel_inp').val(); var sel_fw = $('#sel_fw').val(); var sel_mg = $('#sel_mg').val(); //$(\"#model_yaml_raw_code\").val(unescape(get_model_yaml_code())); // for copying to clipboard $(\"#model_yaml_code\").html(get_model_yaml_code()); $(\"#model-template_yaml_code\").html(get_model_template_yaml_code()); $(\"#models_tsv_code\").html(get_models_tsv_code()); $(\"#model_py_code\").html(get_model_py_code()); $(\"#dataloader_py_code\").html(get_dataloader_py_code()); $(\"#dataloader_yaml_code\").html(get_dataloader_yaml_code()); $('.tab-pane').each(function(i, block) { hljs.highlightBlock(block); }); } refresh_info = function(){ $('.cond').hide(); var sel_inp = $('#sel_inp').val(); var sel_fw = $('#sel_fw').val(); var sel_mg = $('#sel_mg').val(); // deactivate the code tabs $(\".nav-tabs\").children().removeClass(\"active\") $(\".tab-pane\").removeClass(\"active\") if (($.inArray(sel_inp, ['dna', 'dnaAdditional', 'splicing', 'otherInput'])>-1) && ($.inArray(sel_fw, ['keras', 'tensorflow', 'pytorch', 'scikitlearn', 'other'])>-1) && ($.inArray(sel_mg, ['single', 'setSim', 'setDiff'])>-1)){ insert_code_data(); if (sel_mg == \"setSim\"){ //assign active class to top-tab-model-template_yaml and tab-model-template_yaml $(\"#top-tab-model-template_yaml\").addClass(\"active\"); $(\"#tab-model-template_yaml\").addClass(\"active\"); } else { //assign active class to top-tab-model_yaml and tab-model_yaml $(\"#top-tab-model_yaml\").addClass(\"active\"); $(\"#tab-model_yaml\").addClass(\"active\"); } $(\".anyExpl\").show(); $(\".forking\").show(); $(\".\"+sel_inp).show(); $(\".\"+sel_fw).show(); $(\".\"+sel_mg).show(); } } refresh_info();","title":"Getting started"},{"location":"contributing/01_Getting_started/#contributing-models-getting-started","text":"Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the kipoi/models github repository. The minimum requirement for a model is that a model.yaml file is available in the model folder, which defines the type of the model, file paths / URLs, the dataloader, description, software dependencies, etc. We have compiled some of the standard use-cases of model contribution here. Please specify: // Definition of dynamic content var model_class = {\"keras\": \"kipoi.model.KerasModel\", \"tensorflow\": \"kipoi.model.TensorFlowModel\", \"pytorch\": \"kipoi.model.PyTorchModel\", \"scikitlearn\": \"kipoi.model.SklearnModel\", \"other\": \"my_model.MyModel # MyModel class defined in my_model.py\"}; var model_args = {\"keras\": `args: # arguments of kipoi.model.KerasModel arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc`, \"tensorflow\": `args: # arguments of kipoi.model.TensorFlowModel input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: https://zenodo.org/path/to/my/model.tf md5: 1234567890abc`, \"pytorch\": `args: # arguments of kipoi.model.PyTorchModel module_class: my_model.DummyModel # DummyModel defined in ./my_model.py module_kwargs: # Optional kwargs for the DummyModel initialisation x: 1 y: 2 z: 3 weights: # Path to the file containing the state_dict url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc`, \"scikitlearn\": `args: # arguments of kipoi.model.SklearnModel pkl_file: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc predict_method: predict_proba`, \"other\":`args: # Optional. Arguments to be passed to the model initialisation. file_path: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc my_param: 42`, }; var model_template_args = {\"keras\": `args: # arguments of kipoi.model.KerasModel arch: url: {{ model_arch_url }} # refers to models.tsv md5: {{ model_arch_md5 }} weights: url: {{ model_weights_url }} md5: {{ model_weights_md5 }}`, \"tensorflow\": `args: # arguments of kipoi.model.TensorFlowModel input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: {{ model_checkpoint_url }} # refers to models.tsv md5: {{ model_checkpoint_md5 }}`, \"pytorch\": `args: # arguments of kipoi.model.PyTorchModel module_class: my_model.DummyModel # DummyModel defined in my_model.py module_kwargs: # Optional kwargs for the DummyModel initialisation x: 1 y: 2 z: 3 weights: # Path to the file containing the state_dict url: {{ model_weights_url }} # refers to models.tsv md5: {{ model_weights_md5 }}`, \"scikitlearn\": `args: # arguments of kipoi.model.SklearnModel pkl_file: url: {{ model_pkl_url }} # refers to models.tsv md5: {{ model_pkl_md5 }} predict_method: predict_proba`, \"other\":`args: # Optional. Arguments to be passed to the model initialisation. file_path: url: {{ model_file_url }} # refers to models.tsv md5: {{ model_file_md5 }} my_param: 42`, } var models_tsv = {\"keras\": `model\\tmodel_arch_url\\tmodel_arch_md5\\tmodel_weights_url\\tmodel_weights_md5 my_model_1\\thttps://zenodo.org/path/to/my/architecture/file1\\t1234567890abc\\thttps://zenodo.org/path/to/my/model/weights1.h5\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/architecture/file2\\t1234567890abc\\thttps://zenodo.org/path/to/my/model/weights2.h5\\t1234567890abc`, \"tensorflow\": `model\\tmodel_checkpoint_url\\tmodel_checkpoint_md5 my_model_1\\thttps://zenodo.org/path/to/my/model1.tf\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model2.tf\\t1234567890abc`, \"pytorch\": `model\\tmodel_weights_url\\tmodel_weights_md5 my_model_1\\thttps://zenodo.org/path/to/my/model/weights1.pth\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model/weights2.pth\\t1234567890abc`, \"scikitlearn\": `model\\tmodel_pkl_url\\tmodel_pkl_md5 my_model_1\\thttps://zenodo.org/path/to/my/model1.pkl\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model2.pkl\\t1234567890abc`, \"other\":`model\\tmodel_file_url\\tmodel_file_md5 my_model_1\\thttps://zenodo.org/path/to/my/model1.pkl\\t1234567890abc my_model_2\\thttps://zenodo.org/path/to/my/model2.pkl\\t1234567890abc`, }; var model_yaml_dl_entry ={ \"dna\":` defined_as: kipoiseq.dataloaders.SeqIntervalDl default_args: # Optional arguments to the SeqIntervalDl dataloader # See also https://kipoi.org/kipoiseq/dataloaders/#seqintervaldl auto_resize_len: 100 # Automatically resize sequence intervals alphabet_axis: 1 dummy_axis: 2 # Add a dummy axis. Omit in order not to create dummy_axis. alphabet: \"ACGT\" # Order of letters in 1-hot encoding ignore_targets: False # if True, dont return any target variables`, \"dnaAdditional\":`. #Refer to dataloader.yaml in the same folder as this file.`, \"splicing\":` defined_as: kipoiseq.dataloaders.MMSpliceDl default_args: # Optional arguments to the MMSpliceDl dataloader intron5prime_len: 100 # 5' intronic sequence length to take. intron3prime_len: 100 # 3' intronic sequence length to take.` }; var model_yaml = `defined_as: {{ model_class }} {{ model_args }} default_dataloader: {{ model_yaml_dl_entry }} info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting X cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Dataset Y. held-out chromosomes chr8, chr9 and chr22. license: MIT # Software License - if not set defaults to MIT # You can also specify the license in the LICENSE file dependencies: conda: # install via conda - python - h5py - pip pip: # install via pip - keras&gt;=2.0.4 - tensorflow&gt;=1.0 schema: # Model schema. The schema defintion is essential for kipoi plug-ins to work. inputs: # input = single numpy array shape: (100,4) # array shape of a single sample (omitting the batch dimension) doc: input feature description # inputs: # input = dictionary of fields # seq: # shape: (100,4) # doc: input feature description # other_track: # shape: (50,) # doc: input feature description targets: shape: (3,) doc: model prediction description `; var model_py = `from kipoi.model import BaseModel class MyModel(BaseModel): # Implement your Kipoi model def __init__(self, file_path, my_param): ... self.model = load_model_parameters(file_path) # Execute model prediction for input data def predict_on_batch(self, x): # The bare minimum that has to be defined return self.model.predict(x)`; var dataloader_yaml = `defined_as: dataloader.MyDataset # MyDataset impolemented in dataloader.py args: # MyDataset.__init__ argument description features_file: doc: intervals_file: bed3 file containing intervals # Test file URL's example: url: https://raw.githubusercontent.com/../intervals_51bp.tsv md5: a76e47b3df87fd514860cf27fdc10eb4 targets_file: doc: Reference genome FASTA file path. example: url: https://raw.githubusercontent.com/../hg38_chr22_32000000_32300000.fa md5: 01320157a250a3d2eea63e89ecf79eba ignore_targets: doc: if True, don't return any target variables optional: True # if not present, the \"targets\" will not be present info: authors: - name: Your Name github: your_github_account email: your_email@host.org doc: Data-loader returning one-hot encoded sequences given genome intervals dependencies: conda: - python - bioconda::pybedtools - bioconda::pysam - bioconda::pyfaidx - numpy - pandas - pip pip: - kipoiseq output_schema: # Define the dataloader output schema according to the returned values inputs: seq: shape: (100, 4) doc: One-hot encoded DNA sequence other_track: shape: (50,) doc: dummy track targets: shape: (None,) doc: (optional) values following the bed-entry metadata: # additional information about the samples ranges: type: GenomicRanges doc: Ranges describing inputs.seq`; var dataloader_py = `import numpy as np from kipoi.data import Dataset from kipoi.metadata import GenomicRanges from kipoiseq.dataloaders.sequence import BedDataset from kipoiseq.extractors import FastaStringExtractor from kipoiseq.transforms import OneHot class MyDataset(Dataset): \"\"\"Example re-implementation of kipoiseq.dataloaders.SeqIntervalDl Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file, ignore_targets=True): self.bt = BedDataset(intervals_file, bed_columns=3, ignore_targets=ignore_targets) self.fasta_file = fasta_file self.fasta_extractor = None self.transform = OneHot() # one-hot encode DNA sequence def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaStringExtractor(self.fasta_file) # get the intervals interval, targets = self.bed[idx] # resize to 100bp interval = resize_interval(interval, 100, anchor='center') # extract the sequence seq = self.fasta_extractors.extract(interval) # one-hot encode the sequence seq_onehot = self.transform(seq) return { \"inputs\": { \"seq\": seq_onehot, \"other_track\": np.ones((50, )) }, \"targets\": targets, # (optional field) \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } }`; which input data your model requires: Select... DNA sequence (one-hot encoded or string) DNA sequence with additional tracks DNA sequence splicing model Other model input in which framework your model is implemented: Select... Keras TensorFlow PyTorch Sci-Kit learn other whether you want to contribute a: Select... single model set of highly similar models (say models for different TFs) set of models that logically belong together, but may not be very similar .cond:{ visibility: hidden; } .hidden:{ visibility: hidden; }","title":"Contributing models - Getting started"},{"location":"contributing/02_Writing_model.yaml/","text":"model.yaml The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct definitions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future. To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example model.yaml : defined_as: kipoi.model.KerasModel args: # arguments of `kipoi.model.KerasModel` arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc default_dataloader: . # path to the dataloader directory. Or to the dataloader class, e.g.: `kipoiseq.dataloaders.SeqIntervalDl info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python - h5py - pip # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" The model.yaml file has the following mandatory fields: defined_as The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the respective class name in Kipoi has to be used. Therefore defined_as can be one of the followinf values: kipoi.model.KerasModel , kipoi.model.PyTorchModel , kipoi.model.SklearnModel , and kipoi.model.TensorFlowModel . If you wrote your own Kipoi model class, you called it MyModel , and you defined it in the file my_model.py , then the type field would be: my_model.MyModel . The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks. In the model.yaml file the definition of a Keras model would like this: defined_as: kipoi.model.KerasModel args Model arguments define where the files are files and functions are located to instantiate the model. Most entries of args will contain links to zenodo or figshare downloads. The correct definition of args depends on the model defined_as that was selected: kipoi.model.KerasModel models For Keras models the following args are available: weights : URL and md5 of the hdf5 weights or the hdf5 Keras model. arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : URL and md5 of python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', 'cntk') image_dim_ordering : 'tf' or 'th' : Whether the model was trained with using 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. The Keras framework offers different ways to store model architecture and weights: Architecture and weights can be stored separately: defined_as: kipoi.model.KerasModel args: arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc The architecture can be stored together with the weights: defined_as: kipoi.model.KerasModel args: weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that is uploaded with the model architecture and weights. This file defines a dictionary containing custom Keras components called OBJECTS . These objects will be added to custom_objects when loading the model with keras.models.load_model . Example of a custom_keras_objects.py : from concise.layers import SplineT OBJECTS = {\"SplineT\": SplineT} Example of the corresponding model.yaml entry: defined_as: kipoi.model.KerasModel args: ... custom_objects: custom_keras_objects.py Here all the objects present in custom_keras_objects.py will be made available to Keras when loading the model. kipoi.model.PyTorchModel models Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following args : weights , module_class , module_kwargs , module_obj . PyTorch models require python code in which the model is defined. The code that defines the model should not attempt load the weights, as this is done inside the PyTorchModel class in Kipoi using the model.load_state_dict(torch.load(weights)) command. For example the pytorch model definition could be in a file my_pytorch_model.py : from torch import nn class DummyModel(nn.Module): def __init__(self, x, y, z): super(DummyModel, self).__init__() # Some code here def forward(self, x): # some code here return x Assuming that the my_pytorch_model.py file lies in the same folder as the model.yaml , the default way for loading this model in Kiopi is then as follows: defined_as: kipoi.model.PyTorchModel args: module_class: my_pytorch_model.DummyModel module_kwargs: x: 1 y: 2 z: 3 weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If the module class does not have any arguments then module_kwargs can be omitted. If you use Sequential models ( torch.nn.Sequential ) or you generate a module instance in your my_sequential.py file, then you can use the module_obj in model.yaml to load that module: defined_as: kipoi.model.PyTorchModel args: module_obj: my_sequential.sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc where my_sequential.py for example contains: import torch sequential_model = torch.nn.Sequential(...) If you have trouble with the imports or if you would like to import a module from a parent directory you can explicitly specify the python file path: defined_as: kipoi.model.PyTorchModel args: module_file: ./my_sequential.py module_obj: sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If cuda is available on the system then the model will automatically be switched to cuda mode, so the user does not have to take care of that. kipoi.model.SklearnModel models SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: joblib.load(pkl_file) defined_as: kipoi.model.SklearnModel args: pkl_file: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc predict_method: predict_proba # Optional. predict by default. Available: predict, predict_proba, predict_log_proba kipoi.model.TensorFlowModel models Tensorflow models are expected to be stored by calling saver = tf.train.Saver(); saver.save(checkpoint_path) . The input_nodes argument is then a string, list of strings or dictionary of strings that define the input node names. The target_nodes argument is a string, list of strings or dictionary of strings that define the model target node names. defined_as: kipoi.model.TensorFlowModel args: input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: https://zenodo.org/path/to/my/model.tf md5: 1234567890abc If a model requires a constant feed of data which is not provided by the dataloader the const_feed_dict_pkl argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in const_feed_dict_pkl . defined_as: kipoi.model.TensorFlowModel args: ... const_feed_dict_pkl: url: https://zenodo.org/path/to/my/const_feed_dict.pkl md5: 1234567890abc custom models It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined Model class must be a subclass of BaseModel defined in kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! If for example a custom model class definition ( MyModel ) is in a file my_model.py , then the model.yaml will contain: defined_as: my_model.MyModel Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of BaseModel , which in other words means that def predict_on_batch(self, x) has to be implemented. So if batch is for example what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to work. It is likely that MyModel will require additional files to work. The Kipoi way of using such files is by defining Model in the following way: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, external_file): self.data = read_my_file(external_file) #... The file will be downloaded from zenodo or figshare automatically and assigned to the external_file argument if the model.yaml contains: defined_as: my_model.MyModel args: external_file: default: url: https://zenodo.org/path/to/my/data md5: 1234567890abc info The info field of a model.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the model. A short description of what it does and what it is designed for. license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the model and its use cases cite_as : Link to the journal, arXiv, ... trained_on : Description of the training dataset training_procedure : Description of the training procedure A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com - name: Second Author doc: My model description license: GNU tags: - TFBS - tag2 cite_as: http://www.the_journal.com/mypublication trained_on: The XXXX dataset from YYYY training_procedure: 10-fold cross validation default_dataloader The default_dataloader defines the dataloader that should be used for the given model. It can either be defined by a package like kipoiseq or it can be defined by the contributor. Using a pre-defined dataloader If one of the ready-made dataloaders on kipoiseq fits the needs of your model, then please follow the instructions on kipoiseq . The default_dataloader in the model.yaml would then for example be: default_dataloader: defined_as: kipoiseq.dataloaders.SeqIntervalDl default_args: auto_resize_len: 1000 alphabet_axis: 0 dummy_axis: 1 dtype: float32 Using a custom dataloader If you need a specialised dataloader you are encouraged to used as many methods and classes from within kipoiseq as possible as their functionality is tested. See more information on writing a dataloader and its companion yaml. Both of those files should lie in the same folder as the model.yaml . Then the default_dataloader entry in model.yaml is: default_dataloader: . It points to the location of the dataloader.yaml file. If dataloader.yaml lies in different subfolder then default_dataloader: path/to/folder would be used where dataloader.yaml would lie in folder . schema Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are. schema contains two categories inputs and targets which each specify the shapes of the model input and model output. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A dictionary of numpy arrays as inputs or targets: schema: inputs: seq: shape: (1000,4) inp2: shape: (10) inputs The inputs fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input , not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ). targets The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model input name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line). How model types handle schemas The different model types handle those three different encapsulations of numpy arrays differently: KerasModel models Input In case a Keras model is used the batch produced by the dataloader is passed on as it is to the model.predict_on_batch() function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to model.predict_on_batch() . Output The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema. PyTorchModel models Pytorch needs torch.autograd.Variable instances to work. Hence all inputs are automatically converted into Variable objects and results are converted back into numpy arrays transparently. If cuda is available the model will automatically be used in cuda mode and also the input variables will be switched to cuda . Input For prediction the following will happen to the tree different encapsulations of input arrays: A single array: Will be passed directly as the only argument to model call: model(Variable(from_numpy(x))) A list of arrays: The model will be called with the list of converted array as args (e.g.: model(*list_of_variables) ) A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: model(**dict_of_variables) ) Output The model return values will be converted back into encapsulations of numpy arrays, where: a single Variable object will be converted into a numpy arrays lists of Variable objects will be converted into a list of numpy arrays in the same order and SklearnModel models The batch generated by the dataloader will be passed on directly to the SciKit-learn model using model.predict(x) , model.predict_proba(x) or model.predict_log_proba (depending on the predict_method argument). TensorFlowModel models Input The feed_dict for running a tensorflow session is generated by converting the batch samples into the feed_dict using input_nodes defined in the args section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays: If input_nodes is a single string the model will be fed with a dictionary {input_ops: x} If input_nodes is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that. If input_nodes is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that. Output The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the schema field of model.yaml has to match that. dependencies One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. If your model is a python-based model and you have not tested whether your model works in python 2 and python 3, then make sure that you also add the correct python version as a dependency e.g.: python=2.7 . conda Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used. pip Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"model.yaml"},{"location":"contributing/02_Writing_model.yaml/#modelyaml","text":"The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct definitions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future. To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example model.yaml : defined_as: kipoi.model.KerasModel args: # arguments of `kipoi.model.KerasModel` arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc default_dataloader: . # path to the dataloader directory. Or to the dataloader class, e.g.: `kipoiseq.dataloaders.SeqIntervalDl info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python - h5py - pip # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" The model.yaml file has the following mandatory fields:","title":"model.yaml"},{"location":"contributing/02_Writing_model.yaml/#defined_as","text":"The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the respective class name in Kipoi has to be used. Therefore defined_as can be one of the followinf values: kipoi.model.KerasModel , kipoi.model.PyTorchModel , kipoi.model.SklearnModel , and kipoi.model.TensorFlowModel . If you wrote your own Kipoi model class, you called it MyModel , and you defined it in the file my_model.py , then the type field would be: my_model.MyModel . The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks. In the model.yaml file the definition of a Keras model would like this: defined_as: kipoi.model.KerasModel","title":"defined_as"},{"location":"contributing/02_Writing_model.yaml/#args","text":"Model arguments define where the files are files and functions are located to instantiate the model. Most entries of args will contain links to zenodo or figshare downloads. The correct definition of args depends on the model defined_as that was selected:","title":"args"},{"location":"contributing/02_Writing_model.yaml/#kipoimodelkerasmodel-models","text":"For Keras models the following args are available: weights : URL and md5 of the hdf5 weights or the hdf5 Keras model. arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : URL and md5 of python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', 'cntk') image_dim_ordering : 'tf' or 'th' : Whether the model was trained with using 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. The Keras framework offers different ways to store model architecture and weights: Architecture and weights can be stored separately: defined_as: kipoi.model.KerasModel args: arch: url: https://zenodo.org/path/to/my/architecture/file md5: 1234567890abc weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc The architecture can be stored together with the weights: defined_as: kipoi.model.KerasModel args: weights: url: https://zenodo.org/path/to/my/model/weights.h5 md5: 1234567890abc In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that is uploaded with the model architecture and weights. This file defines a dictionary containing custom Keras components called OBJECTS . These objects will be added to custom_objects when loading the model with keras.models.load_model . Example of a custom_keras_objects.py : from concise.layers import SplineT OBJECTS = {\"SplineT\": SplineT} Example of the corresponding model.yaml entry: defined_as: kipoi.model.KerasModel args: ... custom_objects: custom_keras_objects.py Here all the objects present in custom_keras_objects.py will be made available to Keras when loading the model.","title":"kipoi.model.KerasModel models"},{"location":"contributing/02_Writing_model.yaml/#kipoimodelpytorchmodel-models","text":"Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following args : weights , module_class , module_kwargs , module_obj . PyTorch models require python code in which the model is defined. The code that defines the model should not attempt load the weights, as this is done inside the PyTorchModel class in Kipoi using the model.load_state_dict(torch.load(weights)) command. For example the pytorch model definition could be in a file my_pytorch_model.py : from torch import nn class DummyModel(nn.Module): def __init__(self, x, y, z): super(DummyModel, self).__init__() # Some code here def forward(self, x): # some code here return x Assuming that the my_pytorch_model.py file lies in the same folder as the model.yaml , the default way for loading this model in Kiopi is then as follows: defined_as: kipoi.model.PyTorchModel args: module_class: my_pytorch_model.DummyModel module_kwargs: x: 1 y: 2 z: 3 weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If the module class does not have any arguments then module_kwargs can be omitted. If you use Sequential models ( torch.nn.Sequential ) or you generate a module instance in your my_sequential.py file, then you can use the module_obj in model.yaml to load that module: defined_as: kipoi.model.PyTorchModel args: module_obj: my_sequential.sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc where my_sequential.py for example contains: import torch sequential_model = torch.nn.Sequential(...) If you have trouble with the imports or if you would like to import a module from a parent directory you can explicitly specify the python file path: defined_as: kipoi.model.PyTorchModel args: module_file: ./my_sequential.py module_obj: sequential_model weights: url: https://zenodo.org/path/to/my/model/weights.pth md5: 1234567890abc If cuda is available on the system then the model will automatically be switched to cuda mode, so the user does not have to take care of that.","title":"kipoi.model.PyTorchModel models"},{"location":"contributing/02_Writing_model.yaml/#kipoimodelsklearnmodel-models","text":"SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: joblib.load(pkl_file) defined_as: kipoi.model.SklearnModel args: pkl_file: url: https://zenodo.org/path/to/my/model.pkl md5: 1234567890abc predict_method: predict_proba # Optional. predict by default. Available: predict, predict_proba, predict_log_proba","title":"kipoi.model.SklearnModel models"},{"location":"contributing/02_Writing_model.yaml/#kipoimodeltensorflowmodel-models","text":"Tensorflow models are expected to be stored by calling saver = tf.train.Saver(); saver.save(checkpoint_path) . The input_nodes argument is then a string, list of strings or dictionary of strings that define the input node names. The target_nodes argument is a string, list of strings or dictionary of strings that define the model target node names. defined_as: kipoi.model.TensorFlowModel args: input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: url: https://zenodo.org/path/to/my/model.tf md5: 1234567890abc If a model requires a constant feed of data which is not provided by the dataloader the const_feed_dict_pkl argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in const_feed_dict_pkl . defined_as: kipoi.model.TensorFlowModel args: ... const_feed_dict_pkl: url: https://zenodo.org/path/to/my/const_feed_dict.pkl md5: 1234567890abc","title":"kipoi.model.TensorFlowModel models"},{"location":"contributing/02_Writing_model.yaml/#custom-models","text":"It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined Model class must be a subclass of BaseModel defined in kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! If for example a custom model class definition ( MyModel ) is in a file my_model.py , then the model.yaml will contain: defined_as: my_model.MyModel Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of BaseModel , which in other words means that def predict_on_batch(self, x) has to be implemented. So if batch is for example what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to work. It is likely that MyModel will require additional files to work. The Kipoi way of using such files is by defining Model in the following way: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, external_file): self.data = read_my_file(external_file) #... The file will be downloaded from zenodo or figshare automatically and assigned to the external_file argument if the model.yaml contains: defined_as: my_model.MyModel args: external_file: default: url: https://zenodo.org/path/to/my/data md5: 1234567890abc","title":"custom models"},{"location":"contributing/02_Writing_model.yaml/#info","text":"The info field of a model.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the model. A short description of what it does and what it is designed for. license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the model and its use cases cite_as : Link to the journal, arXiv, ... trained_on : Description of the training dataset training_procedure : Description of the training procedure A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com - name: Second Author doc: My model description license: GNU tags: - TFBS - tag2 cite_as: http://www.the_journal.com/mypublication trained_on: The XXXX dataset from YYYY training_procedure: 10-fold cross validation","title":"info"},{"location":"contributing/02_Writing_model.yaml/#default_dataloader","text":"The default_dataloader defines the dataloader that should be used for the given model. It can either be defined by a package like kipoiseq or it can be defined by the contributor.","title":"default_dataloader"},{"location":"contributing/02_Writing_model.yaml/#using-a-pre-defined-dataloader","text":"If one of the ready-made dataloaders on kipoiseq fits the needs of your model, then please follow the instructions on kipoiseq . The default_dataloader in the model.yaml would then for example be: default_dataloader: defined_as: kipoiseq.dataloaders.SeqIntervalDl default_args: auto_resize_len: 1000 alphabet_axis: 0 dummy_axis: 1 dtype: float32","title":"Using a pre-defined dataloader"},{"location":"contributing/02_Writing_model.yaml/#using-a-custom-dataloader","text":"If you need a specialised dataloader you are encouraged to used as many methods and classes from within kipoiseq as possible as their functionality is tested. See more information on writing a dataloader and its companion yaml. Both of those files should lie in the same folder as the model.yaml . Then the default_dataloader entry in model.yaml is: default_dataloader: . It points to the location of the dataloader.yaml file. If dataloader.yaml lies in different subfolder then default_dataloader: path/to/folder would be used where dataloader.yaml would lie in folder .","title":"Using a custom dataloader"},{"location":"contributing/02_Writing_model.yaml/#schema","text":"Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are. schema contains two categories inputs and targets which each specify the shapes of the model input and model output. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A dictionary of numpy arrays as inputs or targets: schema: inputs: seq: shape: (1000,4) inp2: shape: (10)","title":"schema"},{"location":"contributing/02_Writing_model.yaml/#inputs","text":"The inputs fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input , not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ).","title":"inputs"},{"location":"contributing/02_Writing_model.yaml/#targets","text":"The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model input name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).","title":"targets"},{"location":"contributing/02_Writing_model.yaml/#how-model-types-handle-schemas","text":"The different model types handle those three different encapsulations of numpy arrays differently:","title":"How model types handle schemas"},{"location":"contributing/02_Writing_model.yaml/#kerasmodel-models","text":"","title":"KerasModel models"},{"location":"contributing/02_Writing_model.yaml/#input","text":"In case a Keras model is used the batch produced by the dataloader is passed on as it is to the model.predict_on_batch() function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to model.predict_on_batch() .","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output","text":"The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema.","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#pytorchmodel-models","text":"Pytorch needs torch.autograd.Variable instances to work. Hence all inputs are automatically converted into Variable objects and results are converted back into numpy arrays transparently. If cuda is available the model will automatically be used in cuda mode and also the input variables will be switched to cuda .","title":"PyTorchModel models"},{"location":"contributing/02_Writing_model.yaml/#input_1","text":"For prediction the following will happen to the tree different encapsulations of input arrays: A single array: Will be passed directly as the only argument to model call: model(Variable(from_numpy(x))) A list of arrays: The model will be called with the list of converted array as args (e.g.: model(*list_of_variables) ) A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: model(**dict_of_variables) )","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output_1","text":"The model return values will be converted back into encapsulations of numpy arrays, where: a single Variable object will be converted into a numpy arrays lists of Variable objects will be converted into a list of numpy arrays in the same order and","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#sklearnmodel-models","text":"The batch generated by the dataloader will be passed on directly to the SciKit-learn model using model.predict(x) , model.predict_proba(x) or model.predict_log_proba (depending on the predict_method argument).","title":"SklearnModel models"},{"location":"contributing/02_Writing_model.yaml/#tensorflowmodel-models","text":"","title":"TensorFlowModel models"},{"location":"contributing/02_Writing_model.yaml/#input_2","text":"The feed_dict for running a tensorflow session is generated by converting the batch samples into the feed_dict using input_nodes defined in the args section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays: If input_nodes is a single string the model will be fed with a dictionary {input_ops: x} If input_nodes is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that. If input_nodes is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that.","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output_2","text":"The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the schema field of model.yaml has to match that.","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#dependencies","text":"One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. If your model is a python-based model and you have not tested whether your model works in python 2 and python 3, then make sure that you also add the correct python version as a dependency e.g.: python=2.7 .","title":"dependencies"},{"location":"contributing/02_Writing_model.yaml/#conda","text":"Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used.","title":"conda"},{"location":"contributing/02_Writing_model.yaml/#pip","text":"Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"pip"},{"location":"contributing/03_Writing_dataloader.yaml/","text":"dataloader.yaml Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read Writing dataloader.py . To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example dataloader.yaml : defined_as: dataloader.MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: url: https://zenodo.org/path/to/example_files/features.csv # example file md5: 7a6s5d76as5d76a5sd7 targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: url: https://zenodo.org/path/to/example_files/targets.csv # example file md5: 76sd8f7687sd6fs68a67 optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org doc: Model predicting the Iris species dependencies: conda: - python - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: type: int doc: Just an example metadata column type The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values: PreloadedDataset Dataset BatchDataset SampleIterator SampleGenerator BatchIterator BatchGenerator defined_as defined_as indicates where the dataloader class can be found. It is a string value of file.ClassName where file refers to file file.py in the same directory as dataloader.yaml which contains the data-loader class ClassName . E.g.: dataloader.MyDataLoader . This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitly in args (see below). args A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.: args: reference_fasta: example: url: https://zenodo.org/path/to/example_files/chr22.fa md5: 765sadf876a argument_2: example: url: https://zenodo.org/path/to/example_files/example_input.txt md5: 786as8d7aasd An argument has the following fields: doc : A free text field describing the argument example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call kipoi test uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit the URLs of all necessary example files with the model. type : Optional: datatype of the argument ( str , bool , int , float ) default : This field is used to define external zenodo or figshare links that are automatically downloaded and assigned. See example below. optional : Optional: Boolean flag ( true / false ) for an argument if it is optional. If your dataloader requires an external data file at runtime which are not example/test files, you can specify these using the default attribute. default will override the default arguments of the dataloader init method (e.g. dataloader.MyDataloader.__init__ ). Example: defined_as: dataloader.MyDataset args: ... override_me: default: 10 essential_other_file: default: # download and replace with the path on the local filesystem url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a ... info The info field of a dataloader.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the dataloader. A short description of what it does. version : Version of the dataloader license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the dataloader and its use cases A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com doc: Datalaoder for my fancy model description version: 1.0 license: GNU tags: - TFBS - tag2 output_schema output_schema defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data. output_schema contains three categories inputs , targets and metadata . inputs and targets each specify the shapes of data generated for the model input and model. Offering the targets option enables the opportunity to possibly train models with the same dataloader. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: output_schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: output_schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A list of numpy arrays as inputs or targets: output_schema: inputs: seq: shape: (1000,4) inp2: shape: (10) inputs The inputs fields of output_schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input, not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ). associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated metadata field is called ranges then associated_metadata: ranges has to be set. targets The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model target name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line). metadata Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its associated_metadata flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are: doc : A free text description of the metadata element type : The datatype of the metadata field: str , int , float , array , GenomicRanges . Where the convenience class GenomicRanges is defined in kipoi.metadata , which is essentially an in-memory representation of a bed file. Definition of metadata is essential for postprocessing algorihms such as variant effect prediction. Please refer to their detailed description for their requirements. An example of the defintion of dataloader.yaml with metadata can be seen here: output_schema: inputs: - name: seq shape: (1000,4) associated_metadata: my_ranges - name: inp2 shape: (10) ... metadata: my_ranges: type: GenomicRanges doc: Region from where inputs.seq was extracted dependencies One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. conda Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used. pip Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"dataloader.yaml"},{"location":"contributing/03_Writing_dataloader.yaml/#dataloaderyaml","text":"Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read Writing dataloader.py . To help understand the syntax of YAML please take a look at: YAML Syntax Basics Here is an example dataloader.yaml : defined_as: dataloader.MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: url: https://zenodo.org/path/to/example_files/features.csv # example file md5: 7a6s5d76as5d76a5sd7 targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: url: https://zenodo.org/path/to/example_files/targets.csv # example file md5: 76sd8f7687sd6fs68a67 optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org doc: Model predicting the Iris species dependencies: conda: - python - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: type: int doc: Just an example metadata column","title":"dataloader.yaml"},{"location":"contributing/03_Writing_dataloader.yaml/#type","text":"The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values: PreloadedDataset Dataset BatchDataset SampleIterator SampleGenerator BatchIterator BatchGenerator","title":"type"},{"location":"contributing/03_Writing_dataloader.yaml/#defined_as","text":"defined_as indicates where the dataloader class can be found. It is a string value of file.ClassName where file refers to file file.py in the same directory as dataloader.yaml which contains the data-loader class ClassName . E.g.: dataloader.MyDataLoader . This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitly in args (see below).","title":"defined_as"},{"location":"contributing/03_Writing_dataloader.yaml/#args","text":"A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.: args: reference_fasta: example: url: https://zenodo.org/path/to/example_files/chr22.fa md5: 765sadf876a argument_2: example: url: https://zenodo.org/path/to/example_files/example_input.txt md5: 786as8d7aasd An argument has the following fields: doc : A free text field describing the argument example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call kipoi test uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit the URLs of all necessary example files with the model. type : Optional: datatype of the argument ( str , bool , int , float ) default : This field is used to define external zenodo or figshare links that are automatically downloaded and assigned. See example below. optional : Optional: Boolean flag ( true / false ) for an argument if it is optional. If your dataloader requires an external data file at runtime which are not example/test files, you can specify these using the default attribute. default will override the default arguments of the dataloader init method (e.g. dataloader.MyDataloader.__init__ ). Example: defined_as: dataloader.MyDataset args: ... override_me: default: 10 essential_other_file: default: # download and replace with the path on the local filesystem url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a ...","title":"args"},{"location":"contributing/03_Writing_dataloader.yaml/#info","text":"The info field of a dataloader.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the dataloader. A short description of what it does. version : Version of the dataloader license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the dataloader and its use cases A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com doc: Datalaoder for my fancy model description version: 1.0 license: GNU tags: - TFBS - tag2","title":"info"},{"location":"contributing/03_Writing_dataloader.yaml/#output_schema","text":"output_schema defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data. output_schema contains three categories inputs , targets and metadata . inputs and targets each specify the shapes of data generated for the model input and model. Offering the targets option enables the opportunity to possibly train models with the same dataloader. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: output_schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: output_schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A list of numpy arrays as inputs or targets: output_schema: inputs: seq: shape: (1000,4) inp2: shape: (10)","title":"output_schema"},{"location":"contributing/03_Writing_dataloader.yaml/#inputs","text":"The inputs fields of output_schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input, not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ). associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated metadata field is called ranges then associated_metadata: ranges has to be set.","title":"inputs"},{"location":"contributing/03_Writing_dataloader.yaml/#targets","text":"The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model target name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).","title":"targets"},{"location":"contributing/03_Writing_dataloader.yaml/#metadata","text":"Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its associated_metadata flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are: doc : A free text description of the metadata element type : The datatype of the metadata field: str , int , float , array , GenomicRanges . Where the convenience class GenomicRanges is defined in kipoi.metadata , which is essentially an in-memory representation of a bed file. Definition of metadata is essential for postprocessing algorihms such as variant effect prediction. Please refer to their detailed description for their requirements. An example of the defintion of dataloader.yaml with metadata can be seen here: output_schema: inputs: - name: seq shape: (1000,4) associated_metadata: my_ranges - name: inp2 shape: (10) ... metadata: my_ranges: type: GenomicRanges doc: Region from where inputs.seq was extracted","title":"metadata"},{"location":"contributing/03_Writing_dataloader.yaml/#dependencies","text":"One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future.","title":"dependencies"},{"location":"contributing/03_Writing_dataloader.yaml/#conda","text":"Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used.","title":"conda"},{"location":"contributing/03_Writing_dataloader.yaml/#pip","text":"Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"pip"},{"location":"contributing/04_Writing_dataloader.py/","text":"Dataloader Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys: inputs targets (optional) metadata (optional). As the names suggest, the inputs will get feeded to the model to make the predictions and targets could be used to train the model. The metadata field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models). In a batch of data returned by the dataloader, all three fields can be further nested - i.e. inputs can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays. Note that the inputs and targets have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the name fields in the keras.layers.Input layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array. Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield: individual samples batches of data whole dataset Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like bool , float , int , str . These will later get stacked into a one-dimensional numpy array. Dataloader types Specifically, a dataloader has to inherit from one of the following classes defined in kipoi.data : PreloadedDataset Function that returns the whole dataset as a nested dictionary/list of numpy arrays useful when: the dataset is expected to load quickly and fit into the memory Dataset Class that inherits from kipoi.data.Dataset and implements __len__ and __getitem__ methods. __getitem__ returns a single sample from the dataset. useful when: dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches BatchDataset Class that inherits from kipoi.data.BatchDataset and implements __len__ and __getitem__ methods. __getitem__ returns a single batch of samples from the dataset. useful when: dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches SampleIterator Class that inherits from kipoi.data.SampleIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches BatchIterator Class that inherits from kipoi.data.BatchIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single batch of samples sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches SampleGenerator A generator function that yields a single sample from the dataset and returns when all the samples were yielded. useful when: same as for SampleIterator , but can be typically implemented in fewer lines of code BatchGenerator A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded. useful when: same as for BatchIterator , but can be typically implemented in fewer lines of code Here is a table showing the (recommended) requirements for each dataloader type: Dataloader type Length known? Significant benefit from loading data in batches? Fits into memory and loads quickly? PreloadedDataset yes yes yes Dataset yes no no BatchDataset yes yes no SampleIterator no no no BatchIterator no yes no SampleGenerator no no no BatchGenerator no yes no Dataset example Here is an example dataloader that gets as input a fasta file and a bed file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges'). import numpy as np from pybedtools import BedTool from kipoi.data import Dataset from kipoi.metadata import GenomicRanges class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file): self.bt = BedTool(intervals_file) self.fasta_file = fasta_file self.fasta_extractor = None def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaExtractor(self.fasta_file) interval = self.bt[idx] seq = np.squeeze(self.fasta_extractor([interval]), axis=0) return { \"inputs\": seq, # lacks targets \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } } Since FastaExtractor is not multi-processing safe, we have initialized it on the first call of __getitem__ instead of __init__ . The reason for this is that when we use parallel dataloading, each process will get a copy of the SeqDataset(...) object. Upon the first call of __getitem__ the fasta_extractor and hence the underlying file-handle will be setup for each worker independently. Required static files If your dataloader requires an external data file as for example in tutorials/contributing_models , then the Kipoi way of automatically downloading and using that file is by adding an argument to the dataloader implementation: from kipoi.data import Dataset class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file, essential_other_file): fh = open(essential_other_file, \"r\") ... Kipoi can automaticall download the required file from a zenodo or figshare url as if the url was defined as a default in the dataloader.yaml as follows: args: ... essential_other_file: default: url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a Further examples To see examples of other dataloaders, run kipoi init from the command-line and choose each time a different dataloader_type. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model ... Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: The generated model directory will contain a working implementation of a dataloader.","title":"dataloader.py"},{"location":"contributing/04_Writing_dataloader.py/#dataloader","text":"Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in [kipoiseq](https://github.com/kipoi/kipoiseq). The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys: inputs targets (optional) metadata (optional). As the names suggest, the inputs will get feeded to the model to make the predictions and targets could be used to train the model. The metadata field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models). In a batch of data returned by the dataloader, all three fields can be further nested - i.e. inputs can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays. Note that the inputs and targets have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the name fields in the keras.layers.Input layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array. Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield: individual samples batches of data whole dataset Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like bool , float , int , str . These will later get stacked into a one-dimensional numpy array.","title":"Dataloader"},{"location":"contributing/04_Writing_dataloader.py/#dataloader-types","text":"Specifically, a dataloader has to inherit from one of the following classes defined in kipoi.data : PreloadedDataset Function that returns the whole dataset as a nested dictionary/list of numpy arrays useful when: the dataset is expected to load quickly and fit into the memory Dataset Class that inherits from kipoi.data.Dataset and implements __len__ and __getitem__ methods. __getitem__ returns a single sample from the dataset. useful when: dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches BatchDataset Class that inherits from kipoi.data.BatchDataset and implements __len__ and __getitem__ methods. __getitem__ returns a single batch of samples from the dataset. useful when: dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches SampleIterator Class that inherits from kipoi.data.SampleIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches BatchIterator Class that inherits from kipoi.data.BatchIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single batch of samples sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches SampleGenerator A generator function that yields a single sample from the dataset and returns when all the samples were yielded. useful when: same as for SampleIterator , but can be typically implemented in fewer lines of code BatchGenerator A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded. useful when: same as for BatchIterator , but can be typically implemented in fewer lines of code Here is a table showing the (recommended) requirements for each dataloader type: Dataloader type Length known? Significant benefit from loading data in batches? Fits into memory and loads quickly? PreloadedDataset yes yes yes Dataset yes no no BatchDataset yes yes no SampleIterator no no no BatchIterator no yes no SampleGenerator no no no BatchGenerator no yes no","title":"Dataloader types"},{"location":"contributing/04_Writing_dataloader.py/#dataset-example","text":"Here is an example dataloader that gets as input a fasta file and a bed file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges'). import numpy as np from pybedtools import BedTool from kipoi.data import Dataset from kipoi.metadata import GenomicRanges class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file): self.bt = BedTool(intervals_file) self.fasta_file = fasta_file self.fasta_extractor = None def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaExtractor(self.fasta_file) interval = self.bt[idx] seq = np.squeeze(self.fasta_extractor([interval]), axis=0) return { \"inputs\": seq, # lacks targets \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } } Since FastaExtractor is not multi-processing safe, we have initialized it on the first call of __getitem__ instead of __init__ . The reason for this is that when we use parallel dataloading, each process will get a copy of the SeqDataset(...) object. Upon the first call of __getitem__ the fasta_extractor and hence the underlying file-handle will be setup for each worker independently.","title":"Dataset example"},{"location":"contributing/04_Writing_dataloader.py/#required-static-files","text":"If your dataloader requires an external data file as for example in tutorials/contributing_models , then the Kipoi way of automatically downloading and using that file is by adding an argument to the dataloader implementation: from kipoi.data import Dataset class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file, essential_other_file): fh = open(essential_other_file, \"r\") ... Kipoi can automaticall download the required file from a zenodo or figshare url as if the url was defined as a default in the dataloader.yaml as follows: args: ... essential_other_file: default: url: https://zenodo.org/path/to/my/essential/other/file.xyz md5: 765sadf876a","title":"Required static files"},{"location":"contributing/04_Writing_dataloader.py/#further-examples","text":"To see examples of other dataloaders, run kipoi init from the command-line and choose each time a different dataloader_type. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model ... Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: The generated model directory will contain a working implementation of a dataloader.","title":"Further examples"},{"location":"contributing/05_Writing_model.py/","text":"model.py Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type. Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of BaseModel defined in kipoi.model , which in other words means that def predict_on_batch(self, x) has to be implemented. So for example if batch is what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to run the model prediction on the given input. A very simple version of such a model definition that can be stored in for example model.py may be: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, file_path): ... self.model = load_model_parameters(file_path) # Execute model prediction for input data def predict_on_batch(self, x): return self.model.predict(x) This can then be integrated in the model.yaml in the following way: defined_as: model.MyModel args: file_path: # get model parameters from an url url: https://zenodo.org/path/to/my/architecture/file md5: .... ...","title":"model.py"},{"location":"contributing/05_Writing_model.py/#modelpy","text":"Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type. Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of BaseModel defined in kipoi.model , which in other words means that def predict_on_batch(self, x) has to be implemented. So for example if batch is what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to run the model prediction on the given input. A very simple version of such a model definition that can be stored in for example model.py may be: from kipoi.model import BaseModel class MyModel(BaseModel): def __init__(self, file_path): ... self.model = load_model_parameters(file_path) # Execute model prediction for input data def predict_on_batch(self, x): return self.model.predict(x) This can then be integrated in the model.yaml in the following way: defined_as: model.MyModel args: file_path: # get model parameters from an url url: https://zenodo.org/path/to/my/architecture/file md5: .... ...","title":"model.py"},{"location":"contributing/06_dumping_models_programatically/","text":"Contributing multiple very similar models To easily contribute model groups with multiple models of the same kind, you can specify two files describing all the models: model-template.yaml - template for model.yaml models.tsv - tab-separated files holding custom model variables First few lines of model-template.yaml : type: keras args: arch: url: {{ args_arch_url }} md5: {{ args_arch_md5 }} weights: url: {{ args_weights_url }} md5: {{ args_weights_md5 }} First few lines of models.tsv : model args_arch args_weights args_arch_md5 args_weights_md5 args_arch_url args_weights_url A549_ENCSR000DDI model_files/A549_ENCSR000DDI.json model_files/A549_ENCSR000DDI.h5 6d3a971ce766128ca444dd70ef76df70 f23198b146ad8e4d6755cb215fe75e0f https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI.h5?download=1 BE2C_ENCSR000DEB model_files/BE2C_ENCSR000DEB.json model_files/BE2C_ENCSR000DEB.h5 919b2f7f675bebb9217d95021d92af74 159ea3cb7985c08eab8f64151eb1799e https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB?download=1 https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB.h5?download=1 BJ_ENCSR000DEA model_files/BJ_ENCSR000DEA.json model_files/BJ_ENCSR000DEA.h5 6d3a971ce766128ca444dd70ef76df70 9ad8797caff0dd0e8274de6befded4e7 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/BJ_ENCSR000DEA.h5?download=1 CMK_ENCSR000DGJ model_files/CMK_ENCSR000DGJ.json model_files/CMK_ENCSR000DGJ.h5 6d3a971ce766128ca444dd70ef76df70 d5c0c9dd55f1056036cc300ec1f61e1d https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/CMK_ENCSR000DGJ.h5?download=1 One row in models.tsv will represent a single model and will be used to populate model-template.yaml and construct model.yaml using Jinja2 templating language . This allows you to even write if statements in model-template.yaml . See CpGenie model as an example.","title":"Multiple very similar models"},{"location":"contributing/06_dumping_models_programatically/#contributing-multiple-very-similar-models","text":"To easily contribute model groups with multiple models of the same kind, you can specify two files describing all the models: model-template.yaml - template for model.yaml models.tsv - tab-separated files holding custom model variables First few lines of model-template.yaml : type: keras args: arch: url: {{ args_arch_url }} md5: {{ args_arch_md5 }} weights: url: {{ args_weights_url }} md5: {{ args_weights_md5 }} First few lines of models.tsv : model args_arch args_weights args_arch_md5 args_weights_md5 args_arch_url args_weights_url A549_ENCSR000DDI model_files/A549_ENCSR000DDI.json model_files/A549_ENCSR000DDI.h5 6d3a971ce766128ca444dd70ef76df70 f23198b146ad8e4d6755cb215fe75e0f https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI.h5?download=1 BE2C_ENCSR000DEB model_files/BE2C_ENCSR000DEB.json model_files/BE2C_ENCSR000DEB.h5 919b2f7f675bebb9217d95021d92af74 159ea3cb7985c08eab8f64151eb1799e https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB?download=1 https://zenodo.org/record/1466073/files/BE2C_ENCSR000DEB.h5?download=1 BJ_ENCSR000DEA model_files/BJ_ENCSR000DEA.json model_files/BJ_ENCSR000DEA.h5 6d3a971ce766128ca444dd70ef76df70 9ad8797caff0dd0e8274de6befded4e7 https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/BJ_ENCSR000DEA.h5?download=1 CMK_ENCSR000DGJ model_files/CMK_ENCSR000DGJ.json model_files/CMK_ENCSR000DGJ.h5 6d3a971ce766128ca444dd70ef76df70 d5c0c9dd55f1056036cc300ec1f61e1d https://zenodo.org/record/1466073/files/A549_ENCSR000DDI?download=1 https://zenodo.org/record/1466073/files/CMK_ENCSR000DGJ.h5?download=1 One row in models.tsv will represent a single model and will be used to populate model-template.yaml and construct model.yaml using Jinja2 templating language . This allows you to even write if statements in model-template.yaml . See CpGenie model as an example.","title":"Contributing multiple very similar models"},{"location":"tutorials/R-api/","text":"Generated from notebooks/R-api.ipynb Using Kipoi from R Thanks to the reticulate R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that. Make sure you have git-lfs and Kipoi correctly installed: Install git-lfs conda install -c conda-forge git-lfs && git lfs install (alternatively see https://git-lfs.github.com/ ) Install kipoi pip install kipoi Please read docs/using/getting started before going through this notebook. Install and load reticulate Make sure you have the reticulate R package installed # install.packages(\"reticulate\") library(reticulate) Reticulate quick intro In general, using Kipoi from R is almost the same as using it from Python: instead of using object.method() or object.attribute as in python, use $ : object$method() , object$attribute . # short reticulate example os <- import(\"os\") os$chdir(\"/tmp\") os$getcwd() '/tmp' Type mapping R <-> python Reticulate translates objects between R and python in the following way: R Python Examples Single-element vector Scalar 1 , 1L , TRUE , \"foo\" Multi-element vector List c(1.0, 2.0, 3.0) , c(1L, 2L, 3L) List of multiple types Tuple list(1L, TRUE, \"foo\") Named list Dict list(a = 1L, b = 2.0) , dict(x = x_data) Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2) Function Python function function(x) x + 1 NULL, TRUE, FALSE None, True, False NULL , TRUE , FALSE For more info on reticulate, please visit https://github.com/rstudio/reticulate/. Setup the python environment With reticulate::py_config() you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with use_condaenv(...) . This comes handy when using different models depending on different conda environments. reticulate::py_config() python: /home/avsec/bin/anaconda3/bin/python libpython: /home/avsec/bin/anaconda3/lib/libpython3.6m.so pythonhome: /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3 version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) [GCC 7.2.0] numpy: /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy numpy_version: 1.14.0 os: /home/avsec/bin/anaconda3/lib/python3.6/os.py python versions found: /home/avsec/bin/anaconda3/bin/python /usr/bin/python /usr/bin/python3 List all conda environments: reticulate::conda_list() Create a new conda environment for the model: $ kipoi env create HAL Use that environment in R: reticulate::use_condaenv(\"kipoi-HAL') Load kipoi kipoi <- import(\"kipoi\") List models kipoi$list_models()$head() source model version \\ 0 kipoi DeepSEAKeras 0.1 1 kipoi extended_coda 0.1 2 kipoi DeepCpG_DNA/Hou2016_mESC_dna 1.0.4 3 kipoi DeepCpG_DNA/Smallwood2014_2i_dna 1.0.4 4 kipoi DeepCpG_DNA/Hou2016_HepG2_dna 1.0.4 authors \\ 0 [Author(name='Jian Zhou', github=None, email=N... 1 [Author(name='Pang Wei Koh', github='kohpangwe... 2 [Author(name='Christof Angermueller', github='... 3 [Author(name='Christof Angermueller', github='... 4 [Author(name='Christof Angermueller', github='... contributors \\ 0 [Author(name='Lara Urban', github='LaraUrban',... 1 [Author(name='Johnny Israeli', github='jisrael... 2 [Author(name='Roman Kreuzhuber', github='krrom... 3 [Author(name='Roman Kreuzhuber', github='krrom... 4 [Author(name='Roman Kreuzhuber', github='krrom... doc type \\ 0 This CNN is based on the DeepSEA model from Zh... keras 1 Single bp resolution ChIP-seq denoising - http... keras 2 This is the extraction of the DNA-part of the ... keras 3 This is the extraction of the DNA-part of the ... keras 4 This is the extraction of the DNA-part of the ... keras inputs targets \\ 0 seq TFBS_DHS_probs 1 [H3K27AC_subsampled] [H3K27ac] 2 [dna] [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c... 3 [dna] [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ... 4 [dna] [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2... postproc_score_variants license \\ 0 True MIT 1 False MIT 2 True MIT 3 True MIT 4 True MIT cite_as \\ 0 https://doi.org/10.1038/nmeth.3547 1 https://doi.org/10.1093/bioinformatics/btx243 2 https://doi.org/10.1186/s13059-017-1189-z, htt... 3 https://doi.org/10.1186/s13059-017-1189-z, htt... 4 https://doi.org/10.1186/s13059-017-1189-z, htt... trained_on \\ 0 ENCODE and Roadmap Epigenomics chromatin profi... 1 Described in https://academic.oup.com/bioinfor... 2 scBS-seq and scRRBS-seq datasets, https://geno... 3 scBS-seq and scRRBS-seq datasets, https://geno... 4 scBS-seq and scRRBS-seq datasets, https://geno... training_procedure \\ 0 https://www.nature.com/articles/nmeth.3547#met... 1 Described in https://academic.oup.com/bioinfor... 2 Described in https://genomebiology.biomedcentr... 3 Described in https://genomebiology.biomedcentr... 4 Described in https://genomebiology.biomedcentr... tags 0 [Histone modification, DNA binding, DNA access... 1 [Histone modification] 2 [DNA methylation] 3 [DNA methylation] 4 [DNA methylation] reticulate currently doesn't support direct convertion from pandas.DataFrame to R's data.frame . Let's make a convenience function to create an R dataframe via matrix conversion. #' List models as an R data.frame kipoi_list_models <- function() { df_models <- kipoi$list_models() df <- data.frame(df_models$as_matrix()) colnames(df) = df_models$columns$tolist() return(df) } df <- kipoi_list_models() head(df, 2) source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags kipoi DeepSEAKeras 0.1 <environment: 0x556afc757e38> <environment: 0x556afbb0d538> This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras seq TFBS_DHS_probs TRUE MIT https://doi.org/10.1038/nmeth.3547 ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods https://www.nature.com/articles/nmeth.3547#methods <environment: 0x556afcddfd50> kipoi extended_coda 0.1 <environment: 0x556afc764260> <environment: 0x556afbaff708> Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda keras H3K27AC_subsampled H3K27ac FALSE MIT https://doi.org/10.1093/bioinformatics/btx243 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60> Get the kipoi model and make a prediction for the example files To run the following example, make sure you have all the dependencies installed. Run: kipoi$install_model_requirements(\"MaxEntScan/3prime\") from R or kipoi env create MaxEntScan source activate kipoi-MaxEntScan from the command-line. This will install all the required dependencies for both, the model and the dataloader. kipoi$install_model_requirements(\"MaxEntScan/3prime\") model <- kipoi$get_model(\"MaxEntScan/3prime\") predictions <- model$pipeline$predict_example() head(predictions) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 -9.52033554891735 9.54342300799607 Use the model and dataloader independently # Get the dataloader setwd('~/.kipoi/models/MaxEntScan/3prime') dl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) it DataLoaderIter # Retrieve a batch of data batch <- iter_next(it) str(batch) List of 2 $ inputs : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\" $ metadata:List of 5 ..$ geneID : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\" ..$ biotype : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\" ..$ order : num [1:4(1d)] 0 0 1 2 ..$ ranges :List of 5 .. ..$ chr : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\" .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645 .. ..$ end : num [1:4(1d)] 16062813 16118933 16101494 16100668 .. ..$ id : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\" # make the prediction with a model model$predict_on_batch(batch$inputs) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 Troubleshooting Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain. Run the same code in python or CLI When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run $ kipoi test MaxEntScan/3prime --source=kipoi from the command-line first. Dependency issues It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run: $ kipoi env create MaxEntScan $ source activate kipoi-MaxEntScan This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run $ kipoi env create MaxEntScan $ source activate kipoi-MaxEntScan To use that environment in R, run: use_condaenv(\"kipoi-MaxEntScan__3prime\") Make sure you run that code snippet right after importing the reticulate library (i.e. make sure you run it before kipoi <- import('kipoi') ) Float/Double type issues When using a pytorch model: DeepSEA/predict kipoi$install_model_requirements(\"DeepSEA/predict\") # Get the dataloader setwd('~/.kipoi/models/DeepSEA/predict') model <- kipoi$get_model(\"DeepSEA/predict\") dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) # model$predict_on_batch(batch$inputs) We get an error: Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same This means that the feeded array is Double instead of Float. R arrays are by default converted to float64 numpy dtype: np <- import(\"numpy\", convert=FALSE) np$array(0.1)$dtype float64 np$array(batch$inputs)$dtype float64 To fix this, we need to explicitly convert them to float32 before passing the batch to the model: model$predict_on_batch(np$array(batch$inputs, dtype='float32')) 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651","title":"R API"},{"location":"tutorials/R-api/#using-kipoi-from-r","text":"Thanks to the reticulate R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that. Make sure you have git-lfs and Kipoi correctly installed: Install git-lfs conda install -c conda-forge git-lfs && git lfs install (alternatively see https://git-lfs.github.com/ ) Install kipoi pip install kipoi Please read docs/using/getting started before going through this notebook.","title":"Using Kipoi from R"},{"location":"tutorials/R-api/#install-and-load-reticulate","text":"Make sure you have the reticulate R package installed # install.packages(\"reticulate\") library(reticulate)","title":"Install and load reticulate"},{"location":"tutorials/R-api/#reticulate-quick-intro","text":"In general, using Kipoi from R is almost the same as using it from Python: instead of using object.method() or object.attribute as in python, use $ : object$method() , object$attribute . # short reticulate example os <- import(\"os\") os$chdir(\"/tmp\") os$getcwd() '/tmp'","title":"Reticulate quick intro"},{"location":"tutorials/R-api/#type-mapping-r-python","text":"Reticulate translates objects between R and python in the following way: R Python Examples Single-element vector Scalar 1 , 1L , TRUE , \"foo\" Multi-element vector List c(1.0, 2.0, 3.0) , c(1L, 2L, 3L) List of multiple types Tuple list(1L, TRUE, \"foo\") Named list Dict list(a = 1L, b = 2.0) , dict(x = x_data) Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2) Function Python function function(x) x + 1 NULL, TRUE, FALSE None, True, False NULL , TRUE , FALSE For more info on reticulate, please visit https://github.com/rstudio/reticulate/.","title":"Type mapping R &lt;-&gt; python"},{"location":"tutorials/R-api/#setup-the-python-environment","text":"With reticulate::py_config() you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with use_condaenv(...) . This comes handy when using different models depending on different conda environments. reticulate::py_config() python: /home/avsec/bin/anaconda3/bin/python libpython: /home/avsec/bin/anaconda3/lib/libpython3.6m.so pythonhome: /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3 version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) [GCC 7.2.0] numpy: /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy numpy_version: 1.14.0 os: /home/avsec/bin/anaconda3/lib/python3.6/os.py python versions found: /home/avsec/bin/anaconda3/bin/python /usr/bin/python /usr/bin/python3 List all conda environments: reticulate::conda_list() Create a new conda environment for the model: $ kipoi env create HAL Use that environment in R: reticulate::use_condaenv(\"kipoi-HAL')","title":"Setup the python environment"},{"location":"tutorials/R-api/#load-kipoi","text":"kipoi <- import(\"kipoi\")","title":"Load kipoi"},{"location":"tutorials/R-api/#list-models","text":"kipoi$list_models()$head() source model version \\ 0 kipoi DeepSEAKeras 0.1 1 kipoi extended_coda 0.1 2 kipoi DeepCpG_DNA/Hou2016_mESC_dna 1.0.4 3 kipoi DeepCpG_DNA/Smallwood2014_2i_dna 1.0.4 4 kipoi DeepCpG_DNA/Hou2016_HepG2_dna 1.0.4 authors \\ 0 [Author(name='Jian Zhou', github=None, email=N... 1 [Author(name='Pang Wei Koh', github='kohpangwe... 2 [Author(name='Christof Angermueller', github='... 3 [Author(name='Christof Angermueller', github='... 4 [Author(name='Christof Angermueller', github='... contributors \\ 0 [Author(name='Lara Urban', github='LaraUrban',... 1 [Author(name='Johnny Israeli', github='jisrael... 2 [Author(name='Roman Kreuzhuber', github='krrom... 3 [Author(name='Roman Kreuzhuber', github='krrom... 4 [Author(name='Roman Kreuzhuber', github='krrom... doc type \\ 0 This CNN is based on the DeepSEA model from Zh... keras 1 Single bp resolution ChIP-seq denoising - http... keras 2 This is the extraction of the DNA-part of the ... keras 3 This is the extraction of the DNA-part of the ... keras 4 This is the extraction of the DNA-part of the ... keras inputs targets \\ 0 seq TFBS_DHS_probs 1 [H3K27AC_subsampled] [H3K27ac] 2 [dna] [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c... 3 [dna] [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ... 4 [dna] [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2... postproc_score_variants license \\ 0 True MIT 1 False MIT 2 True MIT 3 True MIT 4 True MIT cite_as \\ 0 https://doi.org/10.1038/nmeth.3547 1 https://doi.org/10.1093/bioinformatics/btx243 2 https://doi.org/10.1186/s13059-017-1189-z, htt... 3 https://doi.org/10.1186/s13059-017-1189-z, htt... 4 https://doi.org/10.1186/s13059-017-1189-z, htt... trained_on \\ 0 ENCODE and Roadmap Epigenomics chromatin profi... 1 Described in https://academic.oup.com/bioinfor... 2 scBS-seq and scRRBS-seq datasets, https://geno... 3 scBS-seq and scRRBS-seq datasets, https://geno... 4 scBS-seq and scRRBS-seq datasets, https://geno... training_procedure \\ 0 https://www.nature.com/articles/nmeth.3547#met... 1 Described in https://academic.oup.com/bioinfor... 2 Described in https://genomebiology.biomedcentr... 3 Described in https://genomebiology.biomedcentr... 4 Described in https://genomebiology.biomedcentr... tags 0 [Histone modification, DNA binding, DNA access... 1 [Histone modification] 2 [DNA methylation] 3 [DNA methylation] 4 [DNA methylation] reticulate currently doesn't support direct convertion from pandas.DataFrame to R's data.frame . Let's make a convenience function to create an R dataframe via matrix conversion. #' List models as an R data.frame kipoi_list_models <- function() { df_models <- kipoi$list_models() df <- data.frame(df_models$as_matrix()) colnames(df) = df_models$columns$tolist() return(df) } df <- kipoi_list_models() head(df, 2) source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags kipoi DeepSEAKeras 0.1 <environment: 0x556afc757e38> <environment: 0x556afbb0d538> This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras seq TFBS_DHS_probs TRUE MIT https://doi.org/10.1038/nmeth.3547 ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods https://www.nature.com/articles/nmeth.3547#methods <environment: 0x556afcddfd50> kipoi extended_coda 0.1 <environment: 0x556afc764260> <environment: 0x556afbaff708> Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda keras H3K27AC_subsampled H3K27ac FALSE MIT https://doi.org/10.1093/bioinformatics/btx243 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60>","title":"List models"},{"location":"tutorials/R-api/#get-the-kipoi-model-and-make-a-prediction-for-the-example-files","text":"To run the following example, make sure you have all the dependencies installed. Run: kipoi$install_model_requirements(\"MaxEntScan/3prime\") from R or kipoi env create MaxEntScan source activate kipoi-MaxEntScan from the command-line. This will install all the required dependencies for both, the model and the dataloader. kipoi$install_model_requirements(\"MaxEntScan/3prime\") model <- kipoi$get_model(\"MaxEntScan/3prime\") predictions <- model$pipeline$predict_example() head(predictions) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 -9.52033554891735 9.54342300799607","title":"Get the kipoi model and make a prediction for the example files"},{"location":"tutorials/R-api/#use-the-model-and-dataloader-independently","text":"# Get the dataloader setwd('~/.kipoi/models/MaxEntScan/3prime') dl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) it DataLoaderIter # Retrieve a batch of data batch <- iter_next(it) str(batch) List of 2 $ inputs : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\" $ metadata:List of 5 ..$ geneID : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\" ..$ biotype : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\" ..$ order : num [1:4(1d)] 0 0 1 2 ..$ ranges :List of 5 .. ..$ chr : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\" .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645 .. ..$ end : num [1:4(1d)] 16062813 16118933 16101494 16100668 .. ..$ id : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\" # make the prediction with a model model$predict_on_batch(batch$inputs) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451","title":"Use the model and dataloader independently"},{"location":"tutorials/R-api/#troubleshooting","text":"Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain.","title":"Troubleshooting"},{"location":"tutorials/R-api/#run-the-same-code-in-python-or-cli","text":"When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run $ kipoi test MaxEntScan/3prime --source=kipoi from the command-line first.","title":"Run the same code in python or CLI"},{"location":"tutorials/R-api/#dependency-issues","text":"It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run: $ kipoi env create MaxEntScan $ source activate kipoi-MaxEntScan This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run $ kipoi env create MaxEntScan $ source activate kipoi-MaxEntScan To use that environment in R, run: use_condaenv(\"kipoi-MaxEntScan__3prime\") Make sure you run that code snippet right after importing the reticulate library (i.e. make sure you run it before kipoi <- import('kipoi') )","title":"Dependency issues"},{"location":"tutorials/R-api/#floatdouble-type-issues","text":"When using a pytorch model: DeepSEA/predict kipoi$install_model_requirements(\"DeepSEA/predict\") # Get the dataloader setwd('~/.kipoi/models/DeepSEA/predict') model <- kipoi$get_model(\"DeepSEA/predict\") dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) # model$predict_on_batch(batch$inputs) We get an error: Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same This means that the feeded array is Double instead of Float. R arrays are by default converted to float64 numpy dtype: np <- import(\"numpy\", convert=FALSE) np$array(0.1)$dtype float64 np$array(batch$inputs)$dtype float64 To fix this, we need to explicitly convert them to float32 before passing the batch to the model: model$predict_on_batch(np$array(batch$inputs, dtype='float32')) 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651","title":"Float/Double type issues"},{"location":"tutorials/composing_models/","text":"Generated from notebooks/composing_models.ipynb Composing models by Ziga Avsec Composing models means that we take the predictions of some model and use it as input for another model like this: Three different scenarios can occur when we want to compose models from Kipoi: all models are written in the same framework (say Keras) models are written in different frameworks but can all be executed in the same python environment models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities All models in the same framework In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras. Let's first dump 4 dummy models: import keras.layers as kl from keras.models import Model from keras.models import load_model # create model 1 inp1 = kl.Input((3,), name=\"input1\") out1 = kl.Dense(4)(inp1) m1 = Model(inp1, out1) m1.save(\"/tmp/m1.h5\") # create model 2 inp2 = kl.Input((7,), name=\"input1_model1\") out2 = kl.Dense(3)(inp2) m2 = Model(inp2, out2) m2.save(\"/tmp/m2.h5\") # create model 3 inp3 = kl.Input((6,), name=\"input2\") out3 = kl.Dense(4)(inp3) m3 = Model(inp3, out3) m3.save(\"/tmp/m3.h5\") # create model 4 inp4 = kl.Input((7,), name=\"model2_model3\") out4 = kl.Dense(1)(inp4) m4 = Model(inp4, out4) m4.save(\"/tmp/m4.h5\") Next, we load the models back: ## Load models m1 = load_model(\"/tmp/m1.h5\") m2 = load_model(\"/tmp/m2.h5\") m3 = load_model(\"/tmp/m3.h5\") m4 = load_model(\"/tmp/m4.h5\") /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn('No training configuration found in save file: ' And compose them m2_in = kl.concatenate([m1.output, m1.input]) m2_out = m2(m2_in) m3_in = kl.concatenate([m2_out, m3.output]) out = m4(m3_in) m = Model(inputs=[m1.input, m3.input], outputs=out) from IPython.display import SVG from keras.utils.vis_utils import model_to_dot svg_img = model_to_dot(m, ).create(prog='dot', format='svg') SVG(svg_img) Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting m1.trainable = False . Contributing to Kipoi To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model. Models in different frameworks There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses keras=0.3 and and another one keras=2.0 ). Compatible dependencies To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model. def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1): m1 = kipoi.get_model(\"model1\") m2 = kipoi.get_model(\"model2\") m3 = kipoi.get_model(\"model3\") dl1 = m1.default_dataloader(**dl1_kwargs) dl2 = m1.default_dataloader(**dl2_kwargs) target_gen = get_target_gen(target_file) batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers) batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers) while True: batch1 = next(batch_it1)['inputs'] batch2 = next(batch_it2)['inputs'] targets, ids = next(target_gen) m1_pred = m1.predict_on_batch(batch1) m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1)) m3_pred = m3.predict_on_batch(batch2) yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \"targets\": targets, \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"], \"model3_id\": batch2[\"metadata\"][\"id\"], \"targets_id\": ids, } } # create model 4 inp2 = kl.Input((3,), name=\"model2\") inp3 = kl.Input((4,), name=\"model3\") x = kl.concatenate([inp2, inp3]) out4 = kl.Dense(1)(x) m4 = Model([inp2, inp3], out4) m4.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Train model4 def create_train_gen(**kwargs): while True: gen = new_dataloader(**kwargs) while True: batch = next(gen) yield (batch['inputs'], batch['targets']) train_gen = create_train_gen(...) m4.fit_generator(train_gen, ...) # Dump model4 m4.save(\"model_files/model.h5\") Incompatible dependencies Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk. Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake. Let's consider the following case: # Python part of the Snakefile import os import subprocess py_path = subprocess.check_output(['which', 'python']).decode().strip() env_paths = os.path.join(os.path.dirname(py_path), \"../envs\") def get_args(wildcards): \"\"\"Function returning a dictionary of dataloader kwargs for the corresponding model \"\"\" if wildcards.model == \"model3\": return {\"arg1\": 1} elif wildcards.model == \"model3\": return {\"\"} else: return {\"arg2\": 1} # Yaml part of the Snakefile rule all: inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"]) rule create_evironment: \"\"\"Create a new conda environment for each model\"\"\" output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\" rule run_predictions: \"\"\"Create a new conda environment for each model\"\"\" input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") output: \"predictions/{model}.h5\" params: dl_args: get_args batch_size: 15 threads: 8 shell: \"\"\" source activate kipoi-{wildcards.model} kipoi predict {wildcards.model} \\ -n {threads} \\ --dataloader_args='{params.dl_args}' \\ --batch_size={params.batch_size} \\ -f hdf5 \\ -o {output} \"\"\" This snakefile will generate the following hdf5 files predictions/model1.h5 predictions/model2.h5 To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions import deepdish def new_dataloader(model1_h5, model2_h5, target_file): d1 = deepdish.io.load(model1_h5) d2 = deepdish.io.load(model2_h5) targets = load_target_file(target_file) return { \"inputs\": { \"model1\": d1[\"predictions\"], \"model2\": d2[\"predictions\"], }, \"targets\": targets, \"metadata\": { \"model1_id\": d1[\"metdata\"][\"id\"], \"model2_id\": d2[\"metdata\"][\"id\"], } } # get the training data ... data_train = new_dataloader(\"predictions/model1.h5\" \"predictions/model1.h5\", \"target_file.h5\") # train the model... m4.fit(data_train['inputs'], data_train['targets']) # Dump the model m4.save(\"model_files/model.h5\") Uploading composite models to Kipoi Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so: All models in the same framework Dataloader: newly written, combines dataloaders Model: combines models by stitching them together in the framework Different frameworks, compatible dependencies Dataloader: newly written, combines dataloaders and models Model: final ensembling model (model 4) Different frameworks, in-compatible dependencies Dataloader: newly written, loads data from the hdf5 files containing model predictions Model: final ensembling model (model 4)","title":"Composing models"},{"location":"tutorials/composing_models/#composing-models","text":"by Ziga Avsec Composing models means that we take the predictions of some model and use it as input for another model like this: Three different scenarios can occur when we want to compose models from Kipoi: all models are written in the same framework (say Keras) models are written in different frameworks but can all be executed in the same python environment models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities","title":"Composing models"},{"location":"tutorials/composing_models/#all-models-in-the-same-framework","text":"In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras. Let's first dump 4 dummy models: import keras.layers as kl from keras.models import Model from keras.models import load_model # create model 1 inp1 = kl.Input((3,), name=\"input1\") out1 = kl.Dense(4)(inp1) m1 = Model(inp1, out1) m1.save(\"/tmp/m1.h5\") # create model 2 inp2 = kl.Input((7,), name=\"input1_model1\") out2 = kl.Dense(3)(inp2) m2 = Model(inp2, out2) m2.save(\"/tmp/m2.h5\") # create model 3 inp3 = kl.Input((6,), name=\"input2\") out3 = kl.Dense(4)(inp3) m3 = Model(inp3, out3) m3.save(\"/tmp/m3.h5\") # create model 4 inp4 = kl.Input((7,), name=\"model2_model3\") out4 = kl.Dense(1)(inp4) m4 = Model(inp4, out4) m4.save(\"/tmp/m4.h5\") Next, we load the models back: ## Load models m1 = load_model(\"/tmp/m1.h5\") m2 = load_model(\"/tmp/m2.h5\") m3 = load_model(\"/tmp/m3.h5\") m4 = load_model(\"/tmp/m4.h5\") /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn('No training configuration found in save file: ' And compose them m2_in = kl.concatenate([m1.output, m1.input]) m2_out = m2(m2_in) m3_in = kl.concatenate([m2_out, m3.output]) out = m4(m3_in) m = Model(inputs=[m1.input, m3.input], outputs=out) from IPython.display import SVG from keras.utils.vis_utils import model_to_dot svg_img = model_to_dot(m, ).create(prog='dot', format='svg') SVG(svg_img) Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting m1.trainable = False .","title":"All models in the same framework"},{"location":"tutorials/composing_models/#contributing-to-kipoi","text":"To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model.","title":"Contributing to Kipoi"},{"location":"tutorials/composing_models/#models-in-different-frameworks","text":"There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses keras=0.3 and and another one keras=2.0 ).","title":"Models in different frameworks"},{"location":"tutorials/composing_models/#compatible-dependencies","text":"To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model. def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1): m1 = kipoi.get_model(\"model1\") m2 = kipoi.get_model(\"model2\") m3 = kipoi.get_model(\"model3\") dl1 = m1.default_dataloader(**dl1_kwargs) dl2 = m1.default_dataloader(**dl2_kwargs) target_gen = get_target_gen(target_file) batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers) batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers) while True: batch1 = next(batch_it1)['inputs'] batch2 = next(batch_it2)['inputs'] targets, ids = next(target_gen) m1_pred = m1.predict_on_batch(batch1) m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1)) m3_pred = m3.predict_on_batch(batch2) yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \"targets\": targets, \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"], \"model3_id\": batch2[\"metadata\"][\"id\"], \"targets_id\": ids, } } # create model 4 inp2 = kl.Input((3,), name=\"model2\") inp3 = kl.Input((4,), name=\"model3\") x = kl.concatenate([inp2, inp3]) out4 = kl.Dense(1)(x) m4 = Model([inp2, inp3], out4) m4.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Train model4 def create_train_gen(**kwargs): while True: gen = new_dataloader(**kwargs) while True: batch = next(gen) yield (batch['inputs'], batch['targets']) train_gen = create_train_gen(...) m4.fit_generator(train_gen, ...) # Dump model4 m4.save(\"model_files/model.h5\")","title":"Compatible dependencies"},{"location":"tutorials/composing_models/#incompatible-dependencies","text":"Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk. Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake. Let's consider the following case: # Python part of the Snakefile import os import subprocess py_path = subprocess.check_output(['which', 'python']).decode().strip() env_paths = os.path.join(os.path.dirname(py_path), \"../envs\") def get_args(wildcards): \"\"\"Function returning a dictionary of dataloader kwargs for the corresponding model \"\"\" if wildcards.model == \"model3\": return {\"arg1\": 1} elif wildcards.model == \"model3\": return {\"\"} else: return {\"arg2\": 1} # Yaml part of the Snakefile rule all: inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"]) rule create_evironment: \"\"\"Create a new conda environment for each model\"\"\" output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\" rule run_predictions: \"\"\"Create a new conda environment for each model\"\"\" input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") output: \"predictions/{model}.h5\" params: dl_args: get_args batch_size: 15 threads: 8 shell: \"\"\" source activate kipoi-{wildcards.model} kipoi predict {wildcards.model} \\ -n {threads} \\ --dataloader_args='{params.dl_args}' \\ --batch_size={params.batch_size} \\ -f hdf5 \\ -o {output} \"\"\" This snakefile will generate the following hdf5 files predictions/model1.h5 predictions/model2.h5 To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions import deepdish def new_dataloader(model1_h5, model2_h5, target_file): d1 = deepdish.io.load(model1_h5) d2 = deepdish.io.load(model2_h5) targets = load_target_file(target_file) return { \"inputs\": { \"model1\": d1[\"predictions\"], \"model2\": d2[\"predictions\"], }, \"targets\": targets, \"metadata\": { \"model1_id\": d1[\"metdata\"][\"id\"], \"model2_id\": d2[\"metdata\"][\"id\"], } } # get the training data ... data_train = new_dataloader(\"predictions/model1.h5\" \"predictions/model1.h5\", \"target_file.h5\") # train the model... m4.fit(data_train['inputs'], data_train['targets']) # Dump the model m4.save(\"model_files/model.h5\")","title":"Incompatible dependencies"},{"location":"tutorials/composing_models/#uploading-composite-models-to-kipoi","text":"Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so: All models in the same framework Dataloader: newly written, combines dataloaders Model: combines models by stitching them together in the framework Different frameworks, compatible dependencies Dataloader: newly written, combines dataloaders and models Model: final ensembling model (model 4) Different frameworks, in-compatible dependencies Dataloader: newly written, loads data from the hdf5 files containing model predictions Model: final ensembling model (model 4)","title":"Uploading composite models to Kipoi"},{"location":"tutorials/contributing_models/","text":"Generated from notebooks/contributing_models.ipynb Contributing a model to the Kipoi model repository This notebook will show you how to contribute a model to the Kipoi model repository . For a simple 'model contribution checklist' see also http://kipoi.org/docs/contributing/01_Getting_started . Kipoi basics Contributing a model to Kipoi means writing a sub-folder with all the required files to the Kipoi model repository via pull request. Two main components of the model repository are model and dataloader . Model Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the predict_on_batch(x) method, where x is dictionary/list of numpy arrays. The model contributor needs to provide one of the following: Serialized Keras model Serialized Sklearn model Custom model inheriting from keras.model.BaseModel . all the required files, i.e. weights need to be loaded in the __init__ See http://kipoi.org/docs/contributing/02_Writing_model.yaml/ and http://kipoi.org/docs/contributing/05_Writing_model.py/ for more info. Dataloader Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. Before writing your own dataloader take a look at our kipoiseq repository to see whether your use-case is covered by the available dataloaders. Writing your own dataloader Technically, dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following: PreloadedDataset Dataset BatchDataset SampleIterator BatchIterator SampleGenerator BatchGenerator See http://kipoi.org/docs/contributing/04_Writing_dataloader.py/ for more info. Folder layout Here is an example folder structure of a Kipoi model: MyModel \u251c\u2500\u2500 dataloader.py # implements the dataloader (only necessary if you wrote your own dataloader) \u251c\u2500\u2500 dataloader.yaml # describes the dataloader (only necessary if you wrote your own dataloader) \u2514\u2500\u2500 model.yaml # describes the model The model.yaml and dataloader.yaml files a complete description about the model, the dataloader and the files they depend on. Contributing a simple Iris-classifier Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known Iris dataset. Outline Train the model Generate the model directory Store all data files required for the model and the dataloader in a temporary folder Write model.yaml Write dataloader.yaml Write dataloader.py Test with the model with $ kipoi test . Publish data files on zenodo Update model.yaml and dataloader.yaml to contain the links Test again Commit, push and generate a pull request 1. Train the model Load and pre-process the data import pandas as pd import os from sklearn.preprocessing import LabelBinarizer, StandardScaler from sklearn import datasets iris = datasets.load_iris() # view more info about the dataset # print(iris[\"DESCR\"]) # Data pre-processing y_transformer = LabelBinarizer().fit(iris[\"target\"]) x_transformer = StandardScaler().fit(iris[\"data\"]) x = x_transformer.transform(iris[\"data\"]) y = y_transformer.transform(iris[\"target\"]) x[:3] array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]]) y[:3] array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]) Train an example model Let's train a simple linear-regression model using Keras. from keras.models import Model import keras.layers as kl inp = kl.Input(shape=(4, ), name=\"features\") out = kl.Dense(units=3)(inp) model = Model(inp, out) model.compile(\"adam\", \"categorical_crossentropy\") model.fit(x, y, verbose=0) Using TensorFlow backend. WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead <keras.callbacks.History at 0x2ab58e8ba860> 2. Set the model directory up: In reality, you would also need to Fork the kipoi/models repository Clone your repository fork, ignoring all the git-lfs files $ git clone git@github.com:<your_username>/models.git Create a new folder <mynewmodel> 3. Store the files in a temporary directory All the data of the model will have to be published on zenodo or figshare before the pull request is performed. While setting the Kipoi model up, it is handy the keep the models in a temporary directory in the model folder, which we will delete prior to the pull request. # create the model directory !mkdir contribution_sample_model # create the temporary directory where we will keep the files that should later be published in zenodo or figshare !mkdir contribution_sample_model/tmp Now we can change the current working directory to the model directory: import os os.chdir(\"contribution_sample_model\") 3a. Static files for dataloader Since in our case here we require to write a new dataloader. The dataloader can use some trained transformer instances (here the LabelBinarizer and StandardScaler transformers form sklearn). These should be uploaded with the model files and then referenced correctly in the dataloader.yaml file. We will store the required files in the temporary folder: import pickle with open(\"tmp/y_transformer.pkl\", \"wb\") as f: pickle.dump(y_transformer, f, protocol=2) with open(\"tmp/x_transformer.pkl\", \"wb\") as f: pickle.dump(x_transformer, f, protocol=2) ! ls tmp x_transformer.pkl y_transformer.pkl 3b. Model definition / weights Now that we have the static files that are required by the dataloader, we also need to store the model architecture and weights: # Architecture with open(\"tmp/model.json\", \"w\") as f: f.write(model.to_json()) # Weights model.save_weights(\"tmp/weights.h5\") Alternatively if we would be using a scikit-learn model we would save the pickle file: # Alternatively, for the scikit-learn model we would save the pickle file from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier lr = OneVsRestClassifier(LogisticRegression()) lr.fit(x, y) with open(\"tmp/sklearn_model.pkl\", \"wb\") as f: pickle.dump(lr, f, protocol=2) 3c. Example files for the dataloader Every Kipoi dataloader has to provide a set of example files so that Kipoi can perform its automated tests and users can have an idea what the dataloader files have to look like. Again we will store the files in the temporary folder: # select first 20 rows of the iris dataset X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"]) y = pd.DataFrame({\"class\": iris[\"target\"][:20]}) # store the model input features and targets as csv files with column names: X.to_csv(\"tmp/example_features.csv\", index=False) y.to_csv(\"tmp/example_targets.csv\", index=False) 4 Write the model.yaml Now it is time to write the model.yaml in the model directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. model_yaml = \"\"\" defined_as: kipoi.model.KerasModel # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: tmp/model.json weights: tmp/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python - h5py - pip pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" \"\"\" with open(\"model.yaml\", \"w\") as ofh: ofh.write(model_yaml) 5 and 6 Write the dataloader.yaml and dataloader.py PLEASE REMEMBER: Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in kipoiseq and use those as explained in the Kipoi docs. Now it is time to write the dataloader.yaml . Since we defined the default_dataloader field in model.yaml as . Kipoi will expect that our dataloader.yaml file lies in the same directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. dataloader_yaml = \"\"\" type: Dataset defined_as: dataloader.MyDataset args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: tmp/example_features.csv # example files x_transformer: default: tmp/x_transformer.pkl #default: # url: https://github.com/kipoi/kipoi/raw/57734d716b8dedaffe460855e7cfe8f37ec2d48d/example/models/sklearn_iris/dataloader_files/x_transformer.pkl # md5: bc1bf3c61c418b2d07506a7d0521a893 y_transformer: default: tmp/y_transformer.pkl targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: tmp/example_targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3, ) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: doc: Just an example metadata column \"\"\" with open(\"dataloader.yaml\", \"w\") as ofh: ofh.write(dataloader_yaml) Since we have referred to the dataloader as dataloader.MyDataset we expect a dataloader.py file in the same directory as dataloader.yaml which has to contain the dataloader class, which is here MyDataset . Notice that external static files are arguments to the __init__ function! Their path was defined in the dataloader.yaml . import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } In order to elucidate what the Dataloader class does I will make a few function calls that are usually performed by the Kipoi API in order to generate model input: # instantiate the dataloader ds = MyDataset(\"tmp/example_features.csv\", \"tmp/example_targets.csv\", \"tmp/x_transformer.pkl\", \"tmp/y_transformer.pkl\") # call __getitem__ ds[5] {'inputs': {'features': array([-0.53717756, 1.95766909, -1.17067529, -1.05003079])}, 'targets': array([1, 0, 0]), 'metadata': {'example_row_number': 5}} it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2) next(it) {'inputs': {'features': array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]])}, 'targets': array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]), 'metadata': {'example_row_number': array([0, 1, 2])}} I will now store the code from above in a file so that we can test it: dataloader_py = \"\"\" import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } \"\"\" with open(\"dataloader.py\", \"w\") as ofh: ofh.write(dataloader_py) 7 Test the model Now it is time to test the model. !kipoi test . \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/opt/model-zoo/notebooks/contribution_sample_model/dataloader.MyDataset\u001b[0m Using TensorFlow backend. 2018-10-11 17:41:58.586759: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='tmp/model.json' mode='r' encoding='UTF-8'>\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from tmp/weights.h5\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 28.88it/s] \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.main]\u001b[0m Successfully ran test_predict\u001b[0m 8. Publish data on zenodo or figshare Now that the model works It is time to upload the data files onto zenodo or figshare. To do so follow the instructions on the website. It might be necessary to remove file suffixes in order to be able to load the respective files. 9 Update model.yaml and dataloader.yaml Now the local file paths in model.yaml and dataloader.yaml have to be replaced by the zenodo / figshare URLs in the following way. The entry: args: ... x_transformer: default: tmp/x_transformer.pkl would be replaced by: args: ... x_transformer: default: url: https://zenodo.org/path/to/example_files/x_transformer.pkl md5: 76a5sd76asd57 So every local path has to be replaced by the url and md5 combination. Where md5 is the md5 sum of the file. If you cannot find the the md5 sum on the zenodo / figshare website you can for example run curl https://zenodo.org/.../x_transformer.pkl | md5sum to calculate the md5 sum. Now after replacing all the files, test the setup again by running kipoi test . and then delete the tmp folder. Now the only file(s) remaining in the folder should be model.yaml (and in this case also: dataloader.py dataloader.yaml ). 9 Test again Now that you have deleted the temporary files, rerun the test to make sure everything works fine. 10 Commit and push Now commit the model.yaml and if needed (like in this example) also the dataloader.py and datalaoder.yaml files by running: git add model.yaml . Now you can push back to your fork ( git push ) and submit a pull request with kipoi/models to request adding your model to the Kipoi models. Accessing local models through kipoi In Kipoi it is not necessary to publish your model. You can leverage the full functionality of Kipoi also for local models. All you have to do is specify --source dir when using the CLI or setting source=\"dir\" in the python API. The model name is then the local path to the model folder. import kipoi m = kipoi.get_model(\".\", source=\"dir\") # See also python-sdk.ipynb m.pipeline.predict({\"features_file\": \"tmp/example_features.csv\", \"targets_file\": \"tmp/example_targets.csv\" })[:5] 0it [00:00, ?it/s]\u001b[A 1it [00:00, 19.03it/s]\u001b[A array([[ 3.2324865 , -0.29753828, 0.62135816], [ 2.8549244 , 0.4957999 , 0.6873083 ], [ 3.2744825 , 0.40906954, 0.99161 ], [ 3.1413555 , 0.58123374, 1.0272367 ], [ 3.416262 , -0.34901416, 0.76257455]], dtype=float32) m.info ModelInfo(authors=[Author(name='Your Name', github='your_github_username', email='your_email@host.org')], doc='Model predicting the Iris species', name=None, version='0.1', license='MIT', tags=[], contributors=[], cite_as='https://doi.org:/...', trained_on='Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris)', training_procedure=None) m.default_dataloader dataloader.MyDataset m.model <keras.engine.training.Model at 0x2ab5a3eff668> m.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x2ab5a2d75160>> Recap Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use kipoi init to setup a model directory. Make sure you have read the getting started guide for contributing models.","title":"Contributing models"},{"location":"tutorials/contributing_models/#contributing-a-model-to-the-kipoi-model-repository","text":"This notebook will show you how to contribute a model to the Kipoi model repository . For a simple 'model contribution checklist' see also http://kipoi.org/docs/contributing/01_Getting_started .","title":"Contributing a model to the Kipoi model repository"},{"location":"tutorials/contributing_models/#kipoi-basics","text":"Contributing a model to Kipoi means writing a sub-folder with all the required files to the Kipoi model repository via pull request. Two main components of the model repository are model and dataloader .","title":"Kipoi basics"},{"location":"tutorials/contributing_models/#model","text":"Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the predict_on_batch(x) method, where x is dictionary/list of numpy arrays. The model contributor needs to provide one of the following: Serialized Keras model Serialized Sklearn model Custom model inheriting from keras.model.BaseModel . all the required files, i.e. weights need to be loaded in the __init__ See http://kipoi.org/docs/contributing/02_Writing_model.yaml/ and http://kipoi.org/docs/contributing/05_Writing_model.py/ for more info.","title":"Model"},{"location":"tutorials/contributing_models/#dataloader","text":"Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. Before writing your own dataloader take a look at our kipoiseq repository to see whether your use-case is covered by the available dataloaders.","title":"Dataloader"},{"location":"tutorials/contributing_models/#writing-your-own-dataloader","text":"Technically, dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following: PreloadedDataset Dataset BatchDataset SampleIterator BatchIterator SampleGenerator BatchGenerator See http://kipoi.org/docs/contributing/04_Writing_dataloader.py/ for more info.","title":"Writing your own dataloader"},{"location":"tutorials/contributing_models/#folder-layout","text":"Here is an example folder structure of a Kipoi model: MyModel \u251c\u2500\u2500 dataloader.py # implements the dataloader (only necessary if you wrote your own dataloader) \u251c\u2500\u2500 dataloader.yaml # describes the dataloader (only necessary if you wrote your own dataloader) \u2514\u2500\u2500 model.yaml # describes the model The model.yaml and dataloader.yaml files a complete description about the model, the dataloader and the files they depend on.","title":"Folder layout"},{"location":"tutorials/contributing_models/#contributing-a-simple-iris-classifier","text":"Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known Iris dataset.","title":"Contributing a simple Iris-classifier"},{"location":"tutorials/contributing_models/#outline","text":"Train the model Generate the model directory Store all data files required for the model and the dataloader in a temporary folder Write model.yaml Write dataloader.yaml Write dataloader.py Test with the model with $ kipoi test . Publish data files on zenodo Update model.yaml and dataloader.yaml to contain the links Test again Commit, push and generate a pull request","title":"Outline"},{"location":"tutorials/contributing_models/#1-train-the-model","text":"","title":"1. Train the model"},{"location":"tutorials/contributing_models/#load-and-pre-process-the-data","text":"import pandas as pd import os from sklearn.preprocessing import LabelBinarizer, StandardScaler from sklearn import datasets iris = datasets.load_iris() # view more info about the dataset # print(iris[\"DESCR\"]) # Data pre-processing y_transformer = LabelBinarizer().fit(iris[\"target\"]) x_transformer = StandardScaler().fit(iris[\"data\"]) x = x_transformer.transform(iris[\"data\"]) y = y_transformer.transform(iris[\"target\"]) x[:3] array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]]) y[:3] array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])","title":"Load and pre-process the data"},{"location":"tutorials/contributing_models/#train-an-example-model","text":"Let's train a simple linear-regression model using Keras. from keras.models import Model import keras.layers as kl inp = kl.Input(shape=(4, ), name=\"features\") out = kl.Dense(units=3)(inp) model = Model(inp, out) model.compile(\"adam\", \"categorical_crossentropy\") model.fit(x, y, verbose=0) Using TensorFlow backend. WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead <keras.callbacks.History at 0x2ab58e8ba860>","title":"Train an example model"},{"location":"tutorials/contributing_models/#2-set-the-model-directory-up","text":"In reality, you would also need to Fork the kipoi/models repository Clone your repository fork, ignoring all the git-lfs files $ git clone git@github.com:<your_username>/models.git Create a new folder <mynewmodel>","title":"2. Set the model directory up:"},{"location":"tutorials/contributing_models/#3-store-the-files-in-a-temporary-directory","text":"All the data of the model will have to be published on zenodo or figshare before the pull request is performed. While setting the Kipoi model up, it is handy the keep the models in a temporary directory in the model folder, which we will delete prior to the pull request. # create the model directory !mkdir contribution_sample_model # create the temporary directory where we will keep the files that should later be published in zenodo or figshare !mkdir contribution_sample_model/tmp Now we can change the current working directory to the model directory: import os os.chdir(\"contribution_sample_model\")","title":"3. Store the files in a temporary directory"},{"location":"tutorials/contributing_models/#3a-static-files-for-dataloader","text":"Since in our case here we require to write a new dataloader. The dataloader can use some trained transformer instances (here the LabelBinarizer and StandardScaler transformers form sklearn). These should be uploaded with the model files and then referenced correctly in the dataloader.yaml file. We will store the required files in the temporary folder: import pickle with open(\"tmp/y_transformer.pkl\", \"wb\") as f: pickle.dump(y_transformer, f, protocol=2) with open(\"tmp/x_transformer.pkl\", \"wb\") as f: pickle.dump(x_transformer, f, protocol=2) ! ls tmp x_transformer.pkl y_transformer.pkl","title":"3a. Static files for dataloader"},{"location":"tutorials/contributing_models/#3b-model-definition-weights","text":"Now that we have the static files that are required by the dataloader, we also need to store the model architecture and weights: # Architecture with open(\"tmp/model.json\", \"w\") as f: f.write(model.to_json()) # Weights model.save_weights(\"tmp/weights.h5\") Alternatively if we would be using a scikit-learn model we would save the pickle file: # Alternatively, for the scikit-learn model we would save the pickle file from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier lr = OneVsRestClassifier(LogisticRegression()) lr.fit(x, y) with open(\"tmp/sklearn_model.pkl\", \"wb\") as f: pickle.dump(lr, f, protocol=2)","title":"3b. Model definition / weights"},{"location":"tutorials/contributing_models/#3c-example-files-for-the-dataloader","text":"Every Kipoi dataloader has to provide a set of example files so that Kipoi can perform its automated tests and users can have an idea what the dataloader files have to look like. Again we will store the files in the temporary folder: # select first 20 rows of the iris dataset X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"]) y = pd.DataFrame({\"class\": iris[\"target\"][:20]}) # store the model input features and targets as csv files with column names: X.to_csv(\"tmp/example_features.csv\", index=False) y.to_csv(\"tmp/example_targets.csv\", index=False)","title":"3c. Example files for the dataloader"},{"location":"tutorials/contributing_models/#4-write-the-modelyaml","text":"Now it is time to write the model.yaml in the model directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. model_yaml = \"\"\" defined_as: kipoi.model.KerasModel # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: tmp/model.json weights: tmp/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python - h5py - pip pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" \"\"\" with open(\"model.yaml\", \"w\") as ofh: ofh.write(model_yaml)","title":"4 Write the model.yaml"},{"location":"tutorials/contributing_models/#5-and-6-write-the-dataloaderyaml-and-dataloaderpy","text":"PLEASE REMEMBER: Before writing a dataloader yourself please check whether the same functionality can be achieved using a ready-made dataloader in kipoiseq and use those as explained in the Kipoi docs. Now it is time to write the dataloader.yaml . Since we defined the default_dataloader field in model.yaml as . Kipoi will expect that our dataloader.yaml file lies in the same directory. Since we are in the testing stage we will be using local file paths in the args field - those will be replaced by zenodo links once everything is ready for publication. dataloader_yaml = \"\"\" type: Dataset defined_as: dataloader.MyDataset args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: tmp/example_features.csv # example files x_transformer: default: tmp/x_transformer.pkl #default: # url: https://github.com/kipoi/kipoi/raw/57734d716b8dedaffe460855e7cfe8f37ec2d48d/example/models/sklearn_iris/dataloader_files/x_transformer.pkl # md5: bc1bf3c61c418b2d07506a7d0521a893 y_transformer: default: tmp/y_transformer.pkl targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: tmp/example_targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3, ) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: doc: Just an example metadata column \"\"\" with open(\"dataloader.yaml\", \"w\") as ofh: ofh.write(dataloader_yaml) Since we have referred to the dataloader as dataloader.MyDataset we expect a dataloader.py file in the same directory as dataloader.yaml which has to contain the dataloader class, which is here MyDataset . Notice that external static files are arguments to the __init__ function! Their path was defined in the dataloader.yaml . import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } In order to elucidate what the Dataloader class does I will make a few function calls that are usually performed by the Kipoi API in order to generate model input: # instantiate the dataloader ds = MyDataset(\"tmp/example_features.csv\", \"tmp/example_targets.csv\", \"tmp/x_transformer.pkl\", \"tmp/y_transformer.pkl\") # call __getitem__ ds[5] {'inputs': {'features': array([-0.53717756, 1.95766909, -1.17067529, -1.05003079])}, 'targets': array([1, 0, 0]), 'metadata': {'example_row_number': 5}} it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2) next(it) {'inputs': {'features': array([[-0.90068117, 1.03205722, -1.3412724 , -1.31297673], [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673], [-1.38535265, 0.33784833, -1.39813811, -1.31297673]])}, 'targets': array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]), 'metadata': {'example_row_number': array([0, 1, 2])}} I will now store the code from above in a file so that we can test it: dataloader_py = \"\"\" import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None, x_transformer=None, y_transformer=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(y_transformer) self.x_transformer = read_pickle(x_transformer) self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } \"\"\" with open(\"dataloader.py\", \"w\") as ofh: ofh.write(dataloader_py)","title":"5 and 6 Write the dataloader.yaml and dataloader.py"},{"location":"tutorials/contributing_models/#7-test-the-model","text":"Now it is time to test the model. !kipoi test . \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[33mWARNING\u001b[0m \u001b[44m[kipoi.specs]\u001b[0m doc empty for one of the dataloader `args` fields\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/opt/model-zoo/notebooks/contribution_sample_model/dataloader.MyDataset\u001b[0m Using TensorFlow backend. 2018-10-11 17:41:58.586759: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='tmp/model.json' mode='r' encoding='UTF-8'>\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from tmp/weights.h5\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 28.88it/s] \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.main]\u001b[0m Successfully ran test_predict\u001b[0m","title":"7 Test the model"},{"location":"tutorials/contributing_models/#8-publish-data-on-zenodo-or-figshare","text":"Now that the model works It is time to upload the data files onto zenodo or figshare. To do so follow the instructions on the website. It might be necessary to remove file suffixes in order to be able to load the respective files.","title":"8. Publish data on zenodo or figshare"},{"location":"tutorials/contributing_models/#9-update-modelyaml-and-dataloaderyaml","text":"Now the local file paths in model.yaml and dataloader.yaml have to be replaced by the zenodo / figshare URLs in the following way. The entry: args: ... x_transformer: default: tmp/x_transformer.pkl would be replaced by: args: ... x_transformer: default: url: https://zenodo.org/path/to/example_files/x_transformer.pkl md5: 76a5sd76asd57 So every local path has to be replaced by the url and md5 combination. Where md5 is the md5 sum of the file. If you cannot find the the md5 sum on the zenodo / figshare website you can for example run curl https://zenodo.org/.../x_transformer.pkl | md5sum to calculate the md5 sum. Now after replacing all the files, test the setup again by running kipoi test . and then delete the tmp folder. Now the only file(s) remaining in the folder should be model.yaml (and in this case also: dataloader.py dataloader.yaml ).","title":"9 Update model.yaml and dataloader.yaml"},{"location":"tutorials/contributing_models/#9-test-again","text":"Now that you have deleted the temporary files, rerun the test to make sure everything works fine.","title":"9 Test again"},{"location":"tutorials/contributing_models/#10-commit-and-push","text":"Now commit the model.yaml and if needed (like in this example) also the dataloader.py and datalaoder.yaml files by running: git add model.yaml . Now you can push back to your fork ( git push ) and submit a pull request with kipoi/models to request adding your model to the Kipoi models.","title":"10 Commit and push"},{"location":"tutorials/contributing_models/#accessing-local-models-through-kipoi","text":"In Kipoi it is not necessary to publish your model. You can leverage the full functionality of Kipoi also for local models. All you have to do is specify --source dir when using the CLI or setting source=\"dir\" in the python API. The model name is then the local path to the model folder. import kipoi m = kipoi.get_model(\".\", source=\"dir\") # See also python-sdk.ipynb m.pipeline.predict({\"features_file\": \"tmp/example_features.csv\", \"targets_file\": \"tmp/example_targets.csv\" })[:5] 0it [00:00, ?it/s]\u001b[A 1it [00:00, 19.03it/s]\u001b[A array([[ 3.2324865 , -0.29753828, 0.62135816], [ 2.8549244 , 0.4957999 , 0.6873083 ], [ 3.2744825 , 0.40906954, 0.99161 ], [ 3.1413555 , 0.58123374, 1.0272367 ], [ 3.416262 , -0.34901416, 0.76257455]], dtype=float32) m.info ModelInfo(authors=[Author(name='Your Name', github='your_github_username', email='your_email@host.org')], doc='Model predicting the Iris species', name=None, version='0.1', license='MIT', tags=[], contributors=[], cite_as='https://doi.org:/...', trained_on='Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris)', training_procedure=None) m.default_dataloader dataloader.MyDataset m.model <keras.engine.training.Model at 0x2ab5a3eff668> m.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x2ab5a2d75160>>","title":"Accessing local models through kipoi"},{"location":"tutorials/contributing_models/#recap","text":"Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use kipoi init to setup a model directory. Make sure you have read the getting started guide for contributing models.","title":"Recap"},{"location":"tutorials/python-api/","text":"Generated from notebooks/python-api.ipynb Kipoi python API Quick start There are three basic building blocks in kipoi: Source - provides Models and DataLoaders. Model - makes the prediction given the numpy arrays. Dataloader - loads the data from raw files and transforms them into a form that is directly consumable by the Model List of main commands Get/list sources - kipoi.list_sources() - kipoi.get_source() List models/dataloaders - kipoi.list_models() - kipoi.list_dataloaders() Get model/dataloader - kipoi.get_model() - kipoi.get_dataloader_factory() Load only model/dataloader description from the yaml file without loading the model kipoi.get_model_descr() kipoi.get_dataloader_descr() Install the dependencies - kipoi.install_model_dependencies() - kipoi.install_dataloader_dependencies() import kipoi Source Available sources are specified in the config file located at: ~/.kipoi/config.yaml . Here is an example config file: model_sources: kipoi: # default type: git-lfs # git repository with large file storage (git-lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: ~/.kipoi/models/ # local storage path gl: type: git-lfs # custom model remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git local_path: /s/project/model-zoo There are three different model sources possible: git-lfs - git repository with source files tracked normally by git and all the binary files like model weights (located in files* directories) are tracked by git-lfs . Requires git-lfs to be installed. git - all the files including weights (not recommended) local - local directory containing models defined in subdirectories For git-lfs source type, larger files tracked by git-lfs will be downloaded into the specified directory local_path only after the model has been requested (when invoking kipoi.get_model() ). Note A particular model/dataloader is defined by its source (say kipoi or my_git_models ) and the relative path of the desired model directory from the model source root (say rbp/ ). A directory is considered a model if it contains a model.yaml file. import kipoi import warnings warnings.filterwarnings('ignore') import logging logging.disable(1000) kipoi.list_sources() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source type location local_size n_models n_dataloaders 0 kipoi git-lfs /home/avsec/.kipoi/mo... 1,2G 780 780 s = kipoi.get_source(\"kipoi\") s GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') kipoi.list_models().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags 0 kipoi DeepSEAKeras 0.1 [Author(name='Jian Zh... [Author(name='Lara Ur... This CNN is based on ... keras seq TFBS_DHS_probs True MIT https://doi.org/10.10... ENCODE and Roadmap Ep... https://www.nature.co... [Histone modification... 1 kipoi extended_coda 0.1 [Author(name='Pang We... [Author(name='Johnny ... Single bp resolution ... keras [H3K27AC_subsampled] [H3K27ac] False MIT https://doi.org/10.10... Described in https://... Described in https://... [Histone modification] 2 kipoi DeepCpG_DNA/Hou2016_m... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/mESC1, cpg/mESC2... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 3 kipoi DeepCpG_DNA/Smallwood... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/BS24_1_2I, cpg/B... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 4 kipoi DeepCpG_DNA/Hou2016_H... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/HepG21, cpg/HepG... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] Model Let's choose to use the rbp_eclip/XRCC6 model from kipoi MODEL = \"rbp_eclip/XRCC6\" NOTE: If you are using python2, use a different model like MaxEntScan/3prime to following this example. # Note. Install all the dependencies for that model: # add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow) !kipoi env create {MODEL} !source activate kipoi-{MODEL} model = kipoi.get_model(MODEL) Available fields: Model type args info authors name version tags doc schema inputs targets default_dataloader - loaded dataloader class predict_on_batch() source source_dir pipeline predict() predict_example() predict_generator() Dataloader type defined_as args info (same as for the model) output_schema inputs targets metadata source source_dir example_kwargs init_example() batch_iter() batch_train_iter() batch_predict_iter() load_all() model <kipoi.model.KerasModel at 0x7f95b27af2b0> model.type 'keras' Info model.info ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\' ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017 ', training_procedure='Single task training with ADAM') model.info.version '0.1' Schema dict(model.schema.inputs) {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None), 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None), 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None), 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None), 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)} model.schema.targets ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None) Default dataloader Model already has the default dataloder present. To use it, specify model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/XRCC6' model.default_dataloader dataloader.SeqDistDataset model.default_dataloader.info Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks - tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end ', name=None, version='0.1', license='MIT', tags=[]) Predict_on_batch model.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>> Others # Model source model.source GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') # model location directory model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/XRCC6' DataLoader DataLoader = kipoi.get_dataloader_factory(MODEL) # same as DataLoader = model.default_dataloader A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed: # Display information about the dataloader print(DataLoader.__doc__) Args: intervals_file: file path; tsv file Assumes bed-like `chrom start end id score strand` format. fasta_file: file path; Genome sequence gtf_file: file path; Genome annotation GTF file. filter_protein_coding: Considering genomic landmarks only for protein coding genes preproc_transformer: file path; tranformer used for pre-processing. target_file: file path; path to the targets batch_size: int # Alternatively the dataloader keyword arguments can be displayed using the function: kipoi.print_dl_kwargs(DataLoader) Keyword argument: `intervals_file` doc: bed6 file with `chrom start end id score strand` columns type: str optional: False example: example_files/intervals.bed Keyword argument: `fasta_file` doc: Reference genome sequence type: str optional: False example: example_files/hg38_chr22.fa Keyword argument: `gtf_file` doc: file path; Genome annotation GTF file type: str optional: False example: example_files/gencode.v24.annotation_chr22.gtf Keyword argument: `filter_protein_coding` doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark. type: str optional: True example: True Keyword argument: `target_file` doc: path to the targets (txt) file type: str optional: True example: example_files/targets.tsv Keyword argument: `use_linecache` doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows type: str optional: True -------------------------------------------------------------------------------- Example keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'} Run dataloader on some examples # each dataloader already provides example files which can be used to illustrate its use: DataLoader.example_kwargs {'fasta_file': 'example_files/hg38_chr22.fa', 'filter_protein_coding': True, 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'intervals_file': 'example_files/intervals.bed', 'target_file': 'example_files/targets.tsv'} import os # cd into the source directory os.chdir(DataLoader.source_dir) !tree . \u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py \u251c\u2500\u2500 dataloader_files \u2502 \u2514\u2500\u2500 position_transformer.pkl \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u251c\u2500\u2500 model.yaml -> ../template/model.yaml \u2514\u2500\u2500 __pycache__ \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc \u2514\u2500\u2500 dataloader.cpython-36.pyc 4 directories, 8 files dl = DataLoader(**DataLoader.example_kwargs) # could be also done with DataLoader.init_example() # This particular dataloader is of type Dataset # i.e. it implements the __getitem__ method: dl[0].keys() dict_keys(['inputs', 'targets', 'metadata']) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) len(dl) 14 Get the whole dataset whole_data = dl.load_all() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.24it/s] whole_data.keys() dict_keys(['inputs', 'targets', 'metadata']) whole_data[\"inputs\"][\"seq\"].shape (14, 101, 4) Get the iterator to run predictions it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False) next(it)[\"inputs\"][\"seq\"].shape (1, 101, 4) model.predict_on_batch(next(it)[\"inputs\"]) array([[0.1351]], dtype=float32) Pipeline Pipeline object will take the dataloader arguments and run the whole pipeline directly: dataloader arguments --Dataloader--> numpy arrays --Model--> prediction example_kwargs = model.default_dataloader.example_kwargs preds = model.pipeline.predict_example() preds 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.78it/s] array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) model.pipeline.predict(example_kwargs) 1it [00:01, 1.56s/it] array([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208], dtype=float32) next(model.pipeline.predict_generator(example_kwargs, batch_size=2)) array([[0.4208], [0.0005]], dtype=float32) from kipoi_utils.data_utils import numpy_collate numpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs))) array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) Re-train the Keras model Keras model is stored under the .model attribute. model.model.compile(\"adam\", \"binary_crossentropy\") train_it = dl.batch_train_iter(batch_size=2) # model.model.summary() model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1) Epoch 1/1 3/3 [==============================] - 1s 291ms/step - loss: 1.3592 <keras.callbacks.History at 0x7f95b0095fd0>","title":"Python API"},{"location":"tutorials/python-api/#kipoi-python-api","text":"","title":"Kipoi python API"},{"location":"tutorials/python-api/#quick-start","text":"There are three basic building blocks in kipoi: Source - provides Models and DataLoaders. Model - makes the prediction given the numpy arrays. Dataloader - loads the data from raw files and transforms them into a form that is directly consumable by the Model","title":"Quick start"},{"location":"tutorials/python-api/#list-of-main-commands","text":"Get/list sources - kipoi.list_sources() - kipoi.get_source() List models/dataloaders - kipoi.list_models() - kipoi.list_dataloaders() Get model/dataloader - kipoi.get_model() - kipoi.get_dataloader_factory() Load only model/dataloader description from the yaml file without loading the model kipoi.get_model_descr() kipoi.get_dataloader_descr() Install the dependencies - kipoi.install_model_dependencies() - kipoi.install_dataloader_dependencies() import kipoi","title":"List of main commands"},{"location":"tutorials/python-api/#source","text":"Available sources are specified in the config file located at: ~/.kipoi/config.yaml . Here is an example config file: model_sources: kipoi: # default type: git-lfs # git repository with large file storage (git-lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: ~/.kipoi/models/ # local storage path gl: type: git-lfs # custom model remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git local_path: /s/project/model-zoo There are three different model sources possible: git-lfs - git repository with source files tracked normally by git and all the binary files like model weights (located in files* directories) are tracked by git-lfs . Requires git-lfs to be installed. git - all the files including weights (not recommended) local - local directory containing models defined in subdirectories For git-lfs source type, larger files tracked by git-lfs will be downloaded into the specified directory local_path only after the model has been requested (when invoking kipoi.get_model() ).","title":"Source"},{"location":"tutorials/python-api/#note","text":"A particular model/dataloader is defined by its source (say kipoi or my_git_models ) and the relative path of the desired model directory from the model source root (say rbp/ ). A directory is considered a model if it contains a model.yaml file. import kipoi import warnings warnings.filterwarnings('ignore') import logging logging.disable(1000) kipoi.list_sources() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source type location local_size n_models n_dataloaders 0 kipoi git-lfs /home/avsec/.kipoi/mo... 1,2G 780 780 s = kipoi.get_source(\"kipoi\") s GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') kipoi.list_models().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags 0 kipoi DeepSEAKeras 0.1 [Author(name='Jian Zh... [Author(name='Lara Ur... This CNN is based on ... keras seq TFBS_DHS_probs True MIT https://doi.org/10.10... ENCODE and Roadmap Ep... https://www.nature.co... [Histone modification... 1 kipoi extended_coda 0.1 [Author(name='Pang We... [Author(name='Johnny ... Single bp resolution ... keras [H3K27AC_subsampled] [H3K27ac] False MIT https://doi.org/10.10... Described in https://... Described in https://... [Histone modification] 2 kipoi DeepCpG_DNA/Hou2016_m... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/mESC1, cpg/mESC2... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 3 kipoi DeepCpG_DNA/Smallwood... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/BS24_1_2I, cpg/B... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 4 kipoi DeepCpG_DNA/Hou2016_H... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/HepG21, cpg/HepG... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation]","title":"Note"},{"location":"tutorials/python-api/#model","text":"Let's choose to use the rbp_eclip/XRCC6 model from kipoi MODEL = \"rbp_eclip/XRCC6\" NOTE: If you are using python2, use a different model like MaxEntScan/3prime to following this example. # Note. Install all the dependencies for that model: # add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow) !kipoi env create {MODEL} !source activate kipoi-{MODEL} model = kipoi.get_model(MODEL)","title":"Model"},{"location":"tutorials/python-api/#available-fields","text":"","title":"Available fields:"},{"location":"tutorials/python-api/#model_1","text":"type args info authors name version tags doc schema inputs targets default_dataloader - loaded dataloader class predict_on_batch() source source_dir pipeline predict() predict_example() predict_generator()","title":"Model"},{"location":"tutorials/python-api/#dataloader","text":"type defined_as args info (same as for the model) output_schema inputs targets metadata source source_dir example_kwargs init_example() batch_iter() batch_train_iter() batch_predict_iter() load_all() model <kipoi.model.KerasModel at 0x7f95b27af2b0> model.type 'keras'","title":"Dataloader"},{"location":"tutorials/python-api/#info","text":"model.info ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\' ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017 ', training_procedure='Single task training with ADAM') model.info.version '0.1'","title":"Info"},{"location":"tutorials/python-api/#schema","text":"dict(model.schema.inputs) {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None), 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None), 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None), 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None), 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)} model.schema.targets ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None)","title":"Schema"},{"location":"tutorials/python-api/#default-dataloader","text":"Model already has the default dataloder present. To use it, specify model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/XRCC6' model.default_dataloader dataloader.SeqDistDataset model.default_dataloader.info Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks - tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end ', name=None, version='0.1', license='MIT', tags=[])","title":"Default dataloader"},{"location":"tutorials/python-api/#predict_on_batch","text":"model.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>>","title":"Predict_on_batch"},{"location":"tutorials/python-api/#others","text":"# Model source model.source GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') # model location directory model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/XRCC6'","title":"Others"},{"location":"tutorials/python-api/#dataloader_1","text":"DataLoader = kipoi.get_dataloader_factory(MODEL) # same as DataLoader = model.default_dataloader A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed: # Display information about the dataloader print(DataLoader.__doc__) Args: intervals_file: file path; tsv file Assumes bed-like `chrom start end id score strand` format. fasta_file: file path; Genome sequence gtf_file: file path; Genome annotation GTF file. filter_protein_coding: Considering genomic landmarks only for protein coding genes preproc_transformer: file path; tranformer used for pre-processing. target_file: file path; path to the targets batch_size: int # Alternatively the dataloader keyword arguments can be displayed using the function: kipoi.print_dl_kwargs(DataLoader) Keyword argument: `intervals_file` doc: bed6 file with `chrom start end id score strand` columns type: str optional: False example: example_files/intervals.bed Keyword argument: `fasta_file` doc: Reference genome sequence type: str optional: False example: example_files/hg38_chr22.fa Keyword argument: `gtf_file` doc: file path; Genome annotation GTF file type: str optional: False example: example_files/gencode.v24.annotation_chr22.gtf Keyword argument: `filter_protein_coding` doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark. type: str optional: True example: True Keyword argument: `target_file` doc: path to the targets (txt) file type: str optional: True example: example_files/targets.tsv Keyword argument: `use_linecache` doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows type: str optional: True -------------------------------------------------------------------------------- Example keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}","title":"DataLoader"},{"location":"tutorials/python-api/#run-dataloader-on-some-examples","text":"# each dataloader already provides example files which can be used to illustrate its use: DataLoader.example_kwargs {'fasta_file': 'example_files/hg38_chr22.fa', 'filter_protein_coding': True, 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'intervals_file': 'example_files/intervals.bed', 'target_file': 'example_files/targets.tsv'} import os # cd into the source directory os.chdir(DataLoader.source_dir) !tree . \u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py \u251c\u2500\u2500 dataloader_files \u2502 \u2514\u2500\u2500 position_transformer.pkl \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u251c\u2500\u2500 model.yaml -> ../template/model.yaml \u2514\u2500\u2500 __pycache__ \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc \u2514\u2500\u2500 dataloader.cpython-36.pyc 4 directories, 8 files dl = DataLoader(**DataLoader.example_kwargs) # could be also done with DataLoader.init_example() # This particular dataloader is of type Dataset # i.e. it implements the __getitem__ method: dl[0].keys() dict_keys(['inputs', 'targets', 'metadata']) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) len(dl) 14","title":"Run dataloader on some examples"},{"location":"tutorials/python-api/#get-the-whole-dataset","text":"whole_data = dl.load_all() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.24it/s] whole_data.keys() dict_keys(['inputs', 'targets', 'metadata']) whole_data[\"inputs\"][\"seq\"].shape (14, 101, 4)","title":"Get the whole dataset"},{"location":"tutorials/python-api/#get-the-iterator-to-run-predictions","text":"it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False) next(it)[\"inputs\"][\"seq\"].shape (1, 101, 4) model.predict_on_batch(next(it)[\"inputs\"]) array([[0.1351]], dtype=float32)","title":"Get the iterator to run predictions"},{"location":"tutorials/python-api/#pipeline","text":"Pipeline object will take the dataloader arguments and run the whole pipeline directly: dataloader arguments --Dataloader--> numpy arrays --Model--> prediction example_kwargs = model.default_dataloader.example_kwargs preds = model.pipeline.predict_example() preds 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.78it/s] array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) model.pipeline.predict(example_kwargs) 1it [00:01, 1.56s/it] array([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208], dtype=float32) next(model.pipeline.predict_generator(example_kwargs, batch_size=2)) array([[0.4208], [0.0005]], dtype=float32) from kipoi_utils.data_utils import numpy_collate numpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs))) array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32)","title":"Pipeline"},{"location":"tutorials/python-api/#re-train-the-keras-model","text":"Keras model is stored under the .model attribute. model.model.compile(\"adam\", \"binary_crossentropy\") train_it = dl.batch_train_iter(batch_size=2) # model.model.summary() model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1) Epoch 1/1 3/3 [==============================] - 1s 291ms/step - loss: 1.3592 <keras.callbacks.History at 0x7f95b0095fd0>","title":"Re-train the Keras model"},{"location":"tutorials/tf_binding_models/","text":"Generated from notebooks/tf_binding_models.ipynb Model benchmarking with Kipoi This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example). Load software tools Let's start by loading software for this tutorial: the kipoi model zoo, import kipoi import numpy as np from sklearn.metrics import roc_auc_score Prepare data files Next, we introduce a labeled BED-format interval file and a genome fasta file intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv' fasta_file = 'example_data/hg19_chr22.fa' dl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file} Let's look at the first few lines in the intervals file !head $intervals_file chr22 20208963 20209064 0 chr22 29673572 29673673 0 chr22 28193720 28193821 0 chr22 43864274 43864375 0 chr22 18261550 18261651 0 chr22 7869409 7869510 0 chr22 49798024 49798125 0 chr22 43088594 43088695 0 chr22 35147671 35147772 0 chr22 49486843 49486944 0 The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives. Let's load the labels from the last column: labels = np.loadtxt(intervals_file, usecols=(3,)) Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file. Install DeepBind model software requirements kipoi env create DeepBind --source=kipoi source activate kipoi-DeepBind ## Load DeepBind model deepbind_model_name = \"DeepBind/Homo_sapiens/TF/D00328.018_ChIP-seq_CTCF\" deepbind_model = kipoi.get_model(deepbind_model_name) Using downloaded and verified file: /Users/b260/.kipoi/models/DeepBind/downloaded/model_files/Homo_sapiens/TF/D00328.018_ChIP-seq_CTCF/arch/0d6747991a525b94a1ac9174459c2bf4 Using downloaded and verified file: /Users/b260/.kipoi/models/DeepBind/downloaded/model_files/Homo_sapiens/TF/D00328.018_ChIP-seq_CTCF/weights/838eb7287139a2542f21984e692a9be2 Get DeepBind predictions deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000) 2it [00:02, 1.16s/it] Evaluate DeepBind predictions Let's check the auROC of deepbind predictions: roc_auc_score(labels, deepbind_predictions) 0.614856 kipoi env create pwm_HOCOMOCO source activate kipoi-pwm_HOCOMOCO pwm_model_name = \"pwm_HOCOMOCO/human/JUND\" pwm_model = kipoi.get_model(pwm_model_name) pwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000) print(\"PWM auROC:\") roc_auc_score(labels, pwm_predictions) 0.00B [00:00, ?B/s]Downloading https://zenodo.org/record/1466139/files/human-JUND.h5?download=1 to /Users/b260/.kipoi/models/pwm_HOCOMOCO/downloaded/model_files/human/JUND/weights/bb64a335f37cff4537b1bde4c11cab8b 16.4kB [00:01, 16.0kB/s] 2it [00:01, 1.51it/s]PWM auROC: 0.6431155 In this example, HOCOMOCO PWM's auROC of 64.3% outperforms the DeepBind auROC of 61.5%","title":"Comparing models"},{"location":"tutorials/tf_binding_models/#model-benchmarking-with-kipoi","text":"This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example).","title":"Model benchmarking with Kipoi"},{"location":"tutorials/tf_binding_models/#load-software-tools","text":"Let's start by loading software for this tutorial: the kipoi model zoo, import kipoi import numpy as np from sklearn.metrics import roc_auc_score","title":"Load software tools"},{"location":"tutorials/tf_binding_models/#prepare-data-files","text":"Next, we introduce a labeled BED-format interval file and a genome fasta file intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv' fasta_file = 'example_data/hg19_chr22.fa' dl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file} Let's look at the first few lines in the intervals file !head $intervals_file chr22 20208963 20209064 0 chr22 29673572 29673673 0 chr22 28193720 28193821 0 chr22 43864274 43864375 0 chr22 18261550 18261651 0 chr22 7869409 7869510 0 chr22 49798024 49798125 0 chr22 43088594 43088695 0 chr22 35147671 35147772 0 chr22 49486843 49486944 0 The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives. Let's load the labels from the last column: labels = np.loadtxt(intervals_file, usecols=(3,)) Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file.","title":"Prepare data files"},{"location":"tutorials/tf_binding_models/#install-deepbind-model-software-requirements","text":"kipoi env create DeepBind --source=kipoi source activate kipoi-DeepBind ## Load DeepBind model deepbind_model_name = \"DeepBind/Homo_sapiens/TF/D00328.018_ChIP-seq_CTCF\" deepbind_model = kipoi.get_model(deepbind_model_name) Using downloaded and verified file: /Users/b260/.kipoi/models/DeepBind/downloaded/model_files/Homo_sapiens/TF/D00328.018_ChIP-seq_CTCF/arch/0d6747991a525b94a1ac9174459c2bf4 Using downloaded and verified file: /Users/b260/.kipoi/models/DeepBind/downloaded/model_files/Homo_sapiens/TF/D00328.018_ChIP-seq_CTCF/weights/838eb7287139a2542f21984e692a9be2","title":"Install DeepBind model software requirements"},{"location":"tutorials/tf_binding_models/#get-deepbind-predictions","text":"deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000) 2it [00:02, 1.16s/it]","title":"Get DeepBind predictions"},{"location":"tutorials/tf_binding_models/#evaluate-deepbind-predictions","text":"Let's check the auROC of deepbind predictions: roc_auc_score(labels, deepbind_predictions) 0.614856 kipoi env create pwm_HOCOMOCO source activate kipoi-pwm_HOCOMOCO pwm_model_name = \"pwm_HOCOMOCO/human/JUND\" pwm_model = kipoi.get_model(pwm_model_name) pwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000) print(\"PWM auROC:\") roc_auc_score(labels, pwm_predictions) 0.00B [00:00, ?B/s]Downloading https://zenodo.org/record/1466139/files/human-JUND.h5?download=1 to /Users/b260/.kipoi/models/pwm_HOCOMOCO/downloaded/model_files/human/JUND/weights/bb64a335f37cff4537b1bde4c11cab8b 16.4kB [00:01, 16.0kB/s] 2it [00:01, 1.51it/s]PWM auROC: 0.6431155 In this example, HOCOMOCO PWM's auROC of 64.3% outperforms the DeepBind auROC of 61.5%","title":"Evaluate DeepBind predictions"},{"location":"using/03_Model_sources/","text":"~/.kipoi/config.yaml kipoi package has a config file located at ~/.kipoi/config.yaml . By default, it will look like this (without comments): model_sources: kipoi: # source name type: git # git repository remote_url: git@github.com:kipoi/models.git # git remote local_path: /home/avsec/.kipoi/models/ # local storage path # special model source storing models accessed via github permalinks github-permalink: type: github-permalink local_path: /home/avsec/.kipoi/github-permalink/ model_sources defines all the places where kipoi will search for models and pull them to a local directory. By default, it contains the model-zoo from github.com/kipoi/models which is a normal git repository, All possible model source types In addition to the default kipoi source, you can modify ~/.kipoi/config.yaml and add additional (private or public) model sources. Available model source types are: git - Normal git repository, all the files will be downloaded on checkout. This is the source type used by the public kipoi repository. git-lfs - Model weights will get downloaded from git-lfs upon request. local - Local directory. Example: model_sources: kipoi: type: git remote_url: git@github.com:kipoi/models.git local_path: /home/avsec/.kipoi/models/ my_git_models: type: git remote_url: git@github.com:asd/other_models.git local_path: ~/.kipoi/other_models/ my_local_models: type: local local_path: /data/mymodels/ About model definition A particular model is defined by its source (key under model_sources , say kipoi ) and the relative path of the desired model directory from the model source root (say Basset ). A directory is considered a model if it contains a model.yaml file.","title":"Private and public model sources"},{"location":"using/03_Model_sources/#kipoiconfigyaml","text":"kipoi package has a config file located at ~/.kipoi/config.yaml . By default, it will look like this (without comments): model_sources: kipoi: # source name type: git # git repository remote_url: git@github.com:kipoi/models.git # git remote local_path: /home/avsec/.kipoi/models/ # local storage path # special model source storing models accessed via github permalinks github-permalink: type: github-permalink local_path: /home/avsec/.kipoi/github-permalink/ model_sources defines all the places where kipoi will search for models and pull them to a local directory. By default, it contains the model-zoo from github.com/kipoi/models which is a normal git repository,","title":"~/.kipoi/config.yaml"},{"location":"using/03_Model_sources/#all-possible-model-source-types","text":"In addition to the default kipoi source, you can modify ~/.kipoi/config.yaml and add additional (private or public) model sources. Available model source types are: git - Normal git repository, all the files will be downloaded on checkout. This is the source type used by the public kipoi repository. git-lfs - Model weights will get downloaded from git-lfs upon request. local - Local directory. Example: model_sources: kipoi: type: git remote_url: git@github.com:kipoi/models.git local_path: /home/avsec/.kipoi/models/ my_git_models: type: git remote_url: git@github.com:asd/other_models.git local_path: ~/.kipoi/other_models/ my_local_models: type: local local_path: /data/mymodels/","title":"All possible model source types"},{"location":"using/03_Model_sources/#about-model-definition","text":"A particular model is defined by its source (key under model_sources , say kipoi ) and the relative path of the desired model directory from the model source root (say Basset ). A directory is considered a model if it contains a model.yaml file.","title":"About model definition"},{"location":"using/04_Installing_on_OSX/","text":"Using Kipoi - Installing on OSX Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is. For Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda. We have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. kipoi where you install Kipoi. Steps Make sure you have python installed: You can try by just execting python in your Terminal, if nothing is found you will want to install python (not pythonw ). There are some good explanations on how python 2 can be installed on OSX Sierra and if you are using High Sierra and you prefer python 3 you can follow this . After completing the steps and installing conda or miniconda please procede as described in getting started .","title":"Installing on OSX"},{"location":"using/04_Installing_on_OSX/#using-kipoi-installing-on-osx","text":"Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is. For Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda. We have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. kipoi where you install Kipoi.","title":"Using Kipoi - Installing on OSX"},{"location":"using/04_Installing_on_OSX/#steps","text":"","title":"Steps"},{"location":"using/04_Installing_on_OSX/#make-sure-you-have-python-installed","text":"You can try by just execting python in your Terminal, if nothing is found you will want to install python (not pythonw ). There are some good explanations on how python 2 can be installed on OSX Sierra and if you are using High Sierra and you prefer python 3 you can follow this . After completing the steps and installing conda or miniconda please procede as described in getting started .","title":"Make sure you have python installed:"},{"location":"using/R/","text":"R You can use Kipoi from R via the reticulate package. For a more complete example, see https://github.com/kipoi/kipoi/blob/master/notebooks/R-api.ipynb . Installation Install Kipoi. See how Install R Install the reticulate package. From R, run: install.packages(\"reticulate\") Make sure reticulate is using python from the miniconda/anaconda installation (same as Kipoi): library(reticulate) reticulate::py_config() Usage Use a specific conda environment library(reticulate) reticulate::use_condaenv(\"kipoi-Basset) or install the dependencies from R: kipoi$install_model_requirements(\"Basset\") Get the model: kipoi <- import('kipoi') model <- kipoi$get_model('Basset') Make a prediction for example files predictions <- model$pipeline$predict_example() Use dataloader and model separately # Get the dataloader setwd('~/.kipoi/models/Basset') dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) model$predict_on_batch(batch$inputs) Make predictions for custom files directly: pred <- model$pipeline$predict(dl_kwargs, batch_size=4)","title":"R"},{"location":"using/R/#r","text":"You can use Kipoi from R via the reticulate package. For a more complete example, see https://github.com/kipoi/kipoi/blob/master/notebooks/R-api.ipynb .","title":"R"},{"location":"using/R/#installation","text":"Install Kipoi. See how Install R Install the reticulate package. From R, run: install.packages(\"reticulate\") Make sure reticulate is using python from the miniconda/anaconda installation (same as Kipoi): library(reticulate) reticulate::py_config()","title":"Installation"},{"location":"using/R/#usage","text":"Use a specific conda environment library(reticulate) reticulate::use_condaenv(\"kipoi-Basset) or install the dependencies from R: kipoi$install_model_requirements(\"Basset\") Get the model: kipoi <- import('kipoi') model <- kipoi$get_model('Basset') Make a prediction for example files predictions <- model$pipeline$predict_example() Use dataloader and model separately # Get the dataloader setwd('~/.kipoi/models/Basset') dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) model$predict_on_batch(batch$inputs) Make predictions for custom files directly: pred <- model$pipeline$predict(dl_kwargs, batch_size=4)","title":"Usage"},{"location":"using/cli/","text":"Command-line interface For the command line interface the help command should explain most functionality kipoi -h ls List all models kipoi ls info Get information on how the required dataloader keyword arguments kipoi info Basset predict Run model prediction kipoi get-example Basset -o example kipoi predict Basset \\ --dataloader_args='{\"intervals_file\": \"example/intervals_file\", \"fasta_file\": \"example/fasta_file\"}' \\ -o '/tmp/Basset.example_pred.tsv' # check the results head '/tmp/Basset.example_pred.tsv' You can add --singularity to the command in order to execute the command in the virtual environment. test Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/Basset/example_files In detail, kipoi test <model> checks for three things: - Whether the model specific conda environment can be created and activated successfully - Whether the data scheme that the dataloader outputs and the model expects match or not - Optionally, if there is a 'test'.'expect' field in model.yaml pointing to some *.h5 file, the predictions in the file are compared with the prediction produced by the the model. E.g.: yaml test: expect: url: https://zenodo.org/record/5511940/files/APARENT.site_probabilities.predictions.hdf5?download=1 md5: 1adb12be84240ffb7d7ca556eeb19e01 env create Create a new conda environment for the model kipoi env create Basset source activate kipoi-Basset list List all environments kipoi env list Use source activate <env> or conda activate <env> to activate the environment. See also https://github.com/kipoi/examples for more information.","title":"CLI"},{"location":"using/cli/#command-line-interface","text":"For the command line interface the help command should explain most functionality kipoi -h","title":"Command-line interface"},{"location":"using/cli/#ls","text":"List all models kipoi ls","title":"ls"},{"location":"using/cli/#info","text":"Get information on how the required dataloader keyword arguments kipoi info Basset","title":"info"},{"location":"using/cli/#predict","text":"Run model prediction kipoi get-example Basset -o example kipoi predict Basset \\ --dataloader_args='{\"intervals_file\": \"example/intervals_file\", \"fasta_file\": \"example/fasta_file\"}' \\ -o '/tmp/Basset.example_pred.tsv' # check the results head '/tmp/Basset.example_pred.tsv' You can add --singularity to the command in order to execute the command in the virtual environment.","title":"predict"},{"location":"using/cli/#test","text":"Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/Basset/example_files In detail, kipoi test <model> checks for three things: - Whether the model specific conda environment can be created and activated successfully - Whether the data scheme that the dataloader outputs and the model expects match or not - Optionally, if there is a 'test'.'expect' field in model.yaml pointing to some *.h5 file, the predictions in the file are compared with the prediction produced by the the model. E.g.: yaml test: expect: url: https://zenodo.org/record/5511940/files/APARENT.site_probabilities.predictions.hdf5?download=1 md5: 1adb12be84240ffb7d7ca556eeb19e01","title":"test"},{"location":"using/cli/#env","text":"","title":"env"},{"location":"using/cli/#create","text":"Create a new conda environment for the model kipoi env create Basset source activate kipoi-Basset","title":"create"},{"location":"using/cli/#list","text":"List all environments kipoi env list Use source activate <env> or conda activate <env> to activate the environment. See also https://github.com/kipoi/examples for more information.","title":"list"},{"location":"using/plugins/","text":"Plugins To enable additional functionality beyond just running model predictions, a plugin is available: kipoi-interpret: model interpretation. github , docs Kipoi-interpret Kipoi-interpret is a general (genomics agnostic) plugin and allows to compute the feature importance scores like saliency maps or DeepLift for Kipoi models. import kipoi from kipoi_interpret.importance_scores.gradient import GradientXInput model = kipoi.get_model(\"DeepBind/Homo_sapiens/TF/D00765.001_ChIP-seq_GATA1\") val = GradientXInput(model).score(seq_array)[0] seqlogo_heatmap(val, val.T)","title":"Plugins"},{"location":"using/plugins/#plugins","text":"To enable additional functionality beyond just running model predictions, a plugin is available: kipoi-interpret: model interpretation. github , docs","title":"Plugins"},{"location":"using/plugins/#kipoi-interpret","text":"Kipoi-interpret is a general (genomics agnostic) plugin and allows to compute the feature importance scores like saliency maps or DeepLift for Kipoi models. import kipoi from kipoi_interpret.importance_scores.gradient import GradientXInput model = kipoi.get_model(\"DeepBind/Homo_sapiens/TF/D00765.001_ChIP-seq_GATA1\") val = GradientXInput(model).score(seq_array)[0] seqlogo_heatmap(val, val.T)","title":"Kipoi-interpret"},{"location":"using/python/","text":"Python API See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi List all models kipoi.list_models() Get the model Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 dataloader.yaml # (optional) describes the dataloader \u2514\u2500\u2500 dataloader.py # (optional) implements the dataloader The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"Basset\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink') Access information about the model In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model Test the model Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example() Get predictions for the raw files For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the Basset model, you would run the following: dl_kwargs = model.default_dataloader.download_example('example') # Run the prediction pred = model.pipeline.predict(dl_kwargs, batch_size=4) Setup the dataloader If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\") Predict for a single batch Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs']) Re-train the model it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)","title":"Python"},{"location":"using/python/#python-api","text":"See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi","title":"Python API"},{"location":"using/python/#list-all-models","text":"kipoi.list_models()","title":"List all models"},{"location":"using/python/#get-the-model","text":"Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 dataloader.yaml # (optional) describes the dataloader \u2514\u2500\u2500 dataloader.py # (optional) implements the dataloader The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"Basset\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink')","title":"Get the model"},{"location":"using/python/#access-information-about-the-model","text":"In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model","title":"Access information about the model"},{"location":"using/python/#test-the-model","text":"Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example()","title":"Test the model"},{"location":"using/python/#get-predictions-for-the-raw-files","text":"For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the Basset model, you would run the following: dl_kwargs = model.default_dataloader.download_example('example') # Run the prediction pred = model.pipeline.predict(dl_kwargs, batch_size=4)","title":"Get predictions for the raw files"},{"location":"using/python/#setup-the-dataloader","text":"If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\")","title":"Setup the dataloader"},{"location":"using/python/#predict-for-a-single-batch","text":"Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs'])","title":"Predict for a single batch"},{"location":"using/python/#re-train-the-model","text":"it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)","title":"Re-train the model"}]}